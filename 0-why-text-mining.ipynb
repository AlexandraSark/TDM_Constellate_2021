{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By <a href=\"https://nkelber.com\">Nathan Kelber</a> <br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdKRf0QMWlvK"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [About *JSTOR TDM*](#about)\n",
    "* [Why learn text mining?](#why-learn)\n",
    "* Text Mining Methods by Scholarly Question\n",
    "  * What's it about?\n",
    "    * [Word Frequency](#wf-method)\n",
    "    * Collocation\n",
    "    * TF-IDF\n",
    "    * Topic Analysis\n",
    "  * How are they connected?\n",
    "    * Collocation\n",
    "    * Network Analysis\n",
    "  * How does it feel?\n",
    "    * Sentiment Analysis\n",
    "  * What names are in here?\n",
    "    * Named Entity Recognition\n",
    "  * How are they similar?\n",
    "    * Authorship Attribution\n",
    "    * Clustering\n",
    "    * Supervised Machine Learning\n",
    "* [Why should I learn Python? (And how much?)](#intro-to-python)\n",
    "* [What is a Jupyter Notebook?](#intro-to-jupyter)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEBCxfdVO1QQ"
   },
   "source": [
    "# About JSTOR TDM \n",
    "<a id =\"about\"></a>\n",
    "\n",
    "\n",
    "![JSTOR and Portico Logos](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/JSTORandPorticoLogo.png)\n",
    "\n",
    "Text mining, or the process of deriving new information from pattern and trend analysis of the written word, has the potential to revolutionize research across disciplines, but there is a massive hurdle facing those eager to unleash its power: the coding skills and statistical knowledge it requires can take years to develop.   All too often, researchers are shown the promise of text mining, but then told that promise can only be realized by the select few with the necessary technical skills.  Ted Underwood, Professor of English at the University of Illinois, analogizes this challenge by saying that analytics techniques are a “deceptively gentle welcome mat, followed by a trapdoor.”   \n",
    "\n",
    "JSTOR and Portico are addressing this problem by building a text and data mining platform aimed at teaching and enabling a generation of researchers to text mine.  The JSTOR & Portico text mining platform includes a user interface to allow researchers, students, and instructors to curate, visualize, and save custom datasets.  Researchers may download the extracted features of their curated datasets -- a non-consumptive “bag-of-words” where each journal issue or book in the custom dataset is represented with bibliographic metadata for the articles and chapters, the unique set of words on each page, the part of speech of each word, and the number of times the word occurs on the page. The platform includes a teaching and development environment (a Jupyter Hub) which will be populated with easy-to-use code tutorials and templates where new text miners can analyze their custom datasets and learn to modify the Python or R code to better suit their own research purposes.  Researchers may download and locally hold the extracted features of any content and the full-text of open content, while the full-text of rights restricted content will be available for analysis in a secure computing environment.\n",
    "\n",
    "The content in the text mining platform will at least include all of JSTOR and the content from those Portico publishers who choose to participate (currently, 30 publishers including John Wiley & Sons, Inc., Project Muse, Thieme Publishing Group, and Hindawi).  In addition, we are in discussions with third party content providers about participating with content and the service will include the ability for researchers to upload their own content for analysis.\n",
    "\n",
    "The JSTOR & Portico text mining service will provide both free tools and tools accessible exclusively for  institutional participants.  \n",
    "\n",
    "We are working with a set of ten reference institutions from late 2019 and through 2020 to identify and build all of the necessary features, with an aim to release the service in 2021.\n",
    "\n",
    "More information, including access to the prototype, may be found at: https://tdm-pilot.org.\n",
    "\n",
    "Any questions, discussion items, or requests for a demonstration may be sent to Amy Kirchhoff, Text and Data Mining Business Manager (amy.kirchhoff@ithaka.org).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yd-T_5F9NSW1"
   },
   "source": [
    "# Why learn text mining?\n",
    "<a id=\"why-learn\"></a>\n",
    "\n",
    "You may have heard buzz on campus about text analysis, artificial intelligence, or big data. But if you’re a humanities scholar, librarian, or someone who has never used data in their research, it is not obvious why text analysis matters. Many disciplines have eschewed text analysis for decades (and we all know a few faculty members likely to continue for decades more), but there is no doubt that interest in the field is growing. Why should you learn text mining when you could be writing your next article?\n",
    "\n",
    "For researchers, the primary advantage that text and data mining offer is an ability to consider knowledge at non-human scales (both very big and very small). Text analysis can enable us to consider a million books across a thousand-dimensional space, revealing aspects of our records that are not obvious to human readers whether those aspects are imperceptibly small, diffused across centuries, or simply within records never read. What does that mean in practice though? The short answer is more evidence (and more kinds of evidence) for interrogating humanities problems. In [\"Searching for the Victorians\"](http://dancohen.org/2010/10/04/searching-for-the-victorians/) (2010), Dan Cohen asks, 'how much evidence is enough?': \n",
    "\n",
    ">Many humanities scholars have been satisfied, perhaps unconsciously, with the use of a limited number of cases or examples to prove a thesis. Shouldn’t we ask, like the Victorians, what can we do to be most certain about a theory or interpretation? If we use intuition based on close reading, for instance, is that enough?\n",
    ">\n",
    ">Should we be worrying that our scholarship might be anecdotally correct but comprehensively wrong? Is 1 or 10 or 100 or 1000 books an adequate sample to know the Victorians? What we might do with all of Victorian literature—not a sample, or a few canonical texts, as in Houghton’s work, but all of it.\n",
    "\n",
    "To operate as a researcher in the 21st century is to be confronted with the challenges and opportunities of data—at once both being overwhelmed by too much and yet not nearly enough of *the right kind*. As Miriam Posner has pointed out, \"...even if they don’t call their sources data, traditional humanists do have pretty pressing data-management needs\" ([\"Humanities Data: A Necessary Contradiction\"](https://miriamposner.com/blog/humanities-data-a-necessary-contradiction/) 2015). Tom Scheinfeldt suggests that data concerns are becoming the primary concern of the humanities:\n",
    "\n",
    ">The new technology of the Internet has shifted the work of a rapidly growing number of scholars away from thinking big thoughts to forging new tools, methods, materials, techniques, and modes or work that will enable us to harness the still unwieldy, but obviously game-changing, information technologies now sitting on our desktops and in our pockets. These concerns touch all scholars. [\"Sunset for Ideology, Sunrise for Methodology\"](http://dancohen.org/2010/10/04/searching-for-the-victorians/) (2008)\n",
    "\n",
    "Indeed, humanists cannot afford to ignore computational methods since they are, for better or worse, at the heart of modern culture and industry. Future humanists will not be able to study our digital present without becoming adept at reading and manipulating the burgeoning data of our historical record. Ted Underwood describes this new horizon:\n",
    "\n",
    ">It is becoming clear that we have narrated literary history as a sequence of discrete movements and periods because chunks of that size are about as much of the past as a single person could remember and discuss at one time. Apparently, longer arcs of change have been hidden from us by their sheer scale—just as you can drive across a continent noticing mountains and political boundaries but never the curvature of the earth. A single pair of eyes at ground level can't grasp the curve of the horizon, and arguments limited by a single reader's memory can't reveal the largest patterns organizing literary history. <br /> [*Distant Horizons: Digital Evidence and Literary Change*](https://www.press.uchicago.edu/ucp/books/book/chicago/D/bo35853783.html) (2019)\n",
    "\n",
    "For many scholars, text analysis sounds *potentially* powerful and useful, but the reality remains that learning text analysis is not a trivial task. Most humanities coursework does not prepare students to work with data. The good news is that text analysis, like any skill, can be learned to a greater or lesser degree. For historians to study the early modern period, it is very helpful to have a command of Latin. Still, there are plenty of successful early modern scholars that never learn the language (or learn enough to navigate the resources significant to their research).\n",
    "\n",
    "Depending on your research question, *you may not need to learn any coding* to do text analysis. The problem for many scholars is the possible applications for text analysis are not clear, so they are not in a good position to decide what to learn (and how much). At the same time, the sophistication needed for doing text analysis is a moving target. Topic modeling was once a very complicated task, requiring an understanding of the command line. Today, it can be accomplished in minutes using just a mouse. \n",
    "\n",
    "This notebook is intended to help researchers get started with text analysis by addressing the fundamental opportunity-cost question for doing this kind of research:\n",
    "\n",
    "* How can (and has) text analysis improve(d) scholarship?\n",
    "* What can I do with the knowledge I already have?\n",
    "* What method(s) could I learn quickly to advance my research?\n",
    "* What tools and resources are available to help?\n",
    "\n",
    "In this introduction, we explain the various kinds of text analysis for a general scholarly audience. What are they? Why would you use them? How long will it take to apply them? (The methods presented here are among the most well-known but certainly not exhaustive.) Afterward, you'll be prepared to decide how much or how little text analysis may be useful to your research. As you read about these methods, it will be helpful to keep in mind the current, intractable problems that face your field. Could you use one of these methods to address them? Along the way, we will reference recent scholarly arguments as examples and models. \n",
    "\n",
    "## What are these texts about?\n",
    "* **Word Frequency** *Little or no coding required* <br />\n",
    "Counting the frequency of a word in any given text. This includes Bag of Words and TF-IDF. **Example:** \"Which of these texts focus on women?\"\n",
    "\n",
    "* **Collocation** *Little or no coding required* <br />\n",
    "Examining where words occur close to one another. **Example:** \"Where are women mentioned in relation to home ownership?\"\n",
    "\n",
    "* **Topic Analysis (or Topic Modeling)** *Little or no coding required* <br />\n",
    "Discovering the topics within a group of texts. **Example:** \"What are the most frequent topics discussed in this newspaper?\"\n",
    "\n",
    "* **TF/IDF** *Little or no coding required* <br />\n",
    "Finding the significant words within a text. **Example:** \"What language is most significant within 1970s political speech?\"\n",
    "\n",
    "## How are these texts connected?\n",
    "* **Concordance** *Little or no coding required* <br />\n",
    "Where is this word or phrase used in these documents? **Example:** \"Which journal articles mention Maya Angelou's phrase, 'If you're for the right thing, then you do it without thinking.'\"\n",
    "* **Network Analysis** *Moderate coding required* <br />\n",
    "How are the authors of these texts connected? **Example:** \"What local communities formed around civil rights in 1963?\"\n",
    "\n",
    "## What emotions (or affects) are found within these texts?\n",
    "* **Sentiment Analysis** *Moderate coding required* <br />\n",
    "Does the author use positive or negative language? **Example:** \"How do presidents describe gun control?\"\n",
    "\n",
    "## What names are used in these texts?\n",
    "* **Named Entity Recognition** *Moderate coding required* <br />\n",
    "List every example of a kind of entity from these texts. **Example:** \"What are all of the geographic locations mentioned by Tolstoy?\"\n",
    "\n",
    "## Which of these texts are most similar?\n",
    "\n",
    "* **Authorship Attribution** *Moderate coding required* <br />\n",
    "Find the author of an anonymous document. **Example:** \"Who wrote The Federalist Papers?\"\n",
    "* **Clustering** *Moderate coding required* <br />\n",
    "Which texts are the most similar? **Example:** \"Is this play closer to comedy or tragedy?\"\n",
    "* **Supervised Machine Learning** *Moderate coding required* <br />\n",
    "Are there other texts similar to this? **Example:** \"Are there other Jim Crow laws like these we have already identified?\"\n",
    "\n",
    "The next section will examine each method in greater depth.\n",
    "___\n",
    "\n",
    "Cohen, Dan. [\"Searching for the Victorians.\"](http://dancohen.org/2010/10/04/searching-for-the-victorians/). (2010).\n",
    "\n",
    "Posner, Miriam. [\"Humanities Data: A Necessary Contradiction\"](https://miriamposner.com/blog/humanities-data-a-necessary-contradiction/). (2015).\n",
    "\n",
    "Scheinfeldt, Tom. [\"Sunset for Ideology, Sunrise for Methodology?\"](http://foundhistory.org/2008/03/sunset-for-ideology-sunrise-for-methodology/). (2008).\n",
    "\n",
    "Underwood, Ted. [*Distant Horizons: Digital Evidence and Literary Change*](https://www.press.uchicago.edu/ucp/books/book/chicago/D/bo35853783.html). (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Text Mining Methods by Scholarly Question\n",
    "\n",
    "  * What's it about\n",
    "    * Word Frequency\n",
    "    * Collocation\n",
    "    * Topic Analysis\n",
    "    * TF-IDF\n",
    "  * How are they connected?\n",
    "    * Collocation\n",
    "    * Network Analysis\n",
    "  * How does it feel?\n",
    "    * Sentiment Analysis\n",
    "  * What names are in here?\n",
    "    * Named Entity Recognition\n",
    "  * How are they similar?\n",
    "    * Authorship Attribution\n",
    "    * Clustering\n",
    "    * Supervised Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8CPcwAuFPwXu",
    "toc-hr-collapsed": true
   },
   "source": [
    "## What's it about?\n",
    "\n",
    "The most common reason to use text analysis is to summarize and/or describe the content of a collection of texts, also known as a [corpus](./key-terms.ipynb#corpus). The following methods can help researchers by:\n",
    "\n",
    "* Giving a broad overview of topics, themes, and language\n",
    "* Locating and ranking the significance of particular words and phrases\n",
    "* Discovering language in each document surrounding particular topics\n",
    "\n",
    "These methods are useful at getting a quick, high-level view of a large variety of materials. Assuming your dataset is ready to analyze (like those created by [JSTOR's Digital Scholar Workbench](https://tdm-pilot.org)), these methods can be executed easily with text analysis software and require little or no coding expertise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtdSja83IMIV"
   },
   "source": [
    "### Word Frequency\n",
    "<a id =\"wf-method\"></a>\n",
    "\n",
    "The [Word Frequency](./key-terms.ipynb#word-frequency) method counts the number of occurrences of individual words within a particular text. Each document is described as a set of words and their counts. [Word Frequency](./key-terms.ipynb#word-frequency) uses a [bag of words](./key-terms.ipynb#bag-of-words) model where the order of words is not significant. Just as the letters of a Scrabble game are tossed into a bag without order, word frequency merely records the number of occurences with no regard to where a particular word occurs within a document. \n",
    "\n",
    "For example, the ten most common words in Shakespeare's Hamlet can be represented in the following table:\n",
    "\n",
    "|  Word  | Count|\n",
    "| -------|:----:|\n",
    "| the    | 1148 |\n",
    "| and    | 970  | \n",
    "| to     | 764  |  \n",
    "| of     | 671  |\n",
    "| i      | 573  |\n",
    "| a      | 550  |\n",
    "| you    | 550  |\n",
    "| my     | 514  |\n",
    "| hamlet | 485  |\n",
    "| in     | 437  |\n",
    "\n",
    "To represent the whole text, our table would have to include all of the 4,728 unique words in the play. While these counts can be useful, the above list contains mostly [function words](./key-terms.ipynb#function-words), common words with little lexical meaning like articles, prepositions, and conjunctions. We can remove the [function words](./key-terms.ipynb#function-words) from our analysis using a [stop words](./key-terms.ipynb#stop-words) list. This is a list of common words we would like to filter out of our analysis. If we filter out common [function words](./key-terms.ipynb#function-words) in English, the result is:\n",
    "\n",
    "|   Word  | Count|\n",
    "| --------|:----:|\n",
    "| hamlet  | 485  |\n",
    "| lord    | 313  | \n",
    "| king    | 199  |  \n",
    "| horatio | 159  |\n",
    "| polonius| 124  |\n",
    "| claudius| 122  |\n",
    "| queen   | 121  |\n",
    "| shall   | 114  |\n",
    "| good    | 109  |\n",
    "| come    | 106  |\n",
    "\n",
    "Our new list contains more [content words](./key-terms.ipynb#content-words), yet it is dominated by character names. The reason for this is that a play-text contains speech headings before each line. The word \"hamlet\" is the most common word in the play because it is counted every time Hamlet has a speaking line. This may be useful information if we are interested in determining who has the most lines in the play. If that is not our goal, we can filter out character names by adding them to our [stop words](./key-terms.ipynb#stop-words) list. Then we get:\n",
    "\n",
    "|  Word |Count|\n",
    "| ------|:---:|\n",
    "| good  | 109 |\n",
    "| come  | 106 | \n",
    "| let   | 95  |  \n",
    "| like  | 85  |\n",
    "| sir   | 75  |\n",
    "| know  | 74  |\n",
    "| enter | 72  |\n",
    "| love  | 68  |\n",
    "| speak | 63  |\n",
    "| make  | 56  |\n",
    "\n",
    "Far from a truly objective viewpoint, text analysis is often about refining and tailoring your analysis. Refining a [stop words](./key-terms.ipynb#stop-words) list is an important part of obtaining useful results using [word frequencies](./key-terms.ipynb#word-frequency).  The researcher must decide whether the filtering is adequate and appropriate. The answer always depends on the context of the argument. Is the above table appropriately filtered? The high frequency of the word \"enter\" is likely from stage directions instead of speaking lines. That may or may not be appropriate depending on the argument at hand.\n",
    "\n",
    "\n",
    "### Examples of Word Frequency\n",
    "\n",
    "One of the most popular kinds of visualization associated with the digital humanities is the tag cloud (or word cloud). A tag cloud visualizes word frequency by connecting the size of a word to its frequency. More common words are larger. \n",
    "![Word cloud of The Narrative of the Life of Frederick Douglass, An American Slave](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tagCloudDouglass.png)\n",
    "**Tag Cloud of *The Narrative of the Life of Frederick Douglass\n",
    "       An American Slave* created using Voyant.**\n",
    "In the visualization above, the ten most common words listed below are the largest words in the tag cloud.\n",
    "\n",
    "|Word   | Count |\n",
    "|----   |:-----:|\n",
    "| mr    |  168  |\n",
    "|slaves | 125   |\n",
    "|master | 124   |\n",
    "|slave  | 122   |\n",
    "|time   | 117   |\n",
    "|man    | 78    |\n",
    "|slavery| 69    |\n",
    "|covey  | 61    |\n",
    "|old    | 58    |\n",
    "|said   | 56    |\n",
    "\n",
    "In the above examples, the word frequencies are counted from a single work, *The Narrative of the Life of Frederick Douglass, An American Slave*. Depending on the size of our dataset, we can choose the appropriate size of the source text (e.g. chapter, book, volume, journal, month of articles, year of books, etc.). Text analysis can help us understand longterm trends in writing and publishing. Let us consider the very large [Early English Books Online- Text Creation Partnership (EEBO-TCP) corpus](https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/) which contains 25,368 early English print books (with an additional 34,963 to be made public in 2021). Using the N-gram Browser of earlyprint.org](https://earlyprint.org/lab/tool_ngram_browser.html?requestFromClient={%221%22:{%22spe%22:%22love,loue%22,%22reg%22:%22%22,%22lem%22:%22%22,%22pos%22:%22%22,%22originalPos%22:%22%22},%222%22:{%22spe%22:%22%22,%22reg%22:%22%22,%22lem%22:%22%22,%22pos%22:%22%22,%22originalPos%22:%22%22},%223%22:{%22spe%22:%22%22,%22reg%22:%22%22,%22lem%22:%22%22,%22pos%22:%22%22,%22originalPos%22:%22%22},%22databaseType%22:%22unigrams%22,%22smoothing%22:%22True%22,%22rollingAverage%22:%2220_year%22,%20%22instructionToggle%22:%20%22show%22}), we can visualize teh historical standardization of the spelling of the word love in the 17th century. \n",
    "\n",
    "![Love vs. loue graph from 1450-1700](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/earlyprint-love-loue.png)\n",
    "\n",
    "From this graph, we can see that approximate crossover point is 1630 when the spelling \"love\" became more popular than \"loue.\"\n",
    "\n",
    "### When should I use word frequency?\n",
    "\n",
    "Generally, word frequency is used as an initial form of data exploration. The kind of scholarly arguments that can be mounted from word frequency analysis are limited in scope, but the technique can help understand larger patterns in the overall dataset that may not be obvious beforehand. While a word frequency analysis may not form the basis of an entire article or chapter, word freqency is the foundation for many advanced text mining methods such as TF-IDF and Topic Analysis.\n",
    "\n",
    "One of the shortcomings of word frequency is that sometimes individual words are not useful units of analysis. For example, a social scientist may be interested in the relationship between nuclear families and drug offenses. Word frequency would allow us to search a thousand recent journal articles for the occurrences of \"nuclear,\" \"family,\" \"drug,\" and \"offense.\" But occurrences of \"nuclear\" could be about power plants and occurrences of \"offense\" may be about many other kinds of crime (or even sports). \n",
    "\n",
    "\"Nuclear family\" and \"drug offense\" are collocations. To address them, we'll need the method found in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1WwbXJ7JQVa"
   },
   "source": [
    "### Collocation\n",
    "\n",
    "An alternative to this approach is using [n-grams](./key-terms.ipynb#n-gram) which can capture phrases in addition to individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4aeJy3YJljK"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kegs0vqGJbYw"
   },
   "source": [
    "### Topic Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PQ1dQ1qRrm-",
    "toc-hr-collapsed": true
   },
   "source": [
    "## How are they connected?\n",
    "\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATtZFWDPVXFs"
   },
   "source": [
    "### Concordance\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0CPpeThVaRN"
   },
   "source": [
    "### Network Analysis\n",
    " Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rMV0GzMRSCCc",
    "toc-hr-collapsed": false
   },
   "source": [
    "## How does it feel?\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79X8F49tVnGE"
   },
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHvG69x1SQUu",
    "toc-hr-collapsed": true
   },
   "source": [
    "## What names are in here? \n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DRzCO0K8Vz5H"
   },
   "source": [
    "### Named Entity Recognition\n",
    " Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pj90uywzSb5w",
    "toc-hr-collapsed": false
   },
   "source": [
    "## How are they similar?\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDhqGL-0V8f8"
   },
   "source": [
    "### Authorship Attribution\n",
    "\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnB_EFD0V-qf"
   },
   "source": [
    "### Clustering\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQ1yEQeuWD65"
   },
   "source": [
    "### Supervised Machine Learning\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmtomRvRkf32"
   },
   "source": [
    "# Why learn Python?\n",
    "<a id =\"intro-to-python\"></a>\n",
    "\n",
    "Lorem Ipsum\n",
    "\n",
    "## Hands-on Learning Opportunities\n",
    "\n",
    "### Intensive Institutes (Travel required)\n",
    "* [Digital Humanities Summer Institute](https://dhsi.org)\n",
    "* [Digital Humanities Research Institute](http://dhinstitutes.org/)\n",
    "* [Humanities Intensive Learning and Teaching](http://dhtraining.org/hilt/)\n",
    "* [Data Matters](http://datamatters.org)\n",
    "\n",
    "\n",
    "## Python Tutorials by Discipline\n",
    "### Humanities\n",
    "* [Python Programming for the Humanities (Folgert Karsdorp)](http://www.karsdorp.io/python-course/)\n",
    "* [Intro to Python Workshop (Digital Humanities Research Institute)](https://github.com/DHRI-Curriculum/python)\n",
    "* [Intro to Python I and II (University Libraries UNC Chapel Hill)](https://unc-libraries-data.github.io/Python/)\n",
    "* [Python Lessons (The Programming Historian)](https://programminghistorian.org/en/lessons/?topic=python)\n",
    "\n",
    "### Libraries\n",
    "* [Python Intro for Libraries (Library Carpentry)](https://librarycarpentry.org/lc-python-intro/)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsaD_m83qEEm"
   },
   "source": [
    "# Why use Jupyter Notebooks?\n",
    "<a id=\"intro-to-jupyter\"></a>\n",
    "\n",
    "Jupyter notebooks are documents that contain both computer code (e.g. python or R) and rich text elements (i.e. explanatory text, figures, links). Jupyter notebooks have two significant advantages for teaching and learning to code:\n",
    "* Minimal Setup\n",
    "  * Traditional code editors may require students to become familiar with terminals, environments, libraries, etc. \n",
    "  * With a Jupyter notebook, users can run code immediately.\n",
    "* Rich Support Content\n",
    "  * Traditional code supports plain text commenting, but has a limited ability to include supporting explanation and information.\n",
    "  * Jupyter notebooks allow instructors to embed many kinds of supporting content including text, images, equations, links, and videos.\n",
    "  \n",
    "\n",
    "\n",
    "\"At present, one of [the] best ways of sharing or publishing these workflows might be with the free and open-source Jupyter Notebook.\" (Dobson 39)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Examples from\n",
    "Programming Historian\n",
    "Ted Underwood's Book\n",
    "DHRI\n",
    "DHSI\n",
    "HILT\n",
    "\n",
    "\n",
    "\n",
    "Essentially, a Jupyter notebook is a file (.ipynb) that can be easily saved, uploaded, downloaded, and converted. (For example, a notebook file (.ipynb) can be converted into a python file (.py), HTML file (.html), or PDF file (.pdf).) Users edit Jupyter notebook files (.ipynb) with specialized software such as the Jupyter Notebook application. Yes, they unfortunately named the file type and the application \"Jupyter Notebook\" which is confusing. \n",
    "\n",
    "To clarify, *The* Jupyter Notebook is a browser application that runs *a* Jupyter Notebook. To clarify the difference, you may hear people refer to them as “the Jupyter Notebook app” and \"a Jupyter notebook.” Fortunately, future versions of the application will be called “JupyterLab” instead of “The Jupyter Notebook” which should alleviate some of this confusion.\n",
    "\n",
    "While Jupyter notebook files can be saved, viewed, and edited on your local machine using The Jupyter Notebook application, it is often helpful to connect Jupyter notebooks to a Jupyter server that contains the unique environment (e.g. the right kernel, dependencies) to execute the notebook’s code. Using a server ensures that the environment for executing the code is consistent and correct.\n",
    "\n",
    "If all of this is confusing, \n",
    "\n",
    "\n",
    "Other Jupyter servers include:\n",
    "\n",
    "* Jupyter Lab (Jupyter's new replacement for \"The Jupyter Notebook\")\n",
    "* Microsoft Azure Notebooks\n",
    "* Google Colab\n",
    "* Kaggle (popular with the data science community)\n",
    "\n",
    "The interfaces differ slightly between these Jupyter servers, but they are essentially the same software.\n",
    "\n",
    "How can I use a Jupyter notebook file?\n",
    "\n",
    "\n",
    "What computer languages do Jupyter notebooks support?\n",
    "The Jupyter system supports over 100 programming languages including Python, Java, R, Julia, Matlab, Octave, Scheme, Processing, Scala, and more. The most common languages used for text and data mining are python and R. \n",
    "How does the Digital Scholar Workbench connect to the TDM Jupyter notebooks?\n",
    "The Digital Scholar Workbench \n",
    "\n",
    "\n",
    "In order to use Jupyter notebooks (or any coding environment) for text and data mining, a user must have facility with the command line and install the appropriate dependencies. In order to make this work more accessible, we have deployed a JupyterHub server with kernels and depencies designed for text and data mining. Teachers and students simply sign in to the Digital Scholar Workbench using their JSTOR login, and they can write, edit, and run code immediately. Whether you are a novice or an advanced programmer, you can:\n",
    "●\tBuild replicable and shareable datasets in minutes\n",
    "●\tStart complex analyses like Topic Modeling or TF/IDF in minutes\n",
    "●\tCreate, edit, run, and share notebooks focused on particular TDM methods\n",
    "\n",
    "---\n",
    "Dobson, James E. [*Critical Digital Humanities*](https://www.press.uillinois.edu/books/catalog/48xfp2zp9780252042270.html). (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KrOd5K9p6kSB"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "WelcomeTDM",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "nteract-on-jupyter@2.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
