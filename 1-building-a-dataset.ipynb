{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By <a href=\"https://nkelber.com\">Nathan Kelber</a> <br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.\n",
    "\n",
    "This notebook describes how to create a dataset using the [Digital Scholar Workbench](https://tdm-pilot.org/). The dataset generated is compatible with the following notebooks:\n",
    "\n",
    "Digital Scholar Workbench Compatible Notebooks\n",
    "* [Metadata](2-metadata.ipynb)\n",
    "* [Word Frequencies](3-word-frequencies.ipynb)\n",
    "* [Significant Terms](4-significant-terms.ipynb)\n",
    "* [Topic Modeling](5-topic-modeling.ipynb)\n",
    "\n",
    "# Notebook Table of Contents\n",
    "\n",
    "* [Introduction](#build-intro)\n",
    "* [Search Tools](#search-tools)\n",
    "* [Visualization Tools](#visualizations-tools)\n",
    "* [Building Your Dataset](#building-dataset)\n",
    "* [Technical Details of Your Dataset](#details-dataset)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "<a name=\"build-intro\"></a>\n",
    "\n",
    "Pick out a set of texts to analyze. Choose from texts within JSTOR and Portico with primary and secondary sources (1700-present) spanning disciplines including:\n",
    "\n",
    "* Agriculture\n",
    "* Anthropology\n",
    "* Education\n",
    "* Fine Arts\n",
    "* Geography\n",
    "* History\n",
    "* Language and Literature\n",
    "* Law\n",
    "* Medicine\n",
    "* Music\n",
    "* Philosophy\n",
    "* Political Science\n",
    "* Psychology\n",
    "* Religion\n",
    "* Science\n",
    "* Social Sciences\n",
    "* Technology\n",
    "\n",
    "Design your dataset to fit your personal research interests using powerful search and visualization tools.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Tools\n",
    "<a name=\"search-tools\"></a>\n",
    "\n",
    "![Search and Visualization Interfaces](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/searchandvisualization.png)\n",
    "\n",
    "**The search tools (left) and visualization tools (right) for creating a dataset.**\n",
    "\n",
    "## Keyword\n",
    "\n",
    "In the keyword searchbox, users may:\n",
    "\n",
    "* Enter individual keywords separated by a space. (No commas are necessary.)\n",
    "* Match exact phrases using quotation marks (“prison education”) for titles only. (Phrase-matching is not supported for the body text.)\n",
    "* Use boolean operators (prison AND education) (prison OR education) (education NOT prison). \n",
    "\n",
    "![Keyword User Interface](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/searchui.png)\n",
    "\n",
    "**The Search interface contains filters for keywords, publication title(s), publication dates, language(s), discipline(s), and provider(s).**\n",
    "\n",
    "## Publication Title(s)\n",
    "Users may additionally sort by individual titles if they are interested in a particular journal or other data source.\n",
    "\n",
    "## Publication Dates\n",
    "Sources may be filtered by year.\n",
    "\n",
    "## Language(s)\n",
    "Choose the language(s) you would like returned in search results. The Digital Scholar Workbench supports dozens of languages.\n",
    "\n",
    "## Discipline and Provider\n",
    "![Disciplines](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/disciplines.png)\n",
    "\n",
    "The discipline and provider areas allow users to choose what disciplines to include or exclude for their search. The disciplines are ordered by Library of Congress subject headings.\n",
    "\n",
    "## Creating your Dataset\n",
    "\n",
    "When the dataset reflects your intended research, click “Build.” A pop-up will prompt you for an email address to notify you when the dataset is ready. (Depending on the size of your dataset, this process may take 15-30 minutes.) \n",
    "\n",
    "![Build Button](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/buildbutton.png)\n",
    "\n",
    "**When the dataset meets your specifications, click \"build\".** For next steps, see [Using Your Dataset](#your-dataset).\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Tools\n",
    "<a name=\"visualization-tools\"></a>\n",
    "\n",
    "The search tools for creating a dataset are coupled with visualizations in order to help researchers understand what data is available and where it is coming from.\n",
    "\n",
    "## Your Customized Dataset\n",
    "The section displays the number of issues/volumes in your prospective dataset and their sources. The makeup is displayed in a pie chart.\n",
    "\n",
    "![Visualizing a customized dataset](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/yourCustomizedDataset.png)\n",
    "\n",
    "## Term Frequency\n",
    "The term frequency graph gives a raw count of the number of times a word is mentioned per year in the current dataset. The bulk of data in JSTOR, Portico, and HathiTrust is from the 20th century to the present, so it is likely that the graph will be similar to the one below.\n",
    "\n",
    "![The term frequency visualization](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/termFrequency.png)\n",
    "\n",
    "## Publication Dates\n",
    "The Publication Dates graph gives a raw count of the number of publications from each year of the dataset. The bulk of data in JSTOR, Portico, and HathiTrust is from the 20th century to the present, so it is likely that the graph will be similar to the one below.\n",
    "\n",
    "![The publication dates visualization](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/publicationDates.png)\n",
    "\n",
    "## Discipline Treemap\n",
    "The discipline tree map displays the makeup of the dataset by discipline. Hovering over a particular discipline will indicate the percentage of the dataset represented.\n",
    "\n",
    "![The discipline treemap visualization](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/disciplineTreemap.png)\n",
    "\n",
    "**When the dataset meets your specifications, click \"build\".**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Building the Dataset \n",
    "<a name=\"building-dataset\"></a>\n",
    "\n",
    "![The visualization of dataset processing](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/processingDataset.png)\n",
    "\n",
    "Depending on the size and complexity of your dataset, the build process can take from 5-30 minutes. If you prefer not to wait, enter your email address and you will be contacted automatically as soon as your dataset is ready.\n",
    "\n",
    "![The email prompt for a dataset](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/emailPrompt.png)\n",
    "\n",
    "The resulting email will contain:\n",
    "* A link to a page that summarizes your dataset and allows you to explore it\n",
    "* Your dataset ID that can be copied into specialized Jupyter Notebooks for analysis\n",
    "* A download link for your dataset\n",
    "\n",
    "![Email received after processsing](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/emailWorksetID.png)\n",
    "\n",
    "**Your dataset ID can be used in any of the following Digital Scholar Workbench Notebooks:**\n",
    "\n",
    "* [Metadata](2-metadata.ipynb)\n",
    "* [Word Frequencies](3-word-frequencies.ipynb)\n",
    "* [Significant Terms](4-significant-terms.ipynb)\n",
    "* [Topic Modeling](5-topic-modeling.ipynb)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's in my dataset?\n",
    "<a name=\"details-dataset\"></a>\n",
    "The JSTOR & Portico corpus builder creates datasets that are [non-consumptive](./key-terms.ipynb#non-consumptive) [bags of words](./key-terms.ipynb#bag-of-words). Each [dataset](./key-terms.ipynb#dataset) is a single file that contains all of the information for each document on a single line of code. This information includes:\n",
    "* [Bibliographic Metadata](./key-terms.ipynb#metadata)\n",
    "    * An id containing a stable JSTOR URL for the article or book\n",
    "    * The title of the book or journal article\n",
    "    * The title of the Journal\n",
    "    * The author(s)\n",
    "    * The type of publication\n",
    "    * The publication date\n",
    "    * The publisher\n",
    "* Document and Word Counts\n",
    "    * Total word count\n",
    "    * Unique words and their frequencies\n",
    "    * Page numbers\n",
    "    \n",
    "We are still refining the structure of the dataset file. We anticipate adding additional “features” (such as named entity recognition) in the future. Please reach out to Ted Lawless <Ted.Lawless@ithaka.org> if you have comments or suggestions.\n",
    "\n",
    "# The data format and structure\n",
    "\n",
    "Each dataset is represented by a single [JSON Lines file](./key-terms.ipynb#jsonl) (file extension \".jsonl\"). The data for each document in the [corpus](./key-terms.ipynb#corpus) is a written on a single line. (If there are 1,245 documents in the corpus, the file will 1,245 lines long.) Each line contains a list of key/value pairs that map a **key** concept to a matching **value**. The basic structure looks like\n",
    "\n",
    "> \"Key\": Value\n",
    "\n",
    "![View of the top of a sample file](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/headSectionOfJSONL.png)\n",
    "\n",
    "*This JSON Lines file has been broken down into a nested hierarchy on separate lines using [JSON Editor Online](https://jsoneditoronline.org/). This visualization makes it much easier for human readers to see the key/value pairs that make up a portion of the textual metadata information. The original JSONL is very difficult for human readers to read, but makes it easy to add or subtract individual texts by adding or removing a single line at a time.*\n",
    "\n",
    "In the above example, we can a portion of the metadata for the text. Here are a few items of interest:\n",
    "\n",
    "* The title is \"Shakespeare and the Middling Sort\" (\"title\": \"Shakespeare and the Middling Sort\")\n",
    "* The author is \"Theodore B. Leinwand\" (\"creators\": [\"Theodore B. Leinwand\"])\n",
    "* The text is a journal article (\"doctypeType\": \"article\")\n",
    "* The journal is *Shakespeare Quarterly* (\"isPartOf\": \"Shakespeare Quarterly\")\n",
    "* Identifiers such as ISSN, OCLC, and DOI\n",
    "* PageCount and WordCount\n",
    "\n",
    "At the end of the \n",
    "If you look closely, you'll discover additional metadata such as the publication date, DOI, page numbers, ISSN, and more. The frequency of each word in the text is found within the \"unigramCount\" section. In this context, the word \"[unigram](./key-terms.ipynb#unigram)\" describes a single word construction like the word \"chicken.\" There are also [bigrams](./key-terms.ipynb#bigram) (e.g. \"chicken stock\"), [trigrams](./key-terms.ipynb#trigram) (\"homemade chicken stock\"), and [n-grams](./key-terms.ipynb#n-gram) of any length. At the present time, our tools are only counting unigrams. \n",
    "\n",
    "![The JSONL file section that lists unigrams](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/unigramCountFromJSONL.png)\n",
    "\n",
    "*The start of the section of the JSONL file that lists the [unigrams](./key-terms.ipynb#unigram) for the text*\n",
    "\n",
    "Notice that the beginning of the \"unigramCount\" section mostly contains numbers (represented as [strings](./key-terms.ipynb#string). Each text represents a raw representation of what is on the published page, so it is seems likely that these number references are in fact page numbers. If these numbers are not useful for your analysis, they can be filtered out with [stopwords](./key-terms.pynb#stop-words). JSTOR and Portico do not pre-filter out any words or numbers from corpora. \n",
    "\n",
    "![JSON section showing words](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/unigramCountWords.png)\n",
    "*On each line, a **key** on the left is matched to **value** representing its frequency on the right*\n",
    "\n",
    "Each word is treated as a [string](./key-terms.ipynb#string). Since python strings are case-sensitive so that means that \"Tiger\" is a different string than \"tiger\". Counting all the occurences of the word \"tiger\" then would require combining of the two strings. These methods are covered in later notebooks. \n",
    "\n",
    "# Can analyze an individual chapter of a book?\n",
    "\n",
    "This question mainly applies to Portico content where we do have metadata at the level of book chapters. We hope to support the analysis of individual chapters in the future. (We do have the metadata to implement this feature, so it is mostly a technical challenge at this point.)\n",
    "\n",
    "# What about open content in JSTOR?\n",
    "\n",
    "Open content is currently served as [bags of words](./key-terms.ipynb#bag-of-words), but we are planning to supply full-text. We understand that many text analysis methods require full-text, and we plan to share full-text documents to the greatest extent we are able to. One of the future sources for this content will be the [HathiTrust Digital Library](https://www.hathitrust.org/). \n",
    "\n",
    "# How is the text sourced?\n",
    "\n",
    "Most of the JSTOR content is produced through [optical character recognition (OCR)](./key-terms.ipynb#ocr) which means there are gaps and errors in the content. We are considering methods for assessing the accuracy of the OCR. \n",
    "\n",
    "Some of the Portico content is sourced from the original [XML](./key-terms.ipynb#XML) (the highest reasonable level of accuracy). \n",
    "\n",
    "# What's the difference between your data format and the Extracted Features format used by HathiTrust?\n",
    "\n",
    "We worked closely with HathiTrust to try to develop a shared data format. Ultimately, we decided to improve the [HathiTrust Extracted Features format](https://worksets.htrc.illinois.edu/context/ef_context.json) to support key features our users sought (for example, the ability to analyze texts at the level of individual journal articles instead of at the issue-level). We are excited by the volume of content found within HathiTrust and plan to include more material from them in the future.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
