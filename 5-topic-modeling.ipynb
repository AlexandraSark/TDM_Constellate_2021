{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) Topic Modeling\n",
    "\n",
    "This notebook demonstrates how to do topic modeling using the latent dirichlet allocation method. The following processes are described:\n",
    "\n",
    "* Importing your [dataset](./key-terms.ipynb#dataset)\n",
    "* Checking the import was successful with `len()` and `query()`\n",
    "* Importing libraries including `os`, `warnings`, `gensim`, `nltk`, and `pyLDAvis`\n",
    "* Writing a helper function to help clean up a single [token](./key-terms.ipynb#token)\n",
    "* Building a gensim dictionary and training the model\n",
    "* Computing a topic list\n",
    "* Visualizing the topic list\n",
    "\n",
    "This example uses the [`gensim`](https://radimrehurek.com/gensim/index.html) library for building the topic model. A familiarity with gensim is helpful but not required.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a dataset object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm_client import Dataset\n",
    "\n",
    "dset = Dataset('59c090b6-3851-3c65-e016-9181833b4a2c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find total number of documents in the dataset using the `len()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6687"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q=*%3A*&fq=yearPublished%3A%5B1700%20TO%202019%5D&fq=-provider%3Aportico&fq=isPartOf%3A(%22Shakespeare%20Quarterly%22)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for processing tokens from the extracted features for volumes in the curated dataset. This function:\n",
    "\n",
    "* lowercases all tokens\n",
    "* discards all tokens less than 4 characters\n",
    "* discards non alphabetical tokens - e.g. --9\n",
    "* removes stopwords using NLTK's stopword list\n",
    "* Lemmatizes the token using NLTK's [WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token(token):\n",
    "    token = token.lower()\n",
    "    if len(token) < 4:\n",
    "        return\n",
    "    if not(token.isalpha()):\n",
    "        return\n",
    "    if token in stop_words:\n",
    "        return\n",
    "    return WordNetLemmatizer().lemmatize(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the volumes in the dataset and make a list of tokens for each volume and then add to a list of the 25 documents in the dataset. We are limiting this example to 25 documents to limit the time it takes to run during demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for n, unigram_count in enumerate(dset.get_features()):\n",
    "    this_doc = []\n",
    "    for token, count in unigram_count.items():\n",
    "        clean_token = process_token(token)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    documents.append(this_doc)\n",
    "    if n >= 24:\n",
    "        break\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a gensim dictionary and corpus and then train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(documents)\n",
    "\n",
    "dictionary.filter_extremes(no_below=len(documents) * .10, no_above=0.5)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "\n",
    "# train model, this might take some time\n",
    "model = gensim.models.LdaModel(\n",
    "    corpus=bow_corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    passes=15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the most significant terms, as determined by the model, for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0     could festival henry michael production actor william made text romeo\n",
      "Topic 1     romantic sonnet poem poet influence author allusion discussion especially dramatic\n",
      "Topic 2     festival night account twelfth puck world harrison article state american\n",
      "Topic 3     merchant festival venice film antonio production actor must jackson number\n",
      "Topic 4     published professor issue modern library paper edited volume theater press\n",
      "Topic 5     hamlet mask literature author culture committed translation drawn love marvin\n",
      "Topic 6     festival world romeo film made modern performance production juliet midsummer\n",
      "Topic 7     romeo juliet poetic sonnet love word form scene fact though\n",
      "Topic 8     could production actor company henry made though playing attempt given\n",
      "Topic 9     catholic theology faith seems belief religious need friar mean element\n"
     ]
    }
   ],
   "source": [
    "for topic_num in range(0, num_topics):\n",
    "    word_ids = model.get_topic_terms(topic_num)\n",
    "    words = []\n",
    "    for wid, weight in word_ids:\n",
    "        word = dictionary.id2token[wid]\n",
    "        words.append(word)\n",
    "    print(\"Topic {}\".format(str(topic_num).ljust(5)), \" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the model using [`pyLDAvis`](https://pyldavis.readthedocs.io/en/latest/). This visualization takes several minutes to an hour to generate depending on the size of your dataset. To run, change do_viz below to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_viz = True\n",
    "if do_viz is True:\n",
    "    pyLDAvis.gensim.prepare(model, bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
