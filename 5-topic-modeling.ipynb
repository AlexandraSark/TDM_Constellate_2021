{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA topic modeling with a curated dataset\n",
    "\n",
    "This example uses the [`gensim`](https://radimrehurek.com/gensim/index.html) library for building the topic model.\n",
    "\n",
    "Depending on your collection size, this example will take between 10 minutes to an hour+ to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a dataset object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm_client import Dataset\n",
    "\n",
    "dset = Dataset('59c090b6-3851-3c65-e016-9181833b4a2c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6687"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q=*%3A*&fq=yearPublished%3A%5B1700%20TO%202019%5D&fq=-provider%3Aportico&fq=isPartOf%3A(%22Shakespeare%20Quarterly%22)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread tdm-download:\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw)\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\", line 80, in create_connection\n",
      "    raise err\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\", line 70, in create_connection\n",
      "    sock.connect(sa)\n",
      "TimeoutError: [Errno 60] Operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 343, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 839, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/urllib3/connection.py\", line 301, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/urllib3/connection.py\", line 168, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e)\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x1a2468bba8>: Failed to establish a new connection: [Errno 60] Operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\", line 399, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.jstor.org', port=443): Max retries exceeded with url: /api/tdm/v1/nb/dataset/features/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a2468bba8>: Failed to establish a new connection: [Errno 60] Operation timed out'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"//anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/nkelber/Google Drive/Professional/Coding Projects/tdm-notebooks/tdm_client.py\", line 229, in _populate\n",
      "    _ = self._fetch(url)\n",
      "  File \"/Users/nkelber/Google Drive/Professional/Coding Projects/tdm-notebooks/tdm_client.py\", line 252, in _fetch\n",
      "    headers=self.request_headers\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/requests/api.py\", line 116, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/requests/api.py\", line 60, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"//anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.jstor.org', port=443): Max retries exceeded with url: /api/tdm/v1/nb/dataset/features/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x1a2468bba8>: Failed to establish a new connection: [Errno 60] Operation timed out'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dset.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for processing tokens from the extracted features for volumes in the curated dataset. This function:\n",
    "\n",
    "* lowercases all tokens\n",
    "* discards all tokens less than 4 characters\n",
    "* discards non alphabetical tokens - e.g. --9\n",
    "* removes stopwords using NLTK's stopword list\n",
    "* Lemmatizes the token using NLTK's [WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token(token):\n",
    "    token = token.lower()\n",
    "    if len(token) < 4:\n",
    "        return\n",
    "    if not(token.isalpha()):\n",
    "        return\n",
    "    if token in stop_words:\n",
    "        return\n",
    "    return WordNetLemmatizer().lemmatize(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the volumes in the dataset and make a list of tokens for each volume and then add to a list of the 25 documents in the dataset. We are limiting this example to 25 documents to limit the time it takes to run during demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for n, unigram_count in enumerate(dset.get_features()):\n",
    "    this_doc = []\n",
    "    for token, count in unigram_count.items():\n",
    "        clean_token = process_token(token)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    documents.append(this_doc)\n",
    "    if n >= 24:\n",
    "        break\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a gensim dictionary and corpus and then train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=len(documents) * .10, no_above=0.5)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "\n",
    "# train model, this might take some time\n",
    "model = gensim.models.LdaModel(\n",
    "    corpus=bow_corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=10,\n",
    "    passes=15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the model using [`pyLDAvis`](https://pyldavis.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(model, bow_corpus, dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
