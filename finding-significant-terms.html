
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>What is “Term Frequency- Inverse Document Frequency” (TF-IDF)? &#8212; Lesson Repository</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  
  <h1 class="site-logo" id="site-title">Lesson Repository</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="book/intro.html">
   Demonstrating Jupyter Book
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Workshop
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="book/chapters/why-text-analysis.html">
   Day 1 - Why Text Analysis?
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="book/chapters/pybasics.html">
   Day 2 - Python Basics I
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="getting-started-with-jupyter.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="getting-started-with-jupyter.html#what-is-markdown">
     What is Markdown?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="getting-started-with-jupyter.html#header-size-1">
     Header Size 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="getting-started-with-jupyter.html#lesson-complete">
     Lesson Complete
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python-basics-1.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python-basics-1.html#expressions-and-operators">
     Expressions and Operators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python-basics-1.html#data-types-integers-floats-and-strings">
     Data Types (Integers, Floats, and Strings)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python-basics-1.html#variables">
     Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python-basics-1.html#functions">
     Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python-basics-1.html#lesson-complete">
     Lesson Complete
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="book/chapters/pybasics.html">
   Day 3 - Python Basics II
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="getting-started-with-jupyter.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="getting-started-with-jupyter.html#what-is-markdown">
     What is Markdown?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="getting-started-with-jupyter.html#header-size-1">
     Header Size 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="getting-started-with-jupyter.html#lesson-complete">
     Lesson Complete
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python-basics-1.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python-basics-1.html#expressions-and-operators">
     Expressions and Operators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python-basics-1.html#data-types-integers-floats-and-strings">
     Data Types (Integers, Floats, and Strings)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python-basics-1.html#variables">
     Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python-basics-1.html#functions">
     Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python-basics-1.html#lesson-complete">
     Lesson Complete
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="book/chapters/word-frequencies.html">
   Day 4 - Exploring Word Frequencies
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="exploring-word-frequencies.html">
     Import your dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="exploring-word-frequencies.html#apply-pre-processing-filters-if-available">
     Apply Pre-Processing Filters (if available)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="exploring-word-frequencies.html#extract-the-unigram-counts-from-the-dataset-json-file">
     Extract the Unigram Counts from the dataset JSON file
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="exploring-word-frequencies.html#find-most-common-unigrams">
     Find Most Common Unigrams
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="exploring-word-frequencies.html#clean-up-tokens">
     Clean Up Tokens
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/finding-significant-terms.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ithaka/tdm-notebooks"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/ithaka/tdm-notebooks/issues/new?title=Issue%20on%20page%20%2Ffinding-significant-terms.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/ithaka/tdm-notebooks/edit/master/finding-significant-terms.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://binder.tdm-pilot.org/v2/gh/ithaka/tdm-notebooks/master?urlpath=tree/finding-significant-terms.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   What is “Term Frequency- Inverse Document Frequency” (TF-IDF)?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#term-frequency-function">
     Term Frequency Function
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inverse-document-frequency-function">
     Inverse Document Frequency Function
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tf-idf-calculation-in-plain-english">
     TF-IDF Calculation in Plain English
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-example-calculation-of-tf-idf">
     An Example Calculation of TF-IDF
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-tf-idf-example-1">
     Computing TF-IDF (Example 1)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-tf-idf-example-2">
     Computing TF-IDF (Example 2)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-tf-idf-example-3">
     Computing TF-IDF (Example 3)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-full-tf-idf-example-table">
     The Full TF-IDF Example Table
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computing-tf-idf-with-your-dataset">
   Computing TF-IDF with your Dataset
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#apply-pre-processing-filters-if-available">
   Apply Pre-Processing Filters (if available)
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-stopwords-list">
   Load Stopwords List
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-a-unigram-processing-function">
   Define a Unigram Processing Function
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-gensim-to-compute-term-frequency-inverse-document-frequency">
   Using Gensim to Compute “Term Frequency- Inverse Document Frequency”
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-a-gensim-dictionary">
     Creating a Gensim Dictionary
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-a-bag-of-words-corpus">
     Creating a Bag of Words Corpus
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-a-single-document">
       Example: A Single Document
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-all-documents">
       Example: All Documents
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-the-tfidfmodel">
     Create the
     <code class="docutils literal notranslate">
      <span class="pre">
       TfidfModel
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><img alt="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png" class="align-left" src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png" /><br /></p>
<p>Created by <a class="reference external" href="http://nkelber.com">Nathan Kelber</a> and Ted Lawless for <a class="reference external" href="https://labs.jstor.org/">JSTOR Labs</a> under <a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons CC BY License</a><br />
For questions/comments/improvements, email <a class="reference external" href="mailto:nathan&#46;kelber&#37;&#52;&#48;ithaka&#46;org">nathan<span>&#46;</span>kelber<span>&#64;</span>ithaka<span>&#46;</span>org</a>.<br /></p>
<hr class="docutils" />
<p><strong>Finding Significant Words Using TF/IDF</strong></p>
<p><strong>Description:</strong>
This <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#jupyter-notebook">notebook</a> shows how to discover significant words. The method for finding significant terms is <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">tf-idf</a>.  The following processes are described:</p>
<ul class="simple">
<li><p>An educational overview of TF-IDF, including how it is calculated</p></li>
<li><p>Using the <code class="docutils literal notranslate"><span class="pre">tdm_client</span></code> to retrieve a dataset</p></li>
<li><p>Filtering based on a pre-processed ID list</p></li>
<li><p>Filtering based on a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#stop-words">stop words list</a></p></li>
<li><p>Cleaning the tokens in the dataset</p></li>
<li><p>Creating a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a></p></li>
<li><p>Creating a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#bag-of-words">bag of words</a> <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#corpus">corpus</a></p></li>
<li><p>Computing the most significant words in your <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#corpus">corpus</a> using <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> implementation of <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a></p></li>
</ul>
<p><strong>Use Case:</strong> For Learners (Detailed explanation, not ideal for researchers)</p>
<p><a class="reference internal" href="finding-significant-terms-for-research.html"><span class="doc std std-doc">Take me to the <strong>Research Version</strong> of this notebook -&gt;</span></a></p>
<p><strong>Difficulty:</strong> Intermediate</p>
<p><strong>Completion time:</strong> 60 minutes</p>
<p><strong>Knowledge Required:</strong></p>
<ul class="simple">
<li><p>Python Basics Series (<a class="reference internal" href="python-basics-1.html"><span class="doc std std-doc">Start Python Basics I</span></a>)</p></li>
</ul>
<p><strong>Knowledge Recommended:</strong></p>
<ul class="simple">
<li><p><span class="xref myst">Exploring Metadata</span></p></li>
<li><p><a class="reference internal" href="working-with-dataset-files.html"><span class="doc std std-doc">Working with Dataset Files</span></a></p></li>
<li><p><a class="reference internal" href="pandas-1.html"><span class="doc std std-doc">Pandas I</span></a></p></li>
<li><p><a class="reference internal" href="creating-stopwords-list.html"><span class="doc std std-doc">Creating a Stopwords List</span></a></p></li>
<li><p>A familiarity with <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> is helpful but not required.</p></li>
</ul>
<p><strong>Data Format:</strong> <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#jsonl">JSON Lines (.jsonl)</a></p>
<p><strong>Libraries Used:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pandas</span></code> to load a preprocessing list</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">csv</span></code> to load a custom stopwords list</p></li>
<li><p><a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> to help compute the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">tf-idf</a> calculations</p></li>
<li><p><a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#nltk">NLTK</a> to create a stopwords list (if no list is supplied)</p></li>
</ul>
<p><strong>Research Pipeline:</strong></p>
<ol class="simple">
<li><p>Build a dataset</p></li>
<li><p>Create a “Pre-Processing CSV” with <a class="reference internal" href="exploring-metadata.html"><span class="doc std std-doc">Exploring Metadata</span></a> (Optional)</p></li>
<li><p>Create a “Custom Stopwords List” with <a class="reference internal" href="creating-stopwords-list.html"><span class="doc std std-doc">Creating a Stopwords List</span></a> (Optional)</p></li>
<li><p>Complete the TF-IDF analysis with this notebook</p></li>
</ol>
<hr class="docutils" />
<div class="section" id="what-is-term-frequency-inverse-document-frequency-tf-idf">
<h1>What is “Term Frequency- Inverse Document Frequency” (TF-IDF)?<a class="headerlink" href="#what-is-term-frequency-inverse-document-frequency-tf-idf" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> is used in <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#machine-learning">machine learning</a> and <a class="reference external" href="https://docs.tdm-pilot.org/key-terms//#nlp">natural language processing</a> for measuring the significance of particular terms for a given document. It consists of two parts that are multiplied together:</p>
<ol class="simple">
<li><p>Term Frequency- A measure of how many times a given word appears in a document</p></li>
<li><p>Inverse Document Frequency- A measure of how many times the same word occurs in other documents within the corpus</p></li>
</ol>
<p>If we were to merely consider <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#word-frequency">word frequency</a>, the most frequent words would be common <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#function-words">function words</a> like: “the”, “and”, “of”. We could use a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#stop-words">stopwords list</a> to remove the common <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#function-words">function words</a>, but that still may not give us results that describe the unique terms in the document since the uniqueness of terms depends on the context of a larger body of documents. In other words, the same term could be significant or insignificant depending on the context. Consider these examples:</p>
<ul class="simple">
<li><p>Given a set of scientific journal articles in biology, the term “lab” may not be significant since biologists often rely on and mention labs in their research. However, if the term “lab” were to occur frequently in a history or English article, then it is likely to be significant since humanities articles rarely discuss labs.</p></li>
<li><p>If we were to look at thousands of articles in literary studies, then the term “postcolonial” may be significant for any given article. However, if were to look at a few hundred articles on the topic of “the global south,” then the term “postcolonial” may occur so frequently that it is not a significant way to differentiate between the articles.</p></li>
</ul>
<p>The <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> calculation reveals the words that are frequent in this document <strong>yet rare in other documents</strong>. The goal is to find out what is unique or remarkable about a document given the context (and <em>the given context</em> can change the results of the analysis).</p>
<p>Here is how the calculation is mathematically written:</p>
<div class="math notranslate nohighlight">
\[tfidf_{t,d} = tf_{t,d} \cdot idf_{t,D}\]</div>
<p>In plain English, this means: <strong>The value of <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> is the product (or multiplication) of a given term’s frequency multiplied by its inverse document frequency.</strong> Let’s unpack these terms one at a time.</p>
<div class="section" id="term-frequency-function">
<h2>Term Frequency Function<a class="headerlink" href="#term-frequency-function" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[tf_{t,d}\]</div>
<p>The number of times (t) a term occurs in a given document (d)</p>
</div>
<div class="section" id="inverse-document-frequency-function">
<h2>Inverse Document Frequency Function<a class="headerlink" href="#inverse-document-frequency-function" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[idf_i = \mbox{log} \frac{N}{|{d : t_i \in d}|}\]</div>
<p>The inverse document frequency can be expanded to the calculation on the right. In plain English, this means: <strong>The log of the total number of documents (N) divided by the number of documents that contain the term</strong></p>
</div>
<div class="section" id="tf-idf-calculation-in-plain-english">
<h2>TF-IDF Calculation in Plain English<a class="headerlink" href="#tf-idf-calculation-in-plain-english" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[(Times-the-word-occurs-in-given-document) \cdot \mbox{log} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}\]</div>
<p>There are variations on the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> formula, but this is the most widely-used version.</p>
</div>
<div class="section" id="an-example-calculation-of-tf-idf">
<h2>An Example Calculation of TF-IDF<a class="headerlink" href="#an-example-calculation-of-tf-idf" title="Permalink to this headline">¶</a></h2>
<p>Let’s take a look at an example to illustrate the fundamentals of <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a>. First, we need several texts to compare. Our texts will be very simple.</p>
<ul class="simple">
<li><p>text1 = ‘The grass was green and spread out the distance like the sea.’</p></li>
<li><p>text2 = ‘Green eggs and ham were spread out like the book.’</p></li>
<li><p>text3 = ‘Green sailors were met like the sea met troubles.’</p></li>
<li><p>text4 = ‘The grass was green.’</p></li>
</ul>
<p>The first step is we need to discover how many unique words are in each text.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>text1</p></th>
<th class="head"><p>text2</p></th>
<th class="head"><p>text3</p></th>
<th class="head"><p>text4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>the</p></td>
<td><p>green</p></td>
<td><p>green</p></td>
<td><p>the</p></td>
</tr>
<tr class="row-odd"><td><p>grass</p></td>
<td><p>eggs</p></td>
<td><p>sailors</p></td>
<td><p>grass</p></td>
</tr>
<tr class="row-even"><td><p>was</p></td>
<td><p>and</p></td>
<td><p>were</p></td>
<td><p>was</p></td>
</tr>
<tr class="row-odd"><td><p>green</p></td>
<td><p>ham</p></td>
<td><p>met</p></td>
<td><p>green</p></td>
</tr>
<tr class="row-even"><td><p>and</p></td>
<td><p>were</p></td>
<td><p>like</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>spread</p></td>
<td><p>spread</p></td>
<td><p>the</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>out</p></td>
<td><p>out</p></td>
<td><p>sea</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>into</p></td>
<td><p>like</p></td>
<td><p>met</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>distance</p></td>
<td><p>the</p></td>
<td><p>troubles</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>like</p></td>
<td><p>book</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>sea</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>Our four texts share some similar words. Next, we create a single list of unique words that occur across all three texts. (When we use the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> library later, we will call this list a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a>.)</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Unique Words</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>and</p></td>
</tr>
<tr class="row-odd"><td><p>book</p></td>
</tr>
<tr class="row-even"><td><p>distance</p></td>
</tr>
<tr class="row-odd"><td><p>eggs</p></td>
</tr>
<tr class="row-even"><td><p>grass</p></td>
</tr>
<tr class="row-odd"><td><p>green</p></td>
</tr>
<tr class="row-even"><td><p>ham</p></td>
</tr>
<tr class="row-odd"><td><p>like</p></td>
</tr>
<tr class="row-even"><td><p>met</p></td>
</tr>
<tr class="row-odd"><td><p>out</p></td>
</tr>
<tr class="row-even"><td><p>sailors</p></td>
</tr>
<tr class="row-odd"><td><p>sea</p></td>
</tr>
<tr class="row-even"><td><p>spread</p></td>
</tr>
<tr class="row-odd"><td><p>the</p></td>
</tr>
<tr class="row-even"><td><p>troubles</p></td>
</tr>
<tr class="row-odd"><td><p>was</p></td>
</tr>
<tr class="row-even"><td><p>were</p></td>
</tr>
</tbody>
</table>
<p>Now let’s count the occurences of each unique word in each sentence</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>word</p></th>
<th class="head"><p>text1</p></th>
<th class="head"><p>text2</p></th>
<th class="head"><p>text3</p></th>
<th class="head"><p>text4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>and</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>book</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>distance</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>eggs</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>grass</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>green</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>ham</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>like</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>met</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>2</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>out</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>sailors</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>sea</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>spread</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>the</p></td>
<td><p>3</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>troubles</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>was</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>were</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="computing-tf-idf-example-1">
<h2>Computing TF-IDF (Example 1)<a class="headerlink" href="#computing-tf-idf-example-1" title="Permalink to this headline">¶</a></h2>
<p>We have enough information now to compute <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> for every word in our corpus. Recall the plain English formula.</p>
<div class="math notranslate nohighlight">
\[(Times-the-word-occurs-in-given-document) \cdot \mbox{log} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}\]</div>
<p>We can use the formula to compute <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> for the most common word in our corpus: ‘the’. In total, we will compute <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> four times (once for each of our texts).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>word</p></th>
<th class="head"><p>text1</p></th>
<th class="head"><p>text2</p></th>
<th class="head"><p>text3</p></th>
<th class="head"><p>text4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>the</p></td>
<td><p>3</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p>text1: $<span class="math notranslate nohighlight">\( tf-idf = 3 \cdot \mbox{log} \frac{4}{(4)} = 3 \cdot \mbox{log} 1 = 3 \cdot 0 = 0\)</span><span class="math notranslate nohighlight">\(
text2: \)</span><span class="math notranslate nohighlight">\( tf-idf = 1 \cdot \mbox{log} \frac{4}{(4)} = 1 \cdot \mbox{log} 1 = 1 \cdot 0 = 0\)</span><span class="math notranslate nohighlight">\(
text3: \)</span><span class="math notranslate nohighlight">\( tf-idf = 1 \cdot \mbox{log} \frac{4}{(4)} = 1 \cdot \mbox{log} 1 = 1 \cdot 0 = 0\)</span><span class="math notranslate nohighlight">\(
text4: \)</span><span class="math notranslate nohighlight">\( tf-idf = 1 \cdot \mbox{log} \frac{4}{(4)} = 1 \cdot \mbox{log} 1 = 1 \cdot 0 = 0\)</span>$</p>
<p>The results of our analysis suggest ‘the’ has a weight of 0 in every document. The word ‘the’ exists in all of our documents, and therefore it is not a significant term to differentiate one document from another.</p>
<p>Given that idf is</p>
<div class="math notranslate nohighlight">
\[\mbox{log} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\mbox{log} 1 = 0\]</div>
<p>we can see that <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> will be 0 for any word that occurs in every document. That is, if a word occurs in every document, then it is not a significant term for any individual document.</p>
</div>
<div class="section" id="computing-tf-idf-example-2">
<h2>Computing TF-IDF (Example 2)<a class="headerlink" href="#computing-tf-idf-example-2" title="Permalink to this headline">¶</a></h2>
<p>Let’s try a second example with the word ‘out’. Recall the plain English formula.</p>
<div class="math notranslate nohighlight">
\[(Times-the-word-occurs-in-given-document) \cdot \mbox{log} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}\]</div>
<p>We will compute <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> four times, once for each of our texts.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>word</p></th>
<th class="head"><p>text1</p></th>
<th class="head"><p>text2</p></th>
<th class="head"><p>text3</p></th>
<th class="head"><p>text4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>out</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>text1: $<span class="math notranslate nohighlight">\( tf-idf = 1 \cdot \mbox{log} \frac{4}{(2)} = 1 \cdot \mbox{log} 2 = 1 \cdot .3010 = .3010\)</span><span class="math notranslate nohighlight">\(
text2: \)</span><span class="math notranslate nohighlight">\( tf-idf = 1 \cdot \mbox{log} \frac{4}{(2)} = 1 \cdot \mbox{log} 2 = 1 \cdot .3010 = .3010\)</span><span class="math notranslate nohighlight">\(
text3: \)</span><span class="math notranslate nohighlight">\( tf-idf = 0 \cdot \mbox{log} \frac{4}{(2)} = 0 \cdot \mbox{log} 2 = 0 \cdot .3010 = 0\)</span><span class="math notranslate nohighlight">\(
text4: \)</span><span class="math notranslate nohighlight">\( tf-idf = 0 \cdot \mbox{log} \frac{4}{(2)} = 0 \cdot \mbox{log} 2 = 0 \cdot .3010 = 0\)</span>$</p>
<p>The results of our analysis suggest ‘out’ has some significance in text1 and text2, but no significance for text3 and text4 where the word does not occur.</p>
</div>
<div class="section" id="computing-tf-idf-example-3">
<h2>Computing TF-IDF (Example 3)<a class="headerlink" href="#computing-tf-idf-example-3" title="Permalink to this headline">¶</a></h2>
<p>Let’s try one last example with the word ‘met’. Here’s the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> formula again:</p>
<div class="math notranslate nohighlight">
\[(Times-the-word-occurs-in-given-document) \cdot \mbox{log} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}\]</div>
<p>And here’s how many times the word ‘met’ occurs in each text.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>word</p></th>
<th class="head"><p>text1</p></th>
<th class="head"><p>text2</p></th>
<th class="head"><p>text3</p></th>
<th class="head"><p>text4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>met</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>2</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>text1: $<span class="math notranslate nohighlight">\( tf-idf = 0 \cdot \mbox{log} \frac{4}{(1)} = 0 \cdot \mbox{log} 4 = 1 \cdot .6021 = 0\)</span><span class="math notranslate nohighlight">\(
text2: \)</span><span class="math notranslate nohighlight">\( tf-idf = 0 \cdot \mbox{log} \frac{4}{(1)} = 0 \cdot \mbox{log} 4 = 1 \cdot .6021 = 0\)</span><span class="math notranslate nohighlight">\(
text3: \)</span><span class="math notranslate nohighlight">\( tf-idf = 2 \cdot \mbox{log} \frac{4}{(1)} = 2 \cdot \mbox{log} 4 = 2 \cdot .6021 = 1.2042\)</span><span class="math notranslate nohighlight">\(
text4: \)</span><span class="math notranslate nohighlight">\( tf-idf = 0 \cdot \mbox{log} \frac{4}{(1)} = 0 \cdot \mbox{log} 4 = 1 \cdot .6021 = 0\)</span>$</p>
<p>As should be expected, we can see that the word ‘met’ is very significant in text3 but not significant in any other text since it does not occur in any other text.</p>
</div>
<div class="section" id="the-full-tf-idf-example-table">
<h2>The Full TF-IDF Example Table<a class="headerlink" href="#the-full-tf-idf-example-table" title="Permalink to this headline">¶</a></h2>
<p>Here are the original sentences for each text:</p>
<ul class="simple">
<li><p>text1 = ‘The grass was green and spread out the distance like the sea.’</p></li>
<li><p>text2 = ‘Green eggs and ham were spread out like the book.’</p></li>
<li><p>text3 = ‘Green sailors were met like the sea met troubles.’</p></li>
<li><p>text4 = ‘The grass was green.’</p></li>
</ul>
<p>And here’s the corresponding <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> scores for each word in each text:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>word</p></th>
<th class="head"><p>text1</p></th>
<th class="head"><p>text2</p></th>
<th class="head"><p>text3</p></th>
<th class="head"><p>text4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>and</p></td>
<td><p>.3010</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>book</p></td>
<td><p>0</p></td>
<td><p>.6021</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>distance</p></td>
<td><p>.6021</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>eggs</p></td>
<td><p>0</p></td>
<td><p>.6021</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>grass</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>.3010</p></td>
</tr>
<tr class="row-odd"><td><p>green</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>ham</p></td>
<td><p>0</p></td>
<td><p>.6021</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>like</p></td>
<td><p>.1249</p></td>
<td><p>.1249</p></td>
<td><p>.1249</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>met</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1.2042</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>out</p></td>
<td><p>.3010</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>sailors</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>.6021</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>sea</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>spread</p></td>
<td><p>.3010</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>the</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>troubles</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>.6021</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>was</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>.3010</p></td>
</tr>
<tr class="row-even"><td><p>were</p></td>
<td><p>0</p></td>
<td><p>.3010</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>There are a few noteworthy things in this data.</p>
<ul class="simple">
<li><p>The <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> score for any word that does not occur in a text is 0.</p></li>
<li><p>The scores for almost every word in text4 are 0 since it is a shorter version of text1. There are no unique words in text4 since text1 contains all the same words. It is also a short text which means that there are only four words to consider. The words ‘the’ and ‘green’ occur in every text, leaving only ‘was’ and ‘grass’ which are also found in text1.</p></li>
<li><p>The words ‘book’, ‘eggs’, and ‘ham’ are significant in text2 since they only occur in that text.</p></li>
</ul>
<p>Now that you have a basic understanding of how <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> is computed at a small scale, let’s try computing <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> on a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#corpus">corpus</a> which could contain millions of words.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="computing-tf-idf-with-your-dataset">
<h1>Computing TF-IDF with your Dataset<a class="headerlink" href="#computing-tf-idf-with-your-dataset" title="Permalink to this headline">¶</a></h1>
<p>We’ll use the tdm_client library to automatically retrieve the dataset in the JSON file format.</p>
<p>Enter a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#dataset-ID">dataset ID</a> in the next code cell.</p>
<p>If you don’t have a dataset ID, you can:</p>
<ul class="simple">
<li><p>Use the sample dataset ID already in the code cell</p></li>
<li><p><a class="reference external" href="https://tdm-pilot.org/builder">Create a new dataset</a></p></li>
<li><p><a class="reference external" href="https://tdm-pilot.org/dataset/dashboard">Use a dataset ID from other pre-built sample datasets</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_id</span> <span class="o">=</span> <span class="s2">&quot;b4668c50-a970-c4d7-eb2c-bb6d04313542&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>Next, import the <code class="docutils literal notranslate"><span class="pre">tdm_client</span></code>, passing the <code class="docutils literal notranslate"><span class="pre">dataset_id</span></code> as an argument using the <code class="docutils literal notranslate"><span class="pre">get_dataset</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing your dataset with a dataset ID</span>
<span class="kn">import</span> <span class="nn">tdm_client</span>
<span class="c1"># Pull in the dataset that matches `dataset_id`</span>
<span class="c1"># in the form of a gzipped JSON lines file.</span>
<span class="n">dataset_file</span> <span class="o">=</span> <span class="n">tdm_client</span><span class="o">.</span><span class="n">get_dataset</span><span class="p">(</span><span class="n">dataset_id</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="apply-pre-processing-filters-if-available">
<h1>Apply Pre-Processing Filters (if available)<a class="headerlink" href="#apply-pre-processing-filters-if-available" title="Permalink to this headline">¶</a></h1>
<p>If you completed pre-processing with the “Exploring Metadata and Pre-processing” notebook, you can use your CSV file of dataset IDs to automatically filter the dataset. Your pre-processed CSV file  must be in the root folder.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import a pre-processed CSV file of filtered dataset IDs.</span>
<span class="c1"># If you do not have a pre-processed CSV file, the analysis</span>
<span class="c1"># will run on the full dataset and may take longer to complete.</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">pre_processed_file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;data/pre-processed_</span><span class="si">{</span><span class="n">dataset_id</span><span class="si">}</span><span class="s1">.csv&#39;</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">pre_processed_file_name</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">pre_processed_file_name</span><span class="p">)</span>
    <span class="n">filtered_id_list</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">use_filtered_list</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Pre-Processed CSV found. Successfully read in &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39; documents.&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span> 
    <span class="n">use_filtered_list</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;No pre-processed CSV file found. Full dataset will be used.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="load-stopwords-list">
<h1>Load Stopwords List<a class="headerlink" href="#load-stopwords-list" title="Permalink to this headline">¶</a></h1>
<p>If you have created a stopword list in the stopwords notebook, we will import it here. (You can always modify the CSV file to add or subtract words then reload the list.) Otherwise, we’ll load the NLTK <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#stop-words">stopwords</a> list automatically.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load a custom data/stop_words.csv if available</span>
<span class="c1"># Otherwise, load the nltk stopwords list in English</span>

<span class="c1"># Create an empty Python list to hold the stopwords</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># The filename of the custom data/stop_words.csv file</span>
<span class="n">stopwords_list_filename</span> <span class="o">=</span> <span class="s1">&#39;data/stop_words.csv&#39;</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">stopwords_list_filename</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">csv</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">stopwords_list_filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">stop_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">csv</span><span class="o">.</span><span class="n">reader</span><span class="p">(</span><span class="n">f</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Custom stopwords list loaded from CSV&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Load the NLTK stopwords list</span>
    <span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
    <span class="n">stop_words</span> <span class="o">=</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;NLTK stopwords list loaded&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="define-a-unigram-processing-function">
<h1>Define a Unigram Processing Function<a class="headerlink" href="#define-a-unigram-processing-function" title="Permalink to this headline">¶</a></h1>
<p>In this step, we gather the unigrams. If there is a Pre-Processing Filter, we will only analyze documents from the filtered ID list. We will also process each unigram, assessing them individually. We will complete the following tasks:</p>
<ul class="simple">
<li><p>Lowercase all tokens</p></li>
<li><p>Remove tokens in stopwords list</p></li>
<li><p>Remove tokens with fewer than 4 characters</p></li>
<li><p>Remove tokens with non-alphabetic characters</p></li>
</ul>
<p>We can define this process in a function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a function that will process individual tokens</span>
<span class="c1"># Only a token that passes through all three `if` </span>
<span class="c1"># statements will be returned. A `True` result for</span>
<span class="c1"># any `if` statement does not return the token. </span>

<span class="k">def</span> <span class="nf">process_token</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">:</span> <span class="c1"># If True, do not return token</span>
        <span class="k">return</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span> <span class="c1"># If True, do not return token</span>
        <span class="k">return</span>
    <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()):</span> <span class="c1"># If True, do not return token</span>
        <span class="k">return</span>
    <span class="k">return</span> <span class="n">token</span> <span class="c1"># If all are False, return the lowercased token</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we process all the unigrams into a list called <code class="docutils literal notranslate"><span class="pre">documents</span></code>. For demonstration purposes, this code runs on a limit of 500 documents, but we can change this to process all the documents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Collecting the unigrams and processing them into `documents`</span>

<span class="n">limit</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># Change number of documents being analyzed. Set to `None` to do all documents.</span>
<span class="c1">#limit = None</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">document_ids</span> <span class="o">=</span> <span class="p">[]</span>
    
<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">tdm_client</span><span class="o">.</span><span class="n">dataset_reader</span><span class="p">(</span><span class="n">dataset_file</span><span class="p">):</span>
    <span class="n">processed_document</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">document_id</span> <span class="o">=</span> <span class="n">document</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">use_filtered_list</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="c1"># Skip documents not in our filtered_id_list</span>
        <span class="k">if</span> <span class="n">document_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">filtered_id_list</span><span class="p">:</span>
            <span class="k">continue</span>
    <span class="n">document_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">document_id</span><span class="p">)</span>
    <span class="n">unigrams</span> <span class="o">=</span> <span class="n">document</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;unigramCount&quot;</span><span class="p">,</span> <span class="p">[])</span>
    <span class="k">for</span> <span class="n">gram</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">unigrams</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">clean_gram</span> <span class="o">=</span> <span class="n">process_token</span><span class="p">(</span><span class="n">gram</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">clean_gram</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">processed_document</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clean_gram</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">processed_document</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">processed_document</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">limit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">n</span> <span class="o">&gt;=</span> <span class="n">limit</span><span class="p">):</span>
        <span class="k">break</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Unigrams collected and processed.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have all the cleaned unigrams in a list, we can use Gensim to compute TF/IDF.</p>
</div>
<hr class="docutils" />
<div class="section" id="using-gensim-to-compute-term-frequency-inverse-document-frequency">
<h1>Using Gensim to Compute “Term Frequency- Inverse Document Frequency”<a class="headerlink" href="#using-gensim-to-compute-term-frequency-inverse-document-frequency" title="Permalink to this headline">¶</a></h1>
<p>It will be helpful to remember the basic steps we did in the explanatory <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> example:</p>
<ol class="simple">
<li><p>Create a list of the frequency of every word in every document</p></li>
<li><p>Create a list of every word in the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#corpus">corpus</a></p></li>
<li><p>Compute <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> based on that data</p></li>
</ol>
<p>So far, we have completed the first item by creating a list of the frequency of every word in every document. Now we need to create a list of every word in the corpus. In <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a>, this is called a “dictionary”. A <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> is similar to a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#python-dictionary">Python dictionary</a>, but here it is called a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> to show it is a specialized kind of dictionary.</p>
<div class="section" id="creating-a-gensim-dictionary">
<h2>Creating a Gensim Dictionary<a class="headerlink" href="#creating-a-gensim-dictionary" title="Permalink to this headline">¶</a></h2>
<p>Let’s create our <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a>. A <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> is a kind of masterlist of all the words across all the documents in our corpus. Each unique word is assigned an ID in the gensim dictionary. The result is a set of key/value pairs of unique tokens and their unique IDs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="n">dictionary</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a>, we can get a preview that displays the number of unique tokens across all of our texts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> stores a unique identifier (starting with 0) for every unique token in the corpus. The <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> does not contain information on word frequencies; it only catalogs all the words in the corpus. You can see the unique ID for each token in the text using the .token2id() method. Your corpus may have hundreds of thousands of unique words so here we just give a preview of the first ten.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">dictionary</span><span class="o">.</span><span class="n">token2id</span><span class="o">.</span><span class="n">items</span><span class="p">())[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span> <span class="c1"># Print the first ten tokens and their associated IDs.</span>
</pre></div>
</div>
</div>
</div>
<p>We can also look up the corresponding ID for a token using the <code class="docutils literal notranslate"><span class="pre">.get</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dictionary</span><span class="o">.</span><span class="n">token2id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;people&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># Get the value for the key &#39;people&#39;. Return 0 if there is no token matching &#39;people&#39;. The number returned is the gensim dictionary ID for the token. </span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="creating-a-bag-of-words-corpus">
<h2>Creating a Bag of Words Corpus<a class="headerlink" href="#creating-a-bag-of-words-corpus" title="Permalink to this headline">¶</a></h2>
<div class="section" id="example-a-single-document">
<h3>Example: A Single Document<a class="headerlink" href="#example-a-single-document" title="Permalink to this headline">¶</a></h3>
<p>The next step is to combine our word frequency data found within <code class="docutils literal notranslate"><span class="pre">documents</span></code> to our <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> token IDs. For every document, we want to know how many times a word (notated by its ID) occurs. We can do a single document first to show how this works. We will create a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#python-list">Python list</a> called <code class="docutils literal notranslate"><span class="pre">example_bow_corpus</span></code> that will turn our word counts into a series of <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tuple">tuples</a> where the first number is the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> token ID and the second number is the word frequency.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_bow_corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="c1"># Create an example bag of words corpus. We select a document at random to use as our sample.</span>
<span class="nb">list</span><span class="p">(</span><span class="n">example_bow_corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span> <span class="c1"># List out the first ten tuples in ``example_bow_corpus``</span>
</pre></div>
</div>
</div>
</div>
<p>Using IDs can seem a little abstract, but we can discover the word associated with a particular ID. For demonstration purposes, the following code will replace the token IDs in the last example with the actual tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_counts</span> <span class="o">=</span> <span class="p">[[(</span><span class="n">dictionary</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="n">count</span><span class="p">)</span> <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">line</span><span class="p">]</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">example_bow_corpus</span><span class="p">]</span>
<span class="nb">list</span><span class="p">(</span><span class="n">word_counts</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We saw before that you could discover the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> ID number by running:</p>
<blockquote>
<div><p>dictionary.token2id.get(‘people’, 0)</p>
</div></blockquote>
<p>If you wanted to discover the token given only the ID number, the method is a little more involved. You could use <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#list-comprehensions">list comprehension</a> to find the <strong>key</strong> token based on the <strong>value</strong> ID. Normally, <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#python-dictionary">Python dictionaries</a> only map from keys to values (not from values to keys). However, we can write a quick <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#list-comprehensions">list comprehension</a> to go the other direction. (It is unlikely one would ever do these methods in practice, but they are shown here to demonstrate how the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> is connected to the list entries in the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> <code class="docutils literal notranslate"><span class="pre">bow_corpus</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">dict_id</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">dict_id</span> <span class="o">==</span> <span class="mi">100</span><span class="p">]</span> <span class="c1"># Find the corresponding token in our gensim dictionary for the gensim dictionary ID</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="example-all-documents">
<h3>Example: All Documents<a class="headerlink" href="#example-all-documents" title="Permalink to this headline">¶</a></h3>
<p>We have seen an example that demonstrates how the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#bag-of-words">bag of words</a> <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#corpus">corpus</a> works on a single document. Let’s apply it now to all of our documents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bow_corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">]</span>
<span class="c1">#print(bow_corpus[:3]) #Show the bag of words corpus for the first 3 documents</span>
</pre></div>
</div>
</div>
</div>
<p>The next step is to create the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> model which will set the parameters for our implementation of <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a>. In our <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> example, the formula for <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> was:</p>
<div class="math notranslate nohighlight">
\[(Times-the-word-occurs-in-given-document) \cdot \mbox{log} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}\]</div>
<p>In <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a>, the default formula for measuring <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> uses log base 2 instead of log base 10, as shown:</p>
<div class="math notranslate nohighlight">
\[(Times-the-word-occurs-in-given-document) \cdot \log_{2} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-the-word)}\]</div>
<p>If you would like to use a different formula for your <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> calculation, there is a description of <a class="reference external" href="https://radimrehurek.com/gensim/models/tfidfmodel.html">parameters you can pass</a>.</p>
</div>
</div>
<div class="section" id="create-the-tfidfmodel">
<h2>Create the <code class="docutils literal notranslate"><span class="pre">TfidfModel</span></code><a class="headerlink" href="#create-the-tfidfmodel" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">TfidfModel</span><span class="p">(</span><span class="n">bow_corpus</span><span class="p">)</span> <span class="c1"># Create our gensim TF-IDF model</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we apply our model to the <code class="docutils literal notranslate"><span class="pre">bow_corpus</span></code> to create our results in <code class="docutils literal notranslate"><span class="pre">corpus_tfidf</span></code>. The <code class="docutils literal notranslate"><span class="pre">corpus_tfidf</span></code> is a python list of each document similar to <code class="docutils literal notranslate"><span class="pre">bow_document</span></code>. Instead of listing the frequency next to the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> ID, however, it contains the TF-IDF](<a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">https://docs.tdm-pilot.org/key-terms/#tf-idf</a>) score for the associated token. Below, we display the first document in <code class="docutils literal notranslate"><span class="pre">corpus_tfidf</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_tfidf</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="n">bow_corpus</span><span class="p">]</span> <span class="c1"># Create TF-IDF scores for the ``bow_corpus`` using our model</span>
<span class="nb">list</span><span class="p">(</span><span class="n">corpus_tfidf</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span> <span class="c1"># List out the TF-IDF scores for the first 10 tokens of the first text in the corpus</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s display the tokens instead of the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> IDs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_tfidf_scores</span> <span class="o">=</span> <span class="p">[[(</span><span class="n">dictionary</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="n">count</span><span class="p">)</span> <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">line</span><span class="p">]</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">corpus_tfidf</span><span class="p">]</span>
<span class="nb">list</span><span class="p">(</span><span class="n">example_tfidf_scores</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span> <span class="c1"># List out the TF-IDF scores for the first 10 tokens of the first text in the corpus</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, let’s sort the terms by their <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> weights to find the most significant terms in the document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sort the tuples in our tf-idf scores list</span>

<span class="k">def</span> <span class="nf">Sort</span><span class="p">(</span><span class="n">tfidf_tuples</span><span class="p">):</span> 
    <span class="n">tfidf_tuples</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">tfidf_tuples</span> 

<span class="nb">list</span><span class="p">(</span><span class="n">Sort</span><span class="p">(</span><span class="n">example_tfidf_scores</span><span class="p">[</span><span class="mi">0</span><span class="p">])[:</span><span class="mi">10</span><span class="p">])</span> <span class="c1">#List the top ten tokens in our example document by their TF-IDF scores</span>
</pre></div>
</div>
</div>
</div>
<p>We could also analyze across the entire corpus to find the most unique terms. These are terms that appear frequently in a single text, but rarely or never appear in other texts. (Often, these will be proper names since a particular article may mention a name often but the name may rarely appear in other articles.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">td</span> <span class="o">=</span> <span class="p">{</span> <span class="c1"># Define a dictionary ``td`` where each document gather</span>
        <span class="n">dictionary</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">_id</span><span class="p">):</span> <span class="n">value</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus_tfidf</span>
        <span class="k">for</span> <span class="n">_id</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">doc</span>
    <span class="p">}</span>
<span class="n">sorted_td</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">td</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">kv</span><span class="p">:</span> <span class="n">kv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># Sort the items of ``td`` into a new variable ``sorted_td``, the ``reverse`` starts from highest to lowest</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">term</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">sorted_td</span><span class="p">[:</span><span class="mi">25</span><span class="p">]:</span> <span class="c1"># Print the top 25 terms in the entire corpus</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And, finally, we can see the most significant term in every document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># For each document, print the ID, most common word, and TF/IDF score</span>

<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">corpus_tfidf</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">word_id</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">document_ids</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word_id</span><span class="p">),</span> <span class="n">score</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Nathan Kelber<br/>
        
          <div class="extra_footer">
            <div>
<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img class="license" alt="Creative Commons License" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This book is licensed under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons BY-NC-SA 4.0 License</a>. The code is licensed under a <a href="https://choosealicense.com/licenses/gpl-3.0/#">GNU General Public License v3.0</a>.
</div>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>