
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Finding Significant Words Using TF/IDF &#8212; Teaching Text Analysis with Constellate</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/constellate-beta.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Teaching Text Analysis with Constellate</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="book/intro.html">
   About
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  The Course
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="book/schedule.html">
   Course Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="book/syllabus.html">
   Course Syllabus
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Lesson Repository
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="book/all-notebooks.html">
   All Notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="book/existing-lessons.html">
   Improving existing lessons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="book/new-lessons.html">
   Creating/Suggesting new lessons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="book/keep-learning.html">
   More ways to keep learning
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference external" href="https://tdm-pilot.org">
   Constellate Platform
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://docs.tdm-pilot.org/">
   Constellate Documentation
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/">
   Text Analysis Glossary
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://jupyterbook.org/intro.html">
   Jupyter Book Documentation
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/finding-significant-terms.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ithaka/tdm-notebooks"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/ithaka/tdm-notebooks/issues/new?title=Issue%20on%20page%20%2Ffinding-significant-terms.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/ithaka/tdm-notebooks/edit/master/finding-significant-terms.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://binder.tdm-pilot.org/v2/gh/ithaka/tdm-notebooks/master?urlpath=tree/finding-significant-terms.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-term-frequency-inverse-document-frequency-tf-idf">
   What is “Term Frequency- Inverse Document Frequency” (TF-IDF)?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#term-frequency-function">
     Term Frequency Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inverse-document-frequency-function">
     Inverse Document Frequency Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tf-idf-calculation-in-plain-english">
     TF-IDF Calculation in Plain English
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-example-calculation-of-tf-idf">
     An Example Calculation of TF-IDF
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-tf-idf-example-1">
     Computing TF-IDF (Example 1)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-tf-idf-example-2">
     Computing TF-IDF (Example 2)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-tf-idf-example-3">
     Computing TF-IDF (Example 3)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-full-tf-idf-example-table">
     The Full TF-IDF Example Table
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computing-tf-idf-with-your-dataset">
   Computing TF-IDF with your Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#apply-pre-processing-filters-if-available">
   Apply Pre-Processing Filters (if available)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-stopwords-list">
   Load Stopwords List
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-a-unigram-processing-function">
   Define a Unigram Processing Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-gensim-to-compute-term-frequency-inverse-document-frequency">
   Using Gensim to Compute “Term Frequency- Inverse Document Frequency”
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-a-gensim-dictionary">
     Creating a Gensim Dictionary
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-a-bag-of-words-corpus">
   Creating a Bag of Words Corpus
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-a-single-document">
     Example: A Single Document
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-all-documents">
     Example: All Documents
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#create-the-tfidfmodel">
   Create the
   <code class="docutils literal notranslate">
    <span class="pre">
     TfidfModel
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#find-top-terms-in-a-single-document">
   Find Top Terms in a Single Document
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#display-top-result-for-all-documents">
   Display Top Result for All Documents
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><img alt="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png" class="align-left" src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png" /><br /></p>
<p>Created by <a class="reference external" href="http://nkelber.com">Nathan Kelber</a> and Ted Lawless for <a class="reference external" href="https://labs.jstor.org/">JSTOR Labs</a> under <a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons CC BY License</a><br />
For questions/comments/improvements, email <a class="reference external" href="mailto:nathan&#46;kelber&#37;&#52;&#48;ithaka&#46;org">nathan<span>&#46;</span>kelber<span>&#64;</span>ithaka<span>&#46;</span>org</a>.<br /></p>
<hr class="docutils" />
<div class="section" id="finding-significant-words-using-tf-idf">
<h1>Finding Significant Words Using TF/IDF<a class="headerlink" href="#finding-significant-words-using-tf-idf" title="Permalink to this headline">¶</a></h1>
<p><strong>Description:</strong>
This <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#jupyter-notebook">notebook</a> shows how to discover significant words. The method for finding significant terms is <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">tf-idf</a>.  The following processes are described:</p>
<ul class="simple">
<li><p>An educational overview of TF-IDF, including how it is calculated</p></li>
<li><p>Using the <code class="docutils literal notranslate"><span class="pre">tdm_client</span></code> to retrieve a dataset</p></li>
<li><p>Filtering based on a pre-processed ID list</p></li>
<li><p>Filtering based on a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#stop-words">stop words list</a></p></li>
<li><p>Cleaning the tokens in the dataset</p></li>
<li><p>Creating a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a></p></li>
<li><p>Creating a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#bag-of-words">bag of words</a> <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#corpus">corpus</a></p></li>
<li><p>Computing the most significant words in your <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#corpus">corpus</a> using <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> implementation of <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a></p></li>
</ul>
<p><strong>Use Case:</strong> For Learners (Detailed explanation, not ideal for researchers)</p>
<p><a class="reference internal" href="finding-significant-terms-for-research.html"><span class="doc std std-doc">Take me to the <strong>Research Version</strong> of this notebook -&gt;</span></a></p>
<p><strong>Difficulty:</strong> Intermediate</p>
<p><strong>Completion time:</strong> 60 minutes</p>
<p><strong>Knowledge Required:</strong></p>
<ul class="simple">
<li><p>Python Basics Series (<a class="reference internal" href="python-basics-1.html"><span class="doc std std-doc">Start Python Basics I</span></a>)</p></li>
</ul>
<p><strong>Knowledge Recommended:</strong></p>
<ul class="simple">
<li><p><span class="xref myst">Exploring Metadata</span></p></li>
<li><p><a class="reference internal" href="working-with-dataset-files.html"><span class="doc std std-doc">Working with Dataset Files</span></a></p></li>
<li><p><a class="reference internal" href="pandas-1.html"><span class="doc std std-doc">Pandas I</span></a></p></li>
<li><p><a class="reference internal" href="creating-stopwords-list.html"><span class="doc std std-doc">Creating a Stopwords List</span></a></p></li>
<li><p>A familiarity with <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> is helpful but not required.</p></li>
</ul>
<p><strong>Data Format:</strong> <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#jsonl">JSON Lines (.jsonl)</a></p>
<p><strong>Libraries Used:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pandas</span></code> to load a preprocessing list</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">csv</span></code> to load a custom stopwords list</p></li>
<li><p><a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> to help compute the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">tf-idf</a> calculations</p></li>
<li><p><a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#nltk">NLTK</a> to create a stopwords list (if no list is supplied)</p></li>
</ul>
<p><strong>Research Pipeline:</strong></p>
<ol class="simple">
<li><p>Build a dataset</p></li>
<li><p>Create a “Pre-Processing CSV” with <a class="reference internal" href="exploring-metadata.html"><span class="doc std std-doc">Exploring Metadata</span></a> (Optional)</p></li>
<li><p>Create a “Custom Stopwords List” with <a class="reference internal" href="creating-stopwords-list.html"><span class="doc std std-doc">Creating a Stopwords List</span></a> (Optional)</p></li>
<li><p>Complete the TF-IDF analysis with this notebook</p></li>
</ol>
<hr class="docutils" />
<div class="section" id="what-is-term-frequency-inverse-document-frequency-tf-idf">
<h2>What is “Term Frequency- Inverse Document Frequency” (TF-IDF)?<a class="headerlink" href="#what-is-term-frequency-inverse-document-frequency-tf-idf" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> is used in <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#machine-learning">machine learning</a> and <a class="reference external" href="https://docs.tdm-pilot.org/key-terms//#nlp">natural language processing</a> for measuring the significance of particular terms for a given document. It consists of two parts that are multiplied together:</p>
<ol class="simple">
<li><p>Term Frequency- A measure of how many times a given word appears in a document</p></li>
<li><p>Inverse Document Frequency- A measure of how many times the same word occurs in other documents within the corpus</p></li>
</ol>
<p>If we were to merely consider <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#word-frequency">word frequency</a>, the most frequent words would be common <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#function-words">function words</a> like: “the”, “and”, “of”. We could use a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#stop-words">stopwords list</a> to remove the common <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#function-words">function words</a>, but that still may not give us results that describe the unique terms in the document since the uniqueness of terms depends on the context of a larger body of documents. In other words, the same term could be significant or insignificant depending on the context. Consider these examples:</p>
<ul class="simple">
<li><p>Given a set of scientific journal articles in biology, the term “lab” may not be significant since biologists often rely on and mention labs in their research. However, if the term “lab” were to occur frequently in a history or English article, then it is likely to be significant since humanities articles rarely discuss labs.</p></li>
<li><p>If we were to look at thousands of articles in literary studies, then the term “postcolonial” may be significant for any given article. However, if were to look at a few hundred articles on the topic of “the global south,” then the term “postcolonial” may occur so frequently that it is not a significant way to differentiate between the articles.</p></li>
</ul>
<p>The <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> calculation reveals the words that are frequent in this document <strong>yet rare in other documents</strong>. The goal is to find out what is unique or remarkable about a document given the context (and <em>the given context</em> can change the results of the analysis).</p>
<p>Here is how the calculation is mathematically written:</p>
<div class="math notranslate nohighlight">
\[tfidf_{t,d} = tf_{t,d} \cdot idf_{t,D}\]</div>
<p>In plain English, this means: <strong>The value of <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> is the product (or multiplication) of a given term’s frequency multiplied by its inverse document frequency.</strong> Let’s unpack these terms one at a time.</p>
<div class="section" id="term-frequency-function">
<h3>Term Frequency Function<a class="headerlink" href="#term-frequency-function" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[tf_{t,d}\]</div>
<p>The number of times (t) a term occurs in a given document (d)</p>
</div>
<div class="section" id="inverse-document-frequency-function">
<h3>Inverse Document Frequency Function<a class="headerlink" href="#inverse-document-frequency-function" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[idf_i = \mbox{log} \frac{N}{|{d : t_i \in d}|}\]</div>
<p>The inverse document frequency can be expanded to the calculation on the right. In plain English, this means: <strong>The log of the total number of documents (N) divided by the number of documents that contain the term</strong></p>
</div>
<div class="section" id="tf-idf-calculation-in-plain-english">
<h3>TF-IDF Calculation in Plain English<a class="headerlink" href="#tf-idf-calculation-in-plain-english" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[(Times-the-word-occurs-in-given-document) \cdot \mbox{log} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}\]</div>
<p>There are variations on the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> formula, but this is the most widely-used version.</p>
</div>
<div class="section" id="an-example-calculation-of-tf-idf">
<h3>An Example Calculation of TF-IDF<a class="headerlink" href="#an-example-calculation-of-tf-idf" title="Permalink to this headline">¶</a></h3>
<p>Let’s take a look at an example to illustrate the fundamentals of <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a>. First, we need several texts to compare. Our texts will be very simple.</p>
<ul class="simple">
<li><p>text1 = ‘The grass was green and spread out the distance like the sea.’</p></li>
<li><p>text2 = ‘Green eggs and ham were spread out like the book.’</p></li>
<li><p>text3 = ‘Green sailors were met like the sea met troubles.’</p></li>
<li><p>text4 = ‘The grass was green.’</p></li>
</ul>
<p>The first step is we need to discover how many unique words are in each text.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>text1</p></th>
<th class="head"><p>text2</p></th>
<th class="head"><p>text3</p></th>
<th class="head"><p>text4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>the</p></td>
<td><p>green</p></td>
<td><p>green</p></td>
<td><p>the</p></td>
</tr>
<tr class="row-odd"><td><p>grass</p></td>
<td><p>eggs</p></td>
<td><p>sailors</p></td>
<td><p>grass</p></td>
</tr>
<tr class="row-even"><td><p>was</p></td>
<td><p>and</p></td>
<td><p>were</p></td>
<td><p>was</p></td>
</tr>
<tr class="row-odd"><td><p>green</p></td>
<td><p>ham</p></td>
<td><p>met</p></td>
<td><p>green</p></td>
</tr>
<tr class="row-even"><td><p>and</p></td>
<td><p>were</p></td>
<td><p>like</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>spread</p></td>
<td><p>spread</p></td>
<td><p>the</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>out</p></td>
<td><p>out</p></td>
<td><p>sea</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>into</p></td>
<td><p>like</p></td>
<td><p>met</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>distance</p></td>
<td><p>the</p></td>
<td><p>troubles</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>like</p></td>
<td><p>book</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>sea</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>Our four texts share some similar words. Next, we create a single list of unique words that occur across all three texts. (When we use the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> library later, we will call this list a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a>.)</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Unique Words</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>and</p></td>
</tr>
<tr class="row-odd"><td><p>book</p></td>
</tr>
<tr class="row-even"><td><p>distance</p></td>
</tr>
<tr class="row-odd"><td><p>eggs</p></td>
</tr>
<tr class="row-even"><td><p>grass</p></td>
</tr>
<tr class="row-odd"><td><p>green</p></td>
</tr>
<tr class="row-even"><td><p>ham</p></td>
</tr>
<tr class="row-odd"><td><p>like</p></td>
</tr>
<tr class="row-even"><td><p>met</p></td>
</tr>
<tr class="row-odd"><td><p>out</p></td>
</tr>
<tr class="row-even"><td><p>sailors</p></td>
</tr>
<tr class="row-odd"><td><p>sea</p></td>
</tr>
<tr class="row-even"><td><p>spread</p></td>
</tr>
<tr class="row-odd"><td><p>the</p></td>
</tr>
<tr class="row-even"><td><p>troubles</p></td>
</tr>
<tr class="row-odd"><td><p>was</p></td>
</tr>
<tr class="row-even"><td><p>were</p></td>
</tr>
</tbody>
</table>
<p>Now let’s count the occurences of each unique word in each sentence</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>word</p></th>
<th class="head"><p>text1</p></th>
<th class="head"><p>text2</p></th>
<th class="head"><p>text3</p></th>
<th class="head"><p>text4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>and</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>book</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>distance</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>eggs</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>grass</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>green</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>ham</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>like</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>met</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>2</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>out</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>sailors</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>sea</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>spread</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>the</p></td>
<td><p>3</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>troubles</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>was</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>were</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="computing-tf-idf-example-1">
<h3>Computing TF-IDF (Example 1)<a class="headerlink" href="#computing-tf-idf-example-1" title="Permalink to this headline">¶</a></h3>
<p>We have enough information now to compute <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> for every word in our corpus. Recall the plain English formula.</p>
<div class="math notranslate nohighlight">
\[(Times-the-word-occurs-in-given-document) \cdot \mbox{log} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}\]</div>
<p>We can use the formula to compute <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> for the most common word in our corpus: ‘the’. In total, we will compute <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> four times (once for each of our texts).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>word</p></th>
<th class="head"><p>text1</p></th>
<th class="head"><p>text2</p></th>
<th class="head"><p>text3</p></th>
<th class="head"><p>text4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>the</p></td>
<td><p>3</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p>text1: $<span class="math notranslate nohighlight">\( tf-idf = 3 \cdot \mbox{log} \frac{4}{(4)} = 3 \cdot \mbox{log} 1 = 3 \cdot 0 = 0\)</span><span class="math notranslate nohighlight">\(
text2: \)</span><span class="math notranslate nohighlight">\( tf-idf = 1 \cdot \mbox{log} \frac{4}{(4)} = 1 \cdot \mbox{log} 1 = 1 \cdot 0 = 0\)</span><span class="math notranslate nohighlight">\(
text3: \)</span><span class="math notranslate nohighlight">\( tf-idf = 1 \cdot \mbox{log} \frac{4}{(4)} = 1 \cdot \mbox{log} 1 = 1 \cdot 0 = 0\)</span><span class="math notranslate nohighlight">\(
text4: \)</span><span class="math notranslate nohighlight">\( tf-idf = 1 \cdot \mbox{log} \frac{4}{(4)} = 1 \cdot \mbox{log} 1 = 1 \cdot 0 = 0\)</span>$</p>
<p>The results of our analysis suggest ‘the’ has a weight of 0 in every document. The word ‘the’ exists in all of our documents, and therefore it is not a significant term to differentiate one document from another.</p>
<p>Given that idf is</p>
<div class="math notranslate nohighlight">
\[\mbox{log} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\mbox{log} 1 = 0\]</div>
<p>we can see that <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> will be 0 for any word that occurs in every document. That is, if a word occurs in every document, then it is not a significant term for any individual document.</p>
</div>
<div class="section" id="computing-tf-idf-example-2">
<h3>Computing TF-IDF (Example 2)<a class="headerlink" href="#computing-tf-idf-example-2" title="Permalink to this headline">¶</a></h3>
<p>Let’s try a second example with the word ‘out’. Recall the plain English formula.</p>
<div class="math notranslate nohighlight">
\[(Times-the-word-occurs-in-given-document) \cdot \mbox{log} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}\]</div>
<p>We will compute <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> four times, once for each of our texts.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>word</p></th>
<th class="head"><p>text1</p></th>
<th class="head"><p>text2</p></th>
<th class="head"><p>text3</p></th>
<th class="head"><p>text4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>out</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>text1: $<span class="math notranslate nohighlight">\( tf-idf = 1 \cdot \mbox{log} \frac{4}{(2)} = 1 \cdot \mbox{log} 2 = 1 \cdot .3010 = .3010\)</span><span class="math notranslate nohighlight">\(
text2: \)</span><span class="math notranslate nohighlight">\( tf-idf = 1 \cdot \mbox{log} \frac{4}{(2)} = 1 \cdot \mbox{log} 2 = 1 \cdot .3010 = .3010\)</span><span class="math notranslate nohighlight">\(
text3: \)</span><span class="math notranslate nohighlight">\( tf-idf = 0 \cdot \mbox{log} \frac{4}{(2)} = 0 \cdot \mbox{log} 2 = 0 \cdot .3010 = 0\)</span><span class="math notranslate nohighlight">\(
text4: \)</span><span class="math notranslate nohighlight">\( tf-idf = 0 \cdot \mbox{log} \frac{4}{(2)} = 0 \cdot \mbox{log} 2 = 0 \cdot .3010 = 0\)</span>$</p>
<p>The results of our analysis suggest ‘out’ has some significance in text1 and text2, but no significance for text3 and text4 where the word does not occur.</p>
</div>
<div class="section" id="computing-tf-idf-example-3">
<h3>Computing TF-IDF (Example 3)<a class="headerlink" href="#computing-tf-idf-example-3" title="Permalink to this headline">¶</a></h3>
<p>Let’s try one last example with the word ‘met’. Here’s the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> formula again:</p>
<div class="math notranslate nohighlight">
\[(Times-the-word-occurs-in-given-document) \cdot \mbox{log} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}\]</div>
<p>And here’s how many times the word ‘met’ occurs in each text.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>word</p></th>
<th class="head"><p>text1</p></th>
<th class="head"><p>text2</p></th>
<th class="head"><p>text3</p></th>
<th class="head"><p>text4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>met</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>2</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>text1: $<span class="math notranslate nohighlight">\( tf-idf = 0 \cdot \mbox{log} \frac{4}{(1)} = 0 \cdot \mbox{log} 4 = 1 \cdot .6021 = 0\)</span><span class="math notranslate nohighlight">\(
text2: \)</span><span class="math notranslate nohighlight">\( tf-idf = 0 \cdot \mbox{log} \frac{4}{(1)} = 0 \cdot \mbox{log} 4 = 1 \cdot .6021 = 0\)</span><span class="math notranslate nohighlight">\(
text3: \)</span><span class="math notranslate nohighlight">\( tf-idf = 2 \cdot \mbox{log} \frac{4}{(1)} = 2 \cdot \mbox{log} 4 = 2 \cdot .6021 = 1.2042\)</span><span class="math notranslate nohighlight">\(
text4: \)</span><span class="math notranslate nohighlight">\( tf-idf = 0 \cdot \mbox{log} \frac{4}{(1)} = 0 \cdot \mbox{log} 4 = 1 \cdot .6021 = 0\)</span>$</p>
<p>As should be expected, we can see that the word ‘met’ is very significant in text3 but not significant in any other text since it does not occur in any other text.</p>
</div>
<div class="section" id="the-full-tf-idf-example-table">
<h3>The Full TF-IDF Example Table<a class="headerlink" href="#the-full-tf-idf-example-table" title="Permalink to this headline">¶</a></h3>
<p>Here are the original sentences for each text:</p>
<ul class="simple">
<li><p>text1 = ‘The grass was green and spread out the distance like the sea.’</p></li>
<li><p>text2 = ‘Green eggs and ham were spread out like the book.’</p></li>
<li><p>text3 = ‘Green sailors were met like the sea met troubles.’</p></li>
<li><p>text4 = ‘The grass was green.’</p></li>
</ul>
<p>And here’s the corresponding <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> scores for each word in each text:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>word</p></th>
<th class="head"><p>text1</p></th>
<th class="head"><p>text2</p></th>
<th class="head"><p>text3</p></th>
<th class="head"><p>text4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>and</p></td>
<td><p>.3010</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>book</p></td>
<td><p>0</p></td>
<td><p>.6021</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>distance</p></td>
<td><p>.6021</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>eggs</p></td>
<td><p>0</p></td>
<td><p>.6021</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>grass</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>.3010</p></td>
</tr>
<tr class="row-odd"><td><p>green</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>ham</p></td>
<td><p>0</p></td>
<td><p>.6021</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>like</p></td>
<td><p>.1249</p></td>
<td><p>.1249</p></td>
<td><p>.1249</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>met</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1.2042</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>out</p></td>
<td><p>.3010</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>sailors</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>.6021</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>sea</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>spread</p></td>
<td><p>.3010</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>the</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>troubles</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>.6021</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>was</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>.3010</p></td>
</tr>
<tr class="row-even"><td><p>were</p></td>
<td><p>0</p></td>
<td><p>.3010</p></td>
<td><p>.3010</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>There are a few noteworthy things in this data.</p>
<ul class="simple">
<li><p>The <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> score for any word that does not occur in a text is 0.</p></li>
<li><p>The scores for almost every word in text4 are 0 since it is a shorter version of text1. There are no unique words in text4 since text1 contains all the same words. It is also a short text which means that there are only four words to consider. The words ‘the’ and ‘green’ occur in every text, leaving only ‘was’ and ‘grass’ which are also found in text1.</p></li>
<li><p>The words ‘book’, ‘eggs’, and ‘ham’ are significant in text2 since they only occur in that text.</p></li>
</ul>
<p>Now that you have a basic understanding of how <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> is computed at a small scale, let’s try computing <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> on a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#corpus">corpus</a> which could contain millions of words.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="computing-tf-idf-with-your-dataset">
<h2>Computing TF-IDF with your Dataset<a class="headerlink" href="#computing-tf-idf-with-your-dataset" title="Permalink to this headline">¶</a></h2>
<p>We’ll use the tdm_client library to automatically retrieve the dataset in the JSON file format.</p>
<p>Enter a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#dataset-ID">dataset ID</a> in the next code cell.</p>
<p>If you don’t have a dataset ID, you can:</p>
<ul class="simple">
<li><p>Use the sample dataset ID already in the code cell</p></li>
<li><p><a class="reference external" href="https://tdm-pilot.org/builder">Create a new dataset</a></p></li>
<li><p><a class="reference external" href="https://tdm-pilot.org/dataset/dashboard">Use a dataset ID from other pre-built sample datasets</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_id</span> <span class="o">=</span> <span class="s2">&quot;d16f6e75-ff6f-3abd-0706-52e73c05df2c&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>Next, import the <code class="docutils literal notranslate"><span class="pre">tdm_client</span></code>, passing the <code class="docutils literal notranslate"><span class="pre">dataset_id</span></code> as an argument using the <code class="docutils literal notranslate"><span class="pre">get_dataset</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing your dataset with a dataset ID</span>
<span class="kn">import</span> <span class="nn">tdm_client</span>
<span class="c1"># Pull in the dataset that matches `dataset_id`</span>
<span class="c1"># in the form of a gzipped JSON lines file.</span>
<span class="n">dataset_file</span> <span class="o">=</span> <span class="n">tdm_client</span><span class="o">.</span><span class="n">get_dataset</span><span class="p">(</span><span class="n">dataset_id</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><p>All documents published in Signs from 1970 - 2021</p><p>3953 documents.</p></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:root:Downloading d16f6e75-ff6f-3abd-0706-52e73c05df2c to d16f6e75-ff6f-3abd-0706-52e73c05df2c.jsonl.gz
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100% |########################################################################|
</pre></div>
</div>
<div class="output text_html">Use and download of datasets is covered by the <a target="_blank" href="https://tdm-pilot.org/terms-and-conditions/">Terms & Conditions of Use</a></div></div>
</div>
</div>
<div class="section" id="apply-pre-processing-filters-if-available">
<h2>Apply Pre-Processing Filters (if available)<a class="headerlink" href="#apply-pre-processing-filters-if-available" title="Permalink to this headline">¶</a></h2>
<p>If you completed pre-processing with the “Exploring Metadata and Pre-processing” notebook, you can use your CSV file of dataset IDs to automatically filter the dataset. Your pre-processed CSV file  must be in the root folder.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import a pre-processed CSV file of filtered dataset IDs.</span>
<span class="c1"># If you do not have a pre-processed CSV file, the analysis</span>
<span class="c1"># will run on the full dataset and may take longer to complete.</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">pre_processed_file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;data/pre-processed_</span><span class="si">{</span><span class="n">dataset_id</span><span class="si">}</span><span class="s1">.csv&#39;</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">pre_processed_file_name</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">pre_processed_file_name</span><span class="p">)</span>
    <span class="n">filtered_id_list</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">use_filtered_list</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Pre-Processed CSV found. Successfully read in &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39; documents.&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span> 
    <span class="n">use_filtered_list</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;No pre-processed CSV file found. Full dataset will be used.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>No pre-processed CSV file found. Full dataset will be used.
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="load-stopwords-list">
<h2>Load Stopwords List<a class="headerlink" href="#load-stopwords-list" title="Permalink to this headline">¶</a></h2>
<p>If you have created a stopword list in the stopwords notebook, we will import it here. (You can always modify the CSV file to add or subtract words then reload the list.) Otherwise, we’ll load the NLTK <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#stop-words">stopwords</a> list automatically.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load a custom data/stop_words.csv if available</span>
<span class="c1"># Otherwise, load the nltk stopwords list in English</span>

<span class="c1"># Create an empty Python list to hold the stopwords</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># The filename of the custom data/stop_words.csv file</span>
<span class="n">stopwords_list_filename</span> <span class="o">=</span> <span class="s1">&#39;data/stop_words.csv&#39;</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">stopwords_list_filename</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">csv</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">stopwords_list_filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">stop_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">csv</span><span class="o">.</span><span class="n">reader</span><span class="p">(</span><span class="n">f</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Custom stopwords list loaded from CSV&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Load the NLTK stopwords list</span>
    <span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
    <span class="n">stop_words</span> <span class="o">=</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;NLTK stopwords list loaded&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NLTK stopwords list loaded
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="define-a-unigram-processing-function">
<h2>Define a Unigram Processing Function<a class="headerlink" href="#define-a-unigram-processing-function" title="Permalink to this headline">¶</a></h2>
<p>In this step, we gather the unigrams. If there is a Pre-Processing Filter, we will only analyze documents from the filtered ID list. We will also process each unigram, assessing them individually. We will complete the following tasks:</p>
<ul class="simple">
<li><p>Lowercase all tokens</p></li>
<li><p>Remove tokens in stopwords list</p></li>
<li><p>Remove tokens with fewer than 4 characters</p></li>
<li><p>Remove tokens with non-alphabetic characters</p></li>
</ul>
<p>We can define this process in a function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a function that will process individual tokens</span>
<span class="c1"># Only a token that passes through all three `if` </span>
<span class="c1"># statements will be returned. A `True` result for</span>
<span class="c1"># any `if` statement does not return the token. </span>

<span class="k">def</span> <span class="nf">process_token</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">:</span> <span class="c1"># If True, do not return token</span>
        <span class="k">return</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span> <span class="c1"># If True, do not return token</span>
        <span class="k">return</span>
    <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()):</span> <span class="c1"># If True, do not return token</span>
        <span class="k">return</span>
    <span class="k">return</span> <span class="n">token</span> <span class="c1"># If all are False, return the lowercased token</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we process all the unigrams into a list called <code class="docutils literal notranslate"><span class="pre">documents</span></code>. For demonstration purposes, this code runs on a limit of 500 documents, but we can change this to process all the documents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Collecting the unigrams and processing them into `documents`</span>

<span class="n">limit</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># Change number of documents being analyzed. Set to `None` to do all documents.</span>
<span class="c1"># limit = None</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">document_ids</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">document_titles</span> <span class="o">=</span> <span class="p">[]</span>
    
<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">tdm_client</span><span class="o">.</span><span class="n">dataset_reader</span><span class="p">(</span><span class="n">dataset_file</span><span class="p">):</span>
    <span class="n">processed_document</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">document_id</span> <span class="o">=</span> <span class="n">document</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span>
    <span class="n">document_title</span> <span class="o">=</span> <span class="n">document</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">use_filtered_list</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="c1"># Skip documents not in our filtered_id_list</span>
        <span class="k">if</span> <span class="n">document_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">filtered_id_list</span><span class="p">:</span>
            <span class="k">continue</span>
    <span class="n">document_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">document_id</span><span class="p">)</span>
    <span class="n">document_titles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">document_title</span><span class="p">)</span>  <span class="c1"># TESTING</span>
    <span class="n">unigrams</span> <span class="o">=</span> <span class="n">document</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;unigramCount&quot;</span><span class="p">,</span> <span class="p">[])</span>
    <span class="k">for</span> <span class="n">gram</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">unigrams</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">clean_gram</span> <span class="o">=</span> <span class="n">process_token</span><span class="p">(</span><span class="n">gram</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">clean_gram</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">processed_document</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clean_gram</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">processed_document</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">processed_document</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">limit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">n</span> <span class="o">&gt;=</span> <span class="n">limit</span><span class="p">):</span>
        <span class="k">break</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Unigrams collected and processed.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Unigrams collected and processed.
</pre></div>
</div>
</div>
</div>
<p>At this point, we have word counts for all our documents collected in the <code class="docutils literal notranslate"><span class="pre">documents</span></code> list. We could see the word frequencies for any given documents by using its index.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import counter to help count word frequencies</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="c1"># Use n to represent a particular document index</span>
<span class="c1"># Change n to see another document</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">6</span>

<span class="n">word_freq</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">documents</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>
<span class="n">word_freq</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;papers&#39;, 2),
 (&#39;prize&#39;, 2),
 (&#39;guidelines&#39;, 2),
 (&#39;feminist&#39;, 2),
 (&#39;leading&#39;, 1),
 (&#39;sure&#39;, 1),
 (&#39;since&#39;, 1),
 (&#39;upon&#39;, 1),
 (&#39;recognize&#39;, 1),
 (&#39;essay&#39;, 1),
 (&#39;paper&#39;, 1),
 (&#39;notes&#39;, 1),
 (&#39;article&#39;, 1),
 (&#39;broad&#39;, 1),
 (&#39;press&#39;, 1),
 (&#39;chicago&#39;, 1),
 (&#39;catharine&#39;, 1),
 (&#39;careers&#39;, 1),
 (&#39;economy&#39;, 1),
 (&#39;globe&#39;, 1),
 (&#39;honor&#39;, 1),
 (&#39;deadline&#39;, 1),
 (&#39;announce&#39;, 1),
 (&#39;falls&#39;, 1),
 (&#39;times&#39;, 1),
 (&#39;around&#39;, 1),
 (&#39;spring&#39;, 1),
 (&#39;competition&#39;, 1),
 (&#39;submissions&#39;, 1),
 (&#39;online&#39;, 1),
 (&#39;journal&#39;, 1),
 (&#39;biennially&#39;, 1),
 (&#39;provided&#39;, 1),
 (&#39;must&#39;, 1),
 (&#39;rubric&#39;, 1),
 (&#39;named&#39;, 1),
 (&#39;interdisciplinary&#39;, 1),
 (&#39;submitting&#39;, 1),
 (&#39;type&#39;, 1),
 (&#39;awarded&#39;, 1),
 (&#39;available&#39;, 1),
 (&#39;seven&#39;, 1),
 (&#39;consideration&#39;, 1),
 (&#39;issue&#39;, 1),
 (&#39;innovation&#39;, 1),
 (&#39;stimpson&#39;, 1),
 (&#39;published&#39;, 1),
 (&#39;review&#39;, 1),
 (&#39;submitted&#39;, 1),
 (&#39;longer&#39;, 1),
 (&#39;women&#39;, 1),
 (&#39;publication&#39;, 1),
 (&#39;possible&#39;, 1),
 (&#39;submission&#39;, 1),
 (&#39;international&#39;, 1),
 (&#39;peer&#39;, 1),
 (&#39;considered&#39;, 1),
 (&#39;excellence&#39;, 1),
 (&#39;anna&#39;, 1),
 (&#39;sexual&#39;, 1),
 (&#39;march&#39;, 1),
 (&#39;major&#39;, 1),
 (&#39;inquiries&#39;, 1),
 (&#39;barter&#39;, 1),
 (&#39;university&#39;, 1),
 (&#39;scholars&#39;, 1),
 (&#39;send&#39;, 1),
 (&#39;culture&#39;, 1),
 (&#39;best&#39;, 1),
 (&#39;cover&#39;, 1),
 (&#39;prizewinning&#39;, 1),
 (&#39;conform&#39;, 1),
 (&#39;author&#39;, 1),
 (&#39;receipt&#39;, 1),
 (&#39;honorarium&#39;, 1),
 (&#39;topic&#39;, 1),
 (&#39;emerging&#39;, 1),
 (&#39;work&#39;, 1),
 (&#39;editor&#39;, 1),
 (&#39;outstanding&#39;, 1),
 (&#39;theresienstadt&#39;, 1),
 (&#39;designed&#39;, 1),
 (&#39;winner&#39;, 1),
 (&#39;words&#39;, 1),
 (&#39;founding&#39;, 1),
 (&#39;early&#39;, 1),
 (&#39;negotiating&#39;, 1),
 (&#39;terminal&#39;, 1),
 (&#39;pleased&#39;, 1),
 (&#39;congratulations&#39;, 1),
 (&#39;appears&#39;, 1),
 (&#39;indicate&#39;, 1),
 (&#39;submit&#39;, 1),
 (&#39;please&#39;, 1),
 (&#39;select&#39;, 1),
 (&#39;invited&#39;, 1),
 (&#39;years&#39;, 1),
 (&#39;signs&#39;, 1)]
</pre></div>
</div>
</div>
</div>
<p>Now that we have all the cleaned unigrams for every document in a list called <code class="docutils literal notranslate"><span class="pre">documents</span></code>, we can use Gensim to compute TF/IDF.</p>
</div>
<hr class="docutils" />
<div class="section" id="using-gensim-to-compute-term-frequency-inverse-document-frequency">
<h2>Using Gensim to Compute “Term Frequency- Inverse Document Frequency”<a class="headerlink" href="#using-gensim-to-compute-term-frequency-inverse-document-frequency" title="Permalink to this headline">¶</a></h2>
<p>It will be helpful to remember the basic steps we did in the explanatory <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> example:</p>
<ol class="simple">
<li><p>Create a list of the frequency of every word in every document</p></li>
<li><p>Create a list of every word in the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#corpus">corpus</a></p></li>
<li><p>Compute <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> based on that data</p></li>
</ol>
<p>So far, we have completed the first item by creating a list of the frequency of every word in every document. Now we need to create a list of every word in the corpus. In <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a>, this is called a “dictionary”. A <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> is similar to a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#python-dictionary">Python dictionary</a>, but here it is called a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> to show it is a specialized kind of dictionary.</p>
<div class="section" id="creating-a-gensim-dictionary">
<h3>Creating a Gensim Dictionary<a class="headerlink" href="#creating-a-gensim-dictionary" title="Permalink to this headline">¶</a></h3>
<p>Let’s create our <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a>. A <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> is a kind of masterlist of all the words across all the documents in our corpus. Each unique word is assigned an ID in the gensim dictionary. The result is a set of key/value pairs of unique tokens and their unique IDs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="n">dictionary</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])
INFO:gensim.corpora.dictionary:built Dictionary(50587 unique tokens: [&#39;absence&#39;, &#39;accepted&#39;, &#39;access&#39;, &#39;according&#39;, &#39;active&#39;]...) from 500 documents (total 492112 corpus positions)
</pre></div>
</div>
</div>
</div>
<p>Now that we have a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a>, we can get a preview that displays the number of unique tokens across all of our texts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dictionary(50587 unique tokens: [&#39;absence&#39;, &#39;accepted&#39;, &#39;access&#39;, &#39;according&#39;, &#39;active&#39;]...)
</pre></div>
</div>
</div>
</div>
<p>The <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> stores a unique identifier (starting with 0) for every unique token in the corpus. The <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> does not contain information on word frequencies; it only catalogs all the words in the corpus. You can see the unique ID for each token in the text using the .token2id() method. Your corpus may have hundreds of thousands of unique words so here we just give a preview of the first ten.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">dictionary</span><span class="o">.</span><span class="n">token2id</span><span class="o">.</span><span class="n">items</span><span class="p">())[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span> <span class="c1"># Print the first ten tokens and their associated IDs.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;absence&#39;: 0,
 &#39;accepted&#39;: 1,
 &#39;access&#39;: 2,
 &#39;according&#39;: 3,
 &#39;active&#39;: 4,
 &#39;adequate&#39;: 5,
 &#39;adhered&#39;: 6,
 &#39;admitted&#39;: 7,
 &#39;adult&#39;: 8,
 &#39;advertisements&#39;: 9}
</pre></div>
</div>
</div>
</div>
<p>We can also look up the corresponding ID for a token using the <code class="docutils literal notranslate"><span class="pre">.get</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the value for the key &#39;people&#39;. Return 0 if there is no token matching &#39;people&#39;. </span>
<span class="c1"># The number returned is the gensim dictionary ID for the token. </span>

<span class="n">dictionary</span><span class="o">.</span><span class="n">token2id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;accents&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>27275
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="creating-a-bag-of-words-corpus">
<h2>Creating a Bag of Words Corpus<a class="headerlink" href="#creating-a-bag-of-words-corpus" title="Permalink to this headline">¶</a></h2>
<div class="section" id="example-a-single-document">
<h3>Example: A Single Document<a class="headerlink" href="#example-a-single-document" title="Permalink to this headline">¶</a></h3>
<p>The next step is to connect our word frequency data found within <code class="docutils literal notranslate"><span class="pre">documents</span></code> to our <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> token IDs. For every document, we want to know how many times a word (notated by its ID) occurs. We can do a single document first to show how this works. We will create a <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#python-list">Python list</a> called <code class="docutils literal notranslate"><span class="pre">example_bow_corpus</span></code> that will turn our word counts into a series of <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tuple">tuples</a> where the first number is the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> token ID and the second number is the word frequency.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create an example bag of words corpus. We select a document at random to use as our sample.</span>
<span class="n">example_bow_corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>

<span class="c1"># List out the first ten tuples in ``example_bow_corpus``</span>
<span class="nb">list</span><span class="p">(</span><span class="n">example_bow_corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0, 1),
 (1, 1),
 (2, 1),
 (3, 1),
 (4, 1),
 (5, 1),
 (6, 1),
 (7, 1),
 (8, 1),
 (9, 1)]
</pre></div>
</div>
</div>
</div>
<p>Using IDs can seem a little abstract, but we can discover the word associated with a particular ID. For demonstration purposes, the following code will replace the token IDs in the last example with the actual tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_counts</span> <span class="o">=</span> <span class="p">[[(</span><span class="n">dictionary</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="n">count</span><span class="p">)</span> <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">line</span><span class="p">]</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">example_bow_corpus</span><span class="p">]</span>
<span class="nb">list</span><span class="p">(</span><span class="n">word_counts</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;absence&#39;, 1),
 (&#39;accepted&#39;, 1),
 (&#39;access&#39;, 1),
 (&#39;according&#39;, 1),
 (&#39;active&#39;, 1),
 (&#39;adequate&#39;, 1),
 (&#39;adhered&#39;, 1),
 (&#39;admitted&#39;, 1),
 (&#39;adult&#39;, 1),
 (&#39;advertisements&#39;, 1)]
</pre></div>
</div>
</div>
</div>
<p>We saw before that you could discover the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> ID number by running:</p>
<blockquote>
<div><p>dictionary.token2id.get(‘people’, 0)</p>
</div></blockquote>
<p>If you wanted to discover the token given only the ID number, the method is a little more involved. You could use <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#list-comprehensions">list comprehension</a> to find the <strong>key</strong> token based on the <strong>value</strong> ID. Normally, <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#python-dictionary">Python dictionaries</a> only map from keys to values (not from values to keys). However, we can write a quick for loop to go the other direction. (It is unlikely one would ever do these methods in practice, but they are shown here to demonstrate how the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> is connected to the list entries in the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> <code class="docutils literal notranslate"><span class="pre">bow_corpus</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">dict_id</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">dict_id</span> <span class="o">==</span> <span class="mi">100</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        
<span class="c1"># This analysis shortened as a list comprehension</span>
<span class="c1"># [token for dict_id, token in dictionary.items() if dict_id == 100] </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>could
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="example-all-documents">
<h3>Example: All Documents<a class="headerlink" href="#example-all-documents" title="Permalink to this headline">¶</a></h3>
<p>We have seen an example that demonstrates how the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a> <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#bag-of-words">bag of words</a> <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#corpus">corpus</a> works on a single document. Let’s apply it now to all of our documents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bow_corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">]</span>
<span class="nb">list</span><span class="p">(</span><span class="n">bow_corpus</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span> <span class="c1">#Show the bag of words corpus for the first 3 documents</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[(0, 1),
  (1, 1),
  (2, 1),
  (3, 1),
  (4, 1),
  (5, 1),
  (6, 1),
  (7, 1),
  (8, 1),
  (9, 1),
  (10, 1),
  (11, 1),
  (12, 1),
  (13, 1),
  (14, 1),
  (15, 1),
  (16, 1),
  (17, 1),
  (18, 1),
  (19, 2),
  (20, 1),
  (21, 1),
  (22, 1),
  (23, 1),
  (24, 1),
  (25, 1),
  (26, 1),
  (27, 1),
  (28, 1),
  (29, 1),
  (30, 1),
  (31, 1),
  (32, 1),
  (33, 1),
  (34, 1),
  (35, 1),
  (36, 1),
  (37, 1),
  (38, 1),
  (39, 1),
  (40, 1),
  (41, 1),
  (42, 1),
  (43, 1),
  (44, 1),
  (45, 1),
  (46, 1),
  (47, 1),
  (48, 1),
  (49, 1),
  (50, 1),
  (51, 1),
  (52, 2),
  (53, 1),
  (54, 1),
  (55, 1),
  (56, 1),
  (57, 1),
  (58, 1),
  (59, 1),
  (60, 1),
  (61, 2),
  (62, 1),
  (63, 1),
  (64, 1),
  (65, 1),
  (66, 1),
  (67, 1),
  (68, 1),
  (69, 1),
  (70, 1),
  (71, 1),
  (72, 1),
  (73, 1),
  (74, 1),
  (75, 1),
  (76, 1),
  (77, 1),
  (78, 1),
  (79, 1),
  (80, 1),
  (81, 1),
  (82, 1),
  (83, 1),
  (84, 1),
  (85, 1),
  (86, 1),
  (87, 1),
  (88, 1),
  (89, 1),
  (90, 1),
  (91, 1),
  (92, 1),
  (93, 1),
  (94, 1),
  (95, 1),
  (96, 1),
  (97, 1),
  (98, 1),
  (99, 1),
  (100, 1),
  (101, 1),
  (102, 1),
  (103, 1),
  (104, 1),
  (105, 1),
  (106, 1),
  (107, 1),
  (108, 2),
  (109, 1),
  (110, 1),
  (111, 1),
  (112, 1),
  (113, 1),
  (114, 1),
  (115, 1),
  (116, 1),
  (117, 1),
  (118, 1),
  (119, 1),
  (120, 1),
  (121, 1),
  (122, 2),
  (123, 1),
  (124, 1),
  (125, 1),
  (126, 1),
  (127, 2),
  (128, 1),
  (129, 1),
  (130, 1),
  (131, 1),
  (132, 1),
  (133, 1),
  (134, 1),
  (135, 1),
  (136, 1),
  (137, 1),
  (138, 1),
  (139, 1),
  (140, 1),
  (141, 1),
  (142, 1),
  (143, 2),
  (144, 2),
  (145, 2),
  (146, 1),
  (147, 1),
  (148, 1),
  (149, 1),
  (150, 1),
  (151, 1),
  (152, 1),
  (153, 1),
  (154, 1),
  (155, 1),
  (156, 1),
  (157, 1),
  (158, 1),
  (159, 1),
  (160, 1),
  (161, 1),
  (162, 1),
  (163, 1),
  (164, 1),
  (165, 1),
  (166, 1),
  (167, 1),
  (168, 1),
  (169, 1),
  (170, 1),
  (171, 1),
  (172, 1),
  (173, 1),
  (174, 1),
  (175, 1),
  (176, 1),
  (177, 1),
  (178, 1),
  (179, 1),
  (180, 1),
  (181, 1),
  (182, 1),
  (183, 1),
  (184, 1),
  (185, 2),
  (186, 1),
  (187, 1),
  (188, 1),
  (189, 1),
  (190, 1),
  (191, 1),
  (192, 2),
  (193, 1),
  (194, 1),
  (195, 1),
  (196, 1),
  (197, 1),
  (198, 1),
  (199, 1),
  (200, 1),
  (201, 2),
  (202, 1),
  (203, 2),
  (204, 1),
  (205, 2),
  (206, 1),
  (207, 1),
  (208, 1),
  (209, 1),
  (210, 1),
  (211, 1),
  (212, 1),
  (213, 1),
  (214, 1),
  (215, 1),
  (216, 1),
  (217, 1),
  (218, 1),
  (219, 1),
  (220, 1),
  (221, 1),
  (222, 1),
  (223, 1),
  (224, 2),
  (225, 1),
  (226, 1),
  (227, 1),
  (228, 2),
  (229, 1),
  (230, 1),
  (231, 1),
  (232, 1),
  (233, 1),
  (234, 1),
  (235, 1),
  (236, 1),
  (237, 1),
  (238, 1),
  (239, 1),
  (240, 1),
  (241, 1),
  (242, 1),
  (243, 1),
  (244, 1),
  (245, 1),
  (246, 1),
  (247, 1),
  (248, 1),
  (249, 1),
  (250, 1),
  (251, 1),
  (252, 1),
  (253, 1),
  (254, 1),
  (255, 1),
  (256, 1),
  (257, 1),
  (258, 1),
  (259, 1),
  (260, 1),
  (261, 1),
  (262, 2),
  (263, 1),
  (264, 1),
  (265, 1),
  (266, 1),
  (267, 1),
  (268, 1),
  (269, 1),
  (270, 1),
  (271, 1),
  (272, 1),
  (273, 1),
  (274, 1),
  (275, 1),
  (276, 1),
  (277, 1),
  (278, 1),
  (279, 1),
  (280, 1),
  (281, 1),
  (282, 1),
  (283, 1),
  (284, 1),
  (285, 1),
  (286, 1),
  (287, 1),
  (288, 1),
  (289, 1),
  (290, 1),
  (291, 1),
  (292, 1),
  (293, 1),
  (294, 1),
  (295, 1),
  (296, 2),
  (297, 1),
  (298, 2),
  (299, 1),
  (300, 1),
  (301, 1),
  (302, 1),
  (303, 1),
  (304, 1),
  (305, 1),
  (306, 1),
  (307, 1),
  (308, 1),
  (309, 1),
  (310, 1),
  (311, 1),
  (312, 1),
  (313, 1),
  (314, 1),
  (315, 1),
  (316, 1),
  (317, 1),
  (318, 1),
  (319, 1),
  (320, 1),
  (321, 1),
  (322, 1),
  (323, 1),
  (324, 1),
  (325, 1),
  (326, 1),
  (327, 1),
  (328, 1),
  (329, 1),
  (330, 1),
  (331, 2),
  (332, 1),
  (333, 1),
  (334, 1),
  (335, 1),
  (336, 1),
  (337, 1),
  (338, 1),
  (339, 1),
  (340, 1),
  (341, 1),
  (342, 1),
  (343, 1),
  (344, 1),
  (345, 1),
  (346, 1),
  (347, 1),
  (348, 1),
  (349, 1),
  (350, 1),
  (351, 1),
  (352, 1),
  (353, 2),
  (354, 1),
  (355, 1),
  (356, 1),
  (357, 1),
  (358, 1),
  (359, 1),
  (360, 1),
  (361, 1),
  (362, 1),
  (363, 1),
  (364, 1),
  (365, 1),
  (366, 1),
  (367, 1),
  (368, 1),
  (369, 1),
  (370, 1),
  (371, 1),
  (372, 1),
  (373, 1),
  (374, 1),
  (375, 1),
  (376, 1),
  (377, 1),
  (378, 1),
  (379, 1),
  (380, 1),
  (381, 1),
  (382, 1),
  (383, 2),
  (384, 1),
  (385, 1),
  (386, 1),
  (387, 1),
  (388, 1),
  (389, 1),
  (390, 1),
  (391, 1),
  (392, 1),
  (393, 1),
  (394, 1),
  (395, 1),
  (396, 1),
  (397, 1),
  (398, 1),
  (399, 1),
  (400, 1),
  (401, 1),
  (402, 1),
  (403, 1),
  (404, 1),
  (405, 1),
  (406, 1),
  (407, 1),
  (408, 1),
  (409, 1),
  (410, 1),
  (411, 1),
  (412, 1),
  (413, 1),
  (414, 1),
  (415, 1),
  (416, 1),
  (417, 1),
  (418, 1),
  (419, 1),
  (420, 1),
  (421, 1),
  (422, 1),
  (423, 1),
  (424, 1),
  (425, 1),
  (426, 1),
  (427, 1),
  (428, 1),
  (429, 1),
  (430, 1),
  (431, 1),
  (432, 1),
  (433, 1),
  (434, 1),
  (435, 1),
  (436, 2),
  (437, 1),
  (438, 1),
  (439, 1),
  (440, 1),
  (441, 1),
  (442, 1),
  (443, 1),
  (444, 1),
  (445, 1),
  (446, 1),
  (447, 1),
  (448, 1),
  (449, 1),
  (450, 1),
  (451, 1),
  (452, 2),
  (453, 1),
  (454, 1),
  (455, 1),
  (456, 1),
  (457, 1),
  (458, 1),
  (459, 1),
  (460, 1),
  (461, 1),
  (462, 1),
  (463, 1),
  (464, 1),
  (465, 1),
  (466, 1),
  (467, 1),
  (468, 1),
  (469, 1),
  (470, 1),
  (471, 2),
  (472, 1),
  (473, 1),
  (474, 1),
  (475, 1),
  (476, 1),
  (477, 2),
  (478, 1),
  (479, 2),
  (480, 1),
  (481, 1),
  (482, 1),
  (483, 1),
  (484, 1),
  (485, 1),
  (486, 1),
  (487, 1),
  (488, 1),
  (489, 2),
  (490, 1),
  (491, 1),
  (492, 1),
  (493, 1),
  (494, 1),
  (495, 1),
  (496, 1),
  (497, 1),
  (498, 1),
  (499, 2),
  (500, 1),
  (501, 1),
  (502, 1),
  (503, 1),
  (504, 1),
  (505, 1),
  (506, 1),
  (507, 1),
  (508, 1),
  (509, 1),
  (510, 1),
  (511, 1),
  (512, 1),
  (513, 1),
  (514, 1),
  (515, 1),
  (516, 1),
  (517, 1),
  (518, 1),
  (519, 1),
  (520, 1),
  (521, 1),
  (522, 1),
  (523, 1),
  (524, 1),
  (525, 1),
  (526, 1),
  (527, 1),
  (528, 1),
  (529, 1),
  (530, 1),
  (531, 1),
  (532, 1),
  (533, 2),
  (534, 1),
  (535, 1),
  (536, 1),
  (537, 1),
  (538, 1),
  (539, 1),
  (540, 2),
  (541, 2),
  (542, 2),
  (543, 1),
  (544, 1),
  (545, 1),
  (546, 1),
  (547, 1),
  (548, 2),
  (549, 1),
  (550, 1),
  (551, 1),
  (552, 1),
  (553, 1),
  (554, 1),
  (555, 1),
  (556, 1),
  (557, 1),
  (558, 1),
  (559, 1),
  (560, 1),
  (561, 1),
  (562, 1),
  (563, 1),
  (564, 1),
  (565, 1),
  (566, 2),
  (567, 1),
  (568, 1),
  (569, 1),
  (570, 1),
  (571, 2),
  (572, 1),
  (573, 1),
  (574, 1),
  (575, 1),
  (576, 1),
  (577, 1)],
 [(14, 1),
  (16, 1),
  (21, 1),
  (23, 1),
  (39, 1),
  (47, 1),
  (48, 1),
  (55, 1),
  (56, 1),
  (63, 1),
  (81, 1),
  (84, 1),
  (100, 1),
  (106, 1),
  (108, 1),
  (111, 1),
  (135, 1),
  (142, 1),
  (155, 1),
  (169, 2),
  (171, 1),
  (172, 1),
  (176, 1),
  (181, 1),
  (184, 1),
  (185, 1),
  (187, 1),
  (190, 1),
  (196, 2),
  (197, 1),
  (205, 1),
  (209, 2),
  (211, 1),
  (212, 1),
  (215, 1),
  (220, 1),
  (222, 1),
  (227, 1),
  (239, 1),
  (242, 1),
  (246, 1),
  (249, 1),
  (250, 1),
  (259, 1),
  (264, 1),
  (267, 1),
  (268, 1),
  (271, 1),
  (275, 1),
  (278, 1),
  (279, 1),
  (282, 1),
  (283, 1),
  (284, 1),
  (285, 2),
  (291, 1),
  (296, 1),
  (319, 1),
  (321, 1),
  (333, 1),
  (334, 1),
  (336, 1),
  (338, 2),
  (345, 1),
  (346, 1),
  (351, 1),
  (354, 1),
  (357, 1),
  (361, 1),
  (363, 1),
  (369, 1),
  (370, 1),
  (371, 1),
  (382, 1),
  (394, 1),
  (397, 1),
  (400, 1),
  (408, 1),
  (409, 1),
  (412, 1),
  (415, 1),
  (416, 1),
  (426, 1),
  (428, 1),
  (442, 1),
  (449, 1),
  (452, 1),
  (456, 1),
  (457, 1),
  (469, 1),
  (471, 2),
  (477, 1),
  (479, 2),
  (486, 1),
  (491, 1),
  (494, 1),
  (496, 1),
  (498, 1),
  (499, 1),
  (520, 1),
  (521, 1),
  (537, 1),
  (541, 1),
  (542, 1),
  (545, 1),
  (549, 1),
  (551, 1),
  (560, 1),
  (565, 1),
  (566, 1),
  (568, 1),
  (569, 1),
  (571, 1),
  (572, 1),
  (573, 1),
  (575, 1),
  (576, 1),
  (577, 1),
  (578, 1),
  (579, 1),
  (580, 1),
  (581, 1),
  (582, 1),
  (583, 1),
  (584, 1),
  (585, 1),
  (586, 1),
  (587, 1),
  (588, 1),
  (589, 1),
  (590, 1),
  (591, 1),
  (592, 1),
  (593, 2),
  (594, 1),
  (595, 1),
  (596, 1),
  (597, 1),
  (598, 1),
  (599, 1),
  (600, 1),
  (601, 1),
  (602, 1),
  (603, 1),
  (604, 1),
  (605, 1),
  (606, 1),
  (607, 1),
  (608, 1),
  (609, 1),
  (610, 1),
  (611, 1),
  (612, 1),
  (613, 1),
  (614, 1),
  (615, 1),
  (616, 1),
  (617, 1),
  (618, 1),
  (619, 1),
  (620, 1),
  (621, 1),
  (622, 1),
  (623, 1),
  (624, 1),
  (625, 1),
  (626, 1),
  (627, 1),
  (628, 1),
  (629, 1),
  (630, 1),
  (631, 1),
  (632, 1),
  (633, 1),
  (634, 1),
  (635, 1),
  (636, 1),
  (637, 1),
  (638, 1),
  (639, 1),
  (640, 1),
  (641, 1),
  (642, 1),
  (643, 1),
  (644, 1),
  (645, 1),
  (646, 1),
  (647, 1),
  (648, 1),
  (649, 1),
  (650, 1),
  (651, 1),
  (652, 1),
  (653, 1),
  (654, 1),
  (655, 1),
  (656, 1),
  (657, 1),
  (658, 1),
  (659, 1),
  (660, 1),
  (661, 1),
  (662, 1),
  (663, 1),
  (664, 1),
  (665, 1),
  (666, 2),
  (667, 1),
  (668, 1),
  (669, 1),
  (670, 1),
  (671, 1),
  (672, 1),
  (673, 1),
  (674, 1),
  (675, 1),
  (676, 1),
  (677, 1),
  (678, 1),
  (679, 1),
  (680, 1),
  (681, 1),
  (682, 1),
  (683, 1),
  (684, 1),
  (685, 2),
  (686, 1),
  (687, 1),
  (688, 1),
  (689, 1),
  (690, 1),
  (691, 1),
  (692, 1),
  (693, 1),
  (694, 1),
  (695, 1),
  (696, 1),
  (697, 1),
  (698, 1),
  (699, 1),
  (700, 1),
  (701, 1),
  (702, 1),
  (703, 1),
  (704, 1),
  (705, 1),
  (706, 1),
  (707, 1),
  (708, 1),
  (709, 1),
  (710, 1),
  (711, 1),
  (712, 1),
  (713, 1),
  (714, 1),
  (715, 1),
  (716, 1),
  (717, 1),
  (718, 1),
  (719, 1),
  (720, 1),
  (721, 1),
  (722, 1),
  (723, 1),
  (724, 1),
  (725, 1),
  (726, 1),
  (727, 1),
  (728, 1),
  (729, 1),
  (730, 1),
  (731, 1),
  (732, 1),
  (733, 1),
  (734, 1),
  (735, 1),
  (736, 1),
  (737, 1),
  (738, 1),
  (739, 1),
  (740, 1),
  (741, 1),
  (742, 1),
  (743, 1),
  (744, 1),
  (745, 1),
  (746, 1),
  (747, 1),
  (748, 1),
  (749, 1),
  (750, 1),
  (751, 1),
  (752, 1),
  (753, 1),
  (754, 1),
  (755, 1),
  (756, 1),
  (757, 1),
  (758, 1),
  (759, 1),
  (760, 2),
  (761, 1),
  (762, 1),
  (763, 1),
  (764, 1),
  (765, 1),
  (766, 1),
  (767, 1),
  (768, 1),
  (769, 1),
  (770, 1),
  (771, 1),
  (772, 1),
  (773, 1),
  (774, 1),
  (775, 1),
  (776, 2),
  (777, 1),
  (778, 1),
  (779, 1),
  (780, 1),
  (781, 1),
  (782, 1),
  (783, 1),
  (784, 1),
  (785, 1),
  (786, 1),
  (787, 1),
  (788, 1),
  (789, 1),
  (790, 1),
  (791, 1),
  (792, 1),
  (793, 1),
  (794, 1),
  (795, 1),
  (796, 1),
  (797, 1),
  (798, 1),
  (799, 1),
  (800, 1),
  (801, 1),
  (802, 1),
  (803, 1),
  (804, 1),
  (805, 1),
  (806, 1),
  (807, 2),
  (808, 1),
  (809, 1),
  (810, 1),
  (811, 1),
  (812, 1),
  (813, 1),
  (814, 1),
  (815, 1),
  (816, 1),
  (817, 1),
  (818, 1),
  (819, 1),
  (820, 1),
  (821, 1),
  (822, 1),
  (823, 1),
  (824, 1),
  (825, 1),
  (826, 1),
  (827, 1),
  (828, 1),
  (829, 1),
  (830, 1),
  (831, 1),
  (832, 1),
  (833, 1),
  (834, 1),
  (835, 1),
  (836, 1),
  (837, 1),
  (838, 1),
  (839, 1),
  (840, 1),
  (841, 1),
  (842, 1),
  (843, 1),
  (844, 1),
  (845, 1),
  (846, 1),
  (847, 1),
  (848, 2),
  (849, 1),
  (850, 1),
  (851, 1),
  (852, 1),
  (853, 1),
  (854, 1),
  (855, 1),
  (856, 1),
  (857, 1),
  (858, 1),
  (859, 1),
  (860, 1),
  (861, 1),
  (862, 1),
  (863, 1),
  (864, 1),
  (865, 1),
  (866, 1),
  (867, 1),
  (868, 1),
  (869, 1),
  (870, 1),
  (871, 1),
  (872, 1),
  (873, 1),
  (874, 1),
  (875, 1),
  (876, 1),
  (877, 1),
  (878, 2),
  (879, 1),
  (880, 1),
  (881, 1),
  (882, 1),
  (883, 1),
  (884, 1),
  (885, 1),
  (886, 1),
  (887, 1),
  (888, 1),
  (889, 1),
  (890, 1),
  (891, 1),
  (892, 1),
  (893, 1),
  (894, 1),
  (895, 1),
  (896, 1),
  (897, 1),
  (898, 1),
  (899, 1),
  (900, 1),
  (901, 1),
  (902, 1),
  (903, 1),
  (904, 1),
  (905, 1),
  (906, 1),
  (907, 1),
  (908, 1),
  (909, 1),
  (910, 1),
  (911, 1),
  (912, 1),
  (913, 1),
  (914, 1),
  (915, 1),
  (916, 1),
  (917, 1),
  (918, 1),
  (919, 1),
  (920, 1),
  (921, 1),
  (922, 1),
  (923, 1),
  (924, 1),
  (925, 1),
  (926, 1),
  (927, 1),
  (928, 1),
  (929, 1),
  (930, 1),
  (931, 1),
  (932, 1),
  (933, 1),
  (934, 1),
  (935, 1),
  (936, 1),
  (937, 1),
  (938, 1),
  (939, 1),
  (940, 1),
  (941, 1),
  (942, 1),
  (943, 1),
  (944, 1),
  (945, 1),
  (946, 1),
  (947, 1),
  (948, 1),
  (949, 1),
  (950, 1),
  (951, 1),
  (952, 1),
  (953, 1),
  (954, 1),
  (955, 2),
  (956, 1),
  (957, 1),
  (958, 1),
  (959, 1),
  (960, 1),
  (961, 1),
  (962, 1),
  (963, 1),
  (964, 1),
  (965, 1),
  (966, 1),
  (967, 1),
  (968, 1),
  (969, 1),
  (970, 1),
  (971, 1),
  (972, 1),
  (973, 1),
  (974, 1),
  (975, 1),
  (976, 1),
  (977, 1),
  (978, 1),
  (979, 1),
  (980, 1),
  (981, 1),
  (982, 1),
  (983, 1),
  (984, 1),
  (985, 1),
  (986, 1),
  (987, 1),
  (988, 1),
  (989, 1),
  (990, 1),
  (991, 1),
  (992, 1),
  (993, 1),
  (994, 1),
  (995, 1),
  (996, 1),
  (997, 1),
  (998, 1),
  (999, 1),
  (1000, 1),
  (1001, 1),
  (1002, 1),
  (1003, 1),
  (1004, 1),
  (1005, 1),
  (1006, 1),
  (1007, 1),
  (1008, 1),
  (1009, 1),
  (1010, 1),
  (1011, 1),
  (1012, 1),
  (1013, 1),
  (1014, 1),
  (1015, 1),
  (1016, 1),
  (1017, 1),
  (1018, 1),
  (1019, 1),
  (1020, 1),
  (1021, 1),
  (1022, 2),
  (1023, 1),
  (1024, 1),
  (1025, 1),
  (1026, 1),
  (1027, 1),
  (1028, 1),
  (1029, 1),
  (1030, 1),
  (1031, 1),
  (1032, 1),
  (1033, 1),
  (1034, 1),
  (1035, 1),
  (1036, 1),
  (1037, 1),
  (1038, 1),
  (1039, 2),
  (1040, 1),
  (1041, 1),
  (1042, 1),
  (1043, 1),
  (1044, 1),
  (1045, 1),
  (1046, 1),
  (1047, 1),
  (1048, 1),
  (1049, 1),
  (1050, 1),
  (1051, 1),
  (1052, 1),
  (1053, 1),
  (1054, 1),
  (1055, 1),
  (1056, 1),
  (1057, 1),
  (1058, 1),
  (1059, 1),
  (1060, 1),
  (1061, 2),
  (1062, 1),
  (1063, 1),
  (1064, 1),
  (1065, 1),
  (1066, 1),
  (1067, 1),
  (1068, 1),
  (1069, 1),
  (1070, 1),
  (1071, 1),
  (1072, 1),
  (1073, 1),
  (1074, 1),
  (1075, 1),
  (1076, 1),
  (1077, 1),
  (1078, 1),
  (1079, 1),
  (1080, 1),
  (1081, 1),
  (1082, 1),
  (1083, 1),
  (1084, 1),
  (1085, 1),
  (1086, 1),
  (1087, 1),
  (1088, 1),
  (1089, 1),
  (1090, 1),
  (1091, 1),
  (1092, 1),
  (1093, 1),
  (1094, 1),
  (1095, 1),
  (1096, 1),
  (1097, 1),
  (1098, 1),
  (1099, 2),
  (1100, 1),
  (1101, 1),
  (1102, 1),
  (1103, 1),
  (1104, 1),
  (1105, 1),
  (1106, 1),
  (1107, 1),
  (1108, 1),
  (1109, 1),
  (1110, 1),
  (1111, 1),
  (1112, 1),
  (1113, 1),
  (1114, 1),
  (1115, 1),
  (1116, 1),
  (1117, 1),
  (1118, 1),
  (1119, 1),
  (1120, 1),
  (1121, 1),
  (1122, 1),
  (1123, 1),
  (1124, 1),
  (1125, 1),
  (1126, 1),
  (1127, 1),
  (1128, 1),
  (1129, 1),
  (1130, 1),
  (1131, 1),
  (1132, 1),
  (1133, 1),
  (1134, 1),
  (1135, 1),
  (1136, 1),
  (1137, 1),
  (1138, 1),
  (1139, 1),
  (1140, 1),
  (1141, 1),
  (1142, 1),
  (1143, 1),
  (1144, 1),
  (1145, 1),
  (1146, 1),
  (1147, 1),
  (1148, 1),
  (1149, 1),
  (1150, 1),
  (1151, 1),
  (1152, 1),
  (1153, 1),
  (1154, 1),
  (1155, 1),
  (1156, 1),
  (1157, 1),
  (1158, 2),
  (1159, 1),
  (1160, 1),
  (1161, 1),
  (1162, 1),
  (1163, 1),
  (1164, 1),
  (1165, 1),
  (1166, 1),
  (1167, 1),
  (1168, 1),
  (1169, 1),
  (1170, 1),
  (1171, 1),
  (1172, 1),
  (1173, 1),
  (1174, 1),
  (1175, 1),
  (1176, 1),
  (1177, 1),
  (1178, 1),
  (1179, 1),
  (1180, 1),
  (1181, 1),
  (1182, 1),
  (1183, 1),
  (1184, 1),
  (1185, 1),
  (1186, 1),
  (1187, 1),
  (1188, 1),
  (1189, 1),
  (1190, 1),
  (1191, 1),
  (1192, 1),
  (1193, 1),
  (1194, 1),
  (1195, 1),
  (1196, 1),
  (1197, 1),
  (1198, 1),
  (1199, 1),
  (1200, 1),
  (1201, 1),
  (1202, 1),
  (1203, 1),
  (1204, 1),
  (1205, 1),
  (1206, 1),
  (1207, 1),
  (1208, 1),
  (1209, 1),
  (1210, 1),
  (1211, 1),
  (1212, 1),
  (1213, 1),
  (1214, 1),
  (1215, 1),
  (1216, 1),
  (1217, 1),
  (1218, 1),
  (1219, 1),
  (1220, 1),
  (1221, 1),
  (1222, 1),
  (1223, 1),
  (1224, 1),
  (1225, 1),
  (1226, 1),
  (1227, 1),
  (1228, 1),
  (1229, 1),
  (1230, 1),
  (1231, 1),
  (1232, 1),
  (1233, 1),
  (1234, 1),
  (1235, 1),
  (1236, 1),
  (1237, 1),
  (1238, 1),
  (1239, 1),
  (1240, 1),
  (1241, 1),
  (1242, 1),
  (1243, 1),
  (1244, 1),
  (1245, 1),
  (1246, 1),
  (1247, 1),
  (1248, 1),
  (1249, 1),
  (1250, 1),
  (1251, 1),
  (1252, 1),
  (1253, 1),
  (1254, 1),
  (1255, 1),
  (1256, 1),
  (1257, 1),
  (1258, 2),
  (1259, 1),
  (1260, 1),
  (1261, 1),
  (1262, 1),
  (1263, 1),
  (1264, 1),
  (1265, 1),
  (1266, 1),
  (1267, 1),
  (1268, 1),
  (1269, 1),
  (1270, 1),
  (1271, 1),
  (1272, 1),
  (1273, 1),
  (1274, 1),
  (1275, 1),
  (1276, 1),
  (1277, 1),
  (1278, 1),
  (1279, 1),
  (1280, 1),
  (1281, 1),
  (1282, 1),
  (1283, 1),
  (1284, 1),
  (1285, 1),
  (1286, 1),
  (1287, 1),
  (1288, 1),
  (1289, 1),
  (1290, 1),
  (1291, 1),
  (1292, 1),
  (1293, 1),
  (1294, 1),
  (1295, 1),
  (1296, 1),
  (1297, 1),
  (1298, 1),
  (1299, 1),
  (1300, 1),
  (1301, 1),
  (1302, 1),
  (1303, 1),
  (1304, 1),
  (1305, 1),
  (1306, 1),
  (1307, 1),
  (1308, 1),
  (1309, 1),
  (1310, 1),
  (1311, 1),
  (1312, 1),
  (1313, 1),
  (1314, 1),
  (1315, 1),
  (1316, 1),
  (1317, 1),
  (1318, 1),
  (1319, 1),
  (1320, 1),
  (1321, 1),
  (1322, 1),
  (1323, 1),
  (1324, 1),
  (1325, 1),
  (1326, 1),
  (1327, 1),
  (1328, 1),
  (1329, 1),
  (1330, 1),
  (1331, 1),
  (1332, 1),
  (1333, 1),
  (1334, 1),
  (1335, 1),
  (1336, 1),
  (1337, 1),
  (1338, 1),
  (1339, 1),
  (1340, 1),
  (1341, 1),
  (1342, 1),
  (1343, 1),
  (1344, 1),
  (1345, 1),
  (1346, 1),
  (1347, 1),
  (1348, 1),
  (1349, 1),
  (1350, 1),
  (1351, 1),
  (1352, 1),
  (1353, 1),
  (1354, 2),
  (1355, 1),
  (1356, 1),
  (1357, 1),
  (1358, 1),
  (1359, 1),
  (1360, 1),
  (1361, 1),
  (1362, 1),
  (1363, 1),
  (1364, 1),
  (1365, 2),
  (1366, 1),
  (1367, 1),
  (1368, 1),
  (1369, 1),
  (1370, 1),
  (1371, 1),
  (1372, 1),
  (1373, 1),
  (1374, 1),
  (1375, 1)],
 [(2, 1),
  (3, 1),
  (4, 1),
  (6, 1),
  (10, 1),
  (16, 1),
  (31, 1),
  (43, 1),
  (56, 1),
  (68, 1),
  (78, 1),
  (97, 1),
  (99, 1),
  (100, 1),
  (118, 1),
  (122, 1),
  (123, 1),
  (140, 1),
  (142, 1),
  (147, 1),
  (169, 1),
  (171, 1),
  (176, 1),
  (185, 1),
  (188, 1),
  (197, 1),
  (203, 1),
  (206, 1),
  (211, 1),
  (212, 1),
  (217, 1),
  (218, 1),
  (219, 1),
  (222, 1),
  (233, 1),
  (236, 1),
  (239, 1),
  (249, 1),
  (261, 2),
  (262, 1),
  (270, 1),
  (275, 2),
  (282, 1),
  (283, 1),
  (286, 1),
  (290, 1),
  (292, 1),
  (294, 1),
  (296, 1),
  (298, 1),
  (299, 1),
  (302, 1),
  (328, 1),
  (329, 1),
  (331, 1),
  (334, 1),
  (345, 1),
  (347, 1),
  (349, 1),
  (354, 1),
  (357, 1),
  (369, 1),
  (375, 1),
  (383, 1),
  (396, 1),
  (397, 1),
  (404, 1),
  (405, 1),
  (406, 1),
  (407, 1),
  (420, 1),
  (421, 1),
  (439, 1),
  (442, 1),
  (446, 1),
  (447, 1),
  (463, 1),
  (477, 1),
  (489, 1),
  (491, 1),
  (498, 1),
  (499, 1),
  (507, 2),
  (510, 1),
  (513, 1),
  (516, 1),
  (521, 1),
  (522, 1),
  (529, 1),
  (533, 1),
  (537, 1),
  (541, 1),
  (549, 1),
  (551, 1),
  (560, 1),
  (563, 1),
  (566, 2),
  (568, 1),
  (569, 1),
  (572, 1),
  (575, 1),
  (577, 1),
  (579, 1),
  (588, 1),
  (630, 1),
  (636, 1),
  (637, 1),
  (692, 1),
  (694, 1),
  (702, 1),
  (706, 1),
  (728, 1),
  (770, 1),
  (786, 1),
  (806, 1),
  (813, 1),
  (817, 1),
  (837, 1),
  (848, 1),
  (859, 1),
  (898, 1),
  (908, 1),
  (921, 1),
  (953, 1),
  (961, 1),
  (966, 1),
  (971, 2),
  (1002, 1),
  (1005, 1),
  (1017, 1),
  (1053, 1),
  (1060, 1),
  (1067, 1),
  (1084, 1),
  (1089, 1),
  (1103, 1),
  (1138, 1),
  (1169, 1),
  (1193, 1),
  (1207, 1),
  (1211, 1),
  (1232, 1),
  (1238, 1),
  (1246, 1),
  (1289, 2),
  (1317, 1),
  (1319, 1),
  (1336, 1),
  (1352, 1),
  (1354, 1),
  (1355, 1),
  (1363, 1),
  (1365, 1),
  (1376, 1),
  (1377, 1),
  (1378, 1),
  (1379, 1),
  (1380, 1),
  (1381, 1),
  (1382, 1),
  (1383, 1),
  (1384, 1),
  (1385, 1),
  (1386, 1),
  (1387, 1),
  (1388, 1),
  (1389, 1),
  (1390, 1),
  (1391, 1),
  (1392, 1),
  (1393, 1),
  (1394, 1),
  (1395, 1),
  (1396, 1),
  (1397, 1),
  (1398, 1),
  (1399, 1),
  (1400, 1),
  (1401, 1),
  (1402, 1),
  (1403, 1),
  (1404, 1),
  (1405, 1),
  (1406, 1),
  (1407, 1),
  (1408, 1),
  (1409, 1),
  (1410, 2),
  (1411, 1),
  (1412, 1),
  (1413, 1),
  (1414, 1),
  (1415, 1),
  (1416, 1),
  (1417, 1),
  (1418, 1),
  (1419, 1),
  (1420, 1),
  (1421, 1),
  (1422, 1),
  (1423, 1),
  (1424, 1),
  (1425, 1),
  (1426, 1),
  (1427, 1),
  (1428, 1),
  (1429, 1),
  (1430, 1),
  (1431, 1),
  (1432, 1),
  (1433, 1),
  (1434, 1),
  (1435, 1),
  (1436, 1),
  (1437, 1),
  (1438, 1),
  (1439, 1),
  (1440, 1),
  (1441, 1),
  (1442, 1),
  (1443, 1),
  (1444, 1),
  (1445, 1),
  (1446, 1),
  (1447, 1),
  (1448, 1),
  (1449, 1),
  (1450, 1),
  (1451, 1),
  (1452, 1),
  (1453, 1),
  (1454, 1),
  (1455, 1),
  (1456, 1),
  (1457, 1),
  (1458, 1),
  (1459, 1),
  (1460, 1),
  (1461, 1),
  (1462, 1),
  (1463, 1),
  (1464, 1),
  (1465, 1),
  (1466, 1),
  (1467, 1),
  (1468, 1),
  (1469, 1),
  (1470, 1),
  (1471, 1),
  (1472, 1),
  (1473, 1),
  (1474, 1),
  (1475, 1),
  (1476, 1),
  (1477, 1),
  (1478, 1),
  (1479, 2),
  (1480, 1),
  (1481, 1),
  (1482, 1),
  (1483, 1),
  (1484, 1),
  (1485, 1),
  (1486, 1),
  (1487, 1),
  (1488, 1),
  (1489, 1),
  (1490, 1),
  (1491, 1),
  (1492, 1),
  (1493, 1),
  (1494, 1),
  (1495, 1),
  (1496, 1),
  (1497, 1),
  (1498, 1),
  (1499, 1),
  (1500, 1),
  (1501, 1),
  (1502, 1),
  (1503, 2),
  (1504, 1),
  (1505, 1),
  (1506, 1),
  (1507, 1),
  (1508, 1),
  (1509, 1),
  (1510, 1),
  (1511, 1),
  (1512, 2),
  (1513, 1),
  (1514, 1),
  (1515, 1),
  (1516, 1),
  (1517, 1),
  (1518, 1),
  (1519, 1),
  (1520, 1),
  (1521, 1),
  (1522, 1),
  (1523, 1),
  (1524, 1),
  (1525, 1),
  (1526, 1),
  (1527, 1),
  (1528, 1),
  (1529, 1),
  (1530, 1),
  (1531, 1),
  (1532, 1),
  (1533, 1),
  (1534, 1),
  (1535, 1),
  (1536, 1),
  (1537, 1),
  (1538, 1),
  (1539, 1),
  (1540, 1),
  (1541, 1),
  (1542, 1),
  (1543, 1),
  (1544, 1),
  (1545, 1),
  (1546, 1),
  (1547, 1),
  (1548, 1),
  (1549, 1),
  (1550, 1),
  (1551, 1),
  (1552, 1),
  (1553, 1),
  (1554, 1),
  (1555, 1),
  (1556, 1),
  (1557, 1),
  (1558, 1),
  (1559, 1),
  (1560, 1),
  (1561, 1),
  (1562, 1),
  (1563, 1),
  (1564, 1),
  (1565, 1),
  (1566, 1),
  (1567, 1),
  (1568, 1),
  (1569, 1),
  (1570, 1),
  (1571, 1),
  (1572, 1),
  (1573, 1),
  (1574, 1),
  (1575, 1),
  (1576, 1),
  (1577, 1),
  (1578, 2),
  (1579, 1),
  (1580, 1),
  (1581, 1),
  (1582, 1),
  (1583, 1),
  (1584, 1),
  (1585, 1),
  (1586, 1),
  (1587, 1),
  (1588, 1),
  (1589, 1),
  (1590, 1),
  (1591, 1),
  (1592, 1),
  (1593, 1),
  (1594, 1),
  (1595, 1),
  (1596, 1),
  (1597, 1),
  (1598, 1),
  (1599, 1),
  (1600, 1),
  (1601, 1),
  (1602, 1),
  (1603, 1),
  (1604, 1),
  (1605, 1),
  (1606, 1),
  (1607, 1),
  (1608, 1),
  (1609, 1),
  (1610, 1),
  (1611, 1),
  (1612, 1),
  (1613, 1),
  (1614, 1),
  (1615, 1),
  (1616, 1),
  (1617, 1),
  (1618, 1),
  (1619, 1),
  (1620, 1),
  (1621, 1),
  (1622, 1),
  (1623, 1),
  (1624, 1),
  (1625, 1),
  (1626, 1),
  (1627, 1),
  (1628, 1),
  (1629, 1),
  (1630, 1),
  (1631, 1),
  (1632, 1),
  (1633, 1),
  (1634, 1),
  (1635, 1),
  (1636, 1),
  (1637, 1),
  (1638, 1),
  (1639, 1),
  (1640, 1),
  (1641, 1),
  (1642, 1),
  (1643, 1),
  (1644, 1),
  (1645, 1),
  (1646, 1),
  (1647, 1),
  (1648, 1),
  (1649, 1),
  (1650, 1),
  (1651, 1),
  (1652, 1),
  (1653, 1),
  (1654, 1),
  (1655, 1),
  (1656, 1),
  (1657, 1),
  (1658, 1),
  (1659, 1),
  (1660, 1),
  (1661, 1),
  (1662, 1),
  (1663, 1),
  (1664, 1),
  (1665, 1),
  (1666, 1),
  (1667, 1),
  (1668, 1),
  (1669, 1),
  (1670, 1),
  (1671, 1),
  (1672, 1),
  (1673, 1),
  (1674, 1),
  (1675, 1),
  (1676, 1),
  (1677, 1),
  (1678, 1),
  (1679, 1),
  (1680, 1),
  (1681, 1),
  (1682, 1),
  (1683, 1),
  (1684, 1),
  (1685, 1),
  (1686, 1),
  (1687, 1),
  (1688, 1),
  (1689, 1),
  (1690, 1),
  (1691, 1),
  (1692, 1),
  (1693, 1),
  (1694, 1),
  (1695, 1),
  (1696, 1),
  (1697, 1)]]
</pre></div>
</div>
</div>
</div>
<p>The next step is to create the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> model which will set the parameters for our implementation of <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a>. In our <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> example, the formula for <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> was:</p>
<div class="math notranslate nohighlight">
\[(Times-the-word-occurs-in-given-document) \cdot \mbox{log} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}\]</div>
<p>In <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim">gensim</a>, the default formula for measuring <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> uses log base 2 instead of log base 10, as shown:</p>
<div class="math notranslate nohighlight">
\[(Times-the-word-occurs-in-given-document) \cdot \log_{2} \frac{(Total-number-of-documents)}{(Number-of-documents-containing-the-word)}\]</div>
<p>If you would like to use a different formula for your <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> calculation, there is a description of <a class="reference external" href="https://radimrehurek.com/gensim/models/tfidfmodel.html">parameters you can pass</a>.</p>
</div>
</div>
<div class="section" id="create-the-tfidfmodel">
<h2>Create the <code class="docutils literal notranslate"><span class="pre">TfidfModel</span></code><a class="headerlink" href="#create-the-tfidfmodel" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">TfidfModel</span><span class="p">(</span><span class="n">bow_corpus</span><span class="p">)</span> <span class="c1"># Create our gensim TF-IDF model</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:gensim.models.tfidfmodel:collecting document frequencies
INFO:gensim.models.tfidfmodel:PROGRESS: processing document #0
INFO:gensim.models.tfidfmodel:calculating IDF weights for 500 documents and 50587 features (468686 matrix non-zeros)
</pre></div>
</div>
</div>
</div>
<p>Now, we apply our model to the <code class="docutils literal notranslate"><span class="pre">bow_corpus</span></code> to create our results in <code class="docutils literal notranslate"><span class="pre">corpus_tfidf</span></code>. The <code class="docutils literal notranslate"><span class="pre">corpus_tfidf</span></code> is a python list of each document similar to <code class="docutils literal notranslate"><span class="pre">bow_document</span></code>. Instead of listing the frequency next to the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> ID, however, it contains the TF-IDF](<a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">https://docs.tdm-pilot.org/key-terms/#tf-idf</a>) score for the associated token. Below, we display the first document in <code class="docutils literal notranslate"><span class="pre">corpus_tfidf</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_tfidf</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="n">bow_corpus</span><span class="p">]</span> <span class="c1"># Create TF-IDF scores for the ``bow_corpus`` using our model</span>
<span class="nb">list</span><span class="p">(</span><span class="n">corpus_tfidf</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span> <span class="c1"># List out the TF-IDF scores for the first 10 tokens of the first text in the corpus</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0, 0.02460407712741214),
 (1, 0.025041715629091937),
 (2, 0.018490700642831325),
 (3, 0.014386035468333138),
 (4, 0.02181851728146566),
 (5, 0.038146996231063636),
 (6, 0.07580722802109822),
 (7, 0.04372470692866349),
 (8, 0.030170711953783545),
 (9, 0.061284618841081)]
</pre></div>
</div>
</div>
</div>
<p>Let’s display the tokens instead of the <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#gensim-dictionary">gensim dictionary</a> IDs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_tfidf_scores</span> <span class="o">=</span> <span class="p">[[(</span><span class="n">dictionary</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="n">count</span><span class="p">)</span> <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">line</span><span class="p">]</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">corpus_tfidf</span><span class="p">]</span>

<span class="c1"># List out the TF-IDF scores for the first 10 tokens of the first text in the corpus</span>
<span class="nb">list</span><span class="p">(</span><span class="n">example_tfidf_scores</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;absence&#39;, 0.02460407712741214),
 (&#39;accepted&#39;, 0.025041715629091937),
 (&#39;access&#39;, 0.018490700642831325),
 (&#39;according&#39;, 0.014386035468333138),
 (&#39;active&#39;, 0.02181851728146566),
 (&#39;adequate&#39;, 0.038146996231063636),
 (&#39;adhered&#39;, 0.07580722802109822),
 (&#39;admitted&#39;, 0.04372470692866349),
 (&#39;adult&#39;, 0.030170711953783545),
 (&#39;advertisements&#39;, 0.061284618841081)]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="find-top-terms-in-a-single-document">
<h2>Find Top Terms in a Single Document<a class="headerlink" href="#find-top-terms-in-a-single-document" title="Permalink to this headline">¶</a></h2>
<p>Finally, let’s sort the terms by their <a class="reference external" href="https://docs.tdm-pilot.org/key-terms/#tf-idf">TF-IDF</a> weights to find the most significant terms in the document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sort the tuples in our tf-idf scores list</span>

<span class="c1"># Choosing a document by its index number</span>
<span class="c1"># Change n to see a different document</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">Sort</span><span class="p">(</span><span class="n">tfidf_tuples</span><span class="p">):</span> 
    <span class="n">tfidf_tuples</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">tfidf_tuples</span> 

<span class="c1"># Print the document id and title</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Title: &#39;</span><span class="p">,</span> <span class="n">document_titles</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ID: &#39;</span><span class="p">,</span> <span class="n">document_ids</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>

<span class="c1">#List the top twenty tokens in our example document by their TF-IDF scores</span>
<span class="nb">list</span><span class="p">(</span><span class="n">Sort</span><span class="p">(</span><span class="n">example_tfidf_scores</span><span class="p">[</span><span class="n">n</span><span class="p">])[:</span><span class="mi">20</span><span class="p">])</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Title:  Women in Singapore: A Report
ID:  http://www.jstor.org/stable/3173430
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;housewives&#39;, 0.11591360440489333),
 (&#39;chew&#39;, 0.10651782429748077),
 (&#39;creches&#39;, 0.10651782429748077),
 (&#39;lien&#39;, 0.10651782429748077),
 (&#39;tediousness&#39;, 0.10651782429748077),
 (&#39;terbalance&#39;, 0.10651782429748077),
 (&#39;athlone&#39;, 0.09463734391611553),
 (&#39;creed&#39;, 0.09463734391611553),
 (&#39;devan&#39;, 0.09463734391611553),
 (&#39;lieved&#39;, 0.09463734391611553),
 (&#39;reversion&#39;, 0.09463734391611553),
 (&#39;singaporean&#39;, 0.09463734391611553),
 (&#39;straits&#39;, 0.09463734391611553),
 (&#39;vented&#39;, 0.09463734391611553),
 (&#39;chong&#39;, 0.08768770840246348),
 (&#39;materialistic&#39;, 0.08768770840246348),
 (&#39;titudes&#39;, 0.08768770840246348),
 (&#39;amelioration&#39;, 0.08275686353475029),
 (&#39;ducted&#39;, 0.08275686353475029),
 (&#39;indoctrinated&#39;, 0.08275686353475029)]
</pre></div>
</div>
</div>
</div>
<p>We could also analyze across the entire corpus to find the most unique terms. These are terms that appear in a particular text, but rarely or never appear in other texts. (Often, these will be proper names since a particular article may mention a name often but the name may rarely appear in other articles. There’s also a fairly good chance these will be errors in optical character recognition.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a dictionary ``td`` where each document gather</span>
<span class="n">td</span> <span class="o">=</span> <span class="p">{</span> 
<span class="n">dictionary</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">_id</span><span class="p">):</span> <span class="n">value</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus_tfidf</span>
<span class="k">for</span> <span class="n">_id</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">doc</span>
<span class="p">}</span>

<span class="c1"># Sort the items of ``td`` into a new variable ``sorted_td``</span>
<span class="c1"># the ``reverse`` starts from highest to lowest</span>
<span class="n">sorted_td</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">td</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">kv</span><span class="p">:</span> <span class="n">kv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">term</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">sorted_td</span><span class="p">[:</span><span class="mi">25</span><span class="p">]:</span> <span class="c1"># Print the top 25 terms in the entire corpus</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>colorism 0.3489234945331017
ledgewood 0.3240198326413133
bellow 0.3183806312070241
pontellier 0.3183806312070241
mourio 0.3089441896807581
fernandes 0.2956710845261856
ecutive 0.28788023544002667
mourao 0.2862570619202311
blight 0.2784579844568313
secretaryfor 0.2784579844568313
shalala 0.2784579844568313
antediluvian 0.27125721463960645
imal 0.27125721463960645
mammoth 0.27125721463960645
ajournal 0.2667399262861068
bauman 0.2659494467819928
raquel 0.2659494467819928
embourgeoisement 0.2522154699834497
dacus 0.25073969802134444
sirow 0.25073969802134444
leela 0.24660423818519647
biennially 0.24312364506941317
congratulations 0.24312364506941317
prizewinning 0.24312364506941317
midlife 0.24258184671443078
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="display-top-result-for-all-documents">
<h2>Display Top Result for All Documents<a class="headerlink" href="#display-top-result-for-all-documents" title="Permalink to this headline">¶</a></h2>
<p>We can see the most significant term in every document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># For each document, print the ID, most significant/unique word, and TF/IDF score</span>

<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">corpus_tfidf</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">word_id</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">document_ids</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word_id</span><span class="p">),</span> <span class="n">score</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>http://www.jstor.org/stable/3173430 housewives 0.11591360440489333
http://www.jstor.org/stable/3172967 affliction 0.09656323157730741
http://www.jstor.org/stable/3175214 aswan 0.10856784148672671
http://www.jstor.org/stable/3174900 tobacco 0.14580803419191213
http://www.jstor.org/stable/10.1086/660175 healing 0.10711901495877817
http://www.jstor.org/stable/3174888 fictionally 0.07854381564205863
http://www.jstor.org/stable/10.1086/674895 biennially 0.24312364506941317
http://www.jstor.org/stable/3173229 policymakers 0.15897721564063289
http://www.jstor.org/stable/10.1086/511836 academicwomen 0.10897747320222248
http://www.jstor.org/stable/10.1086/428410 deadline 0.13127614009042335
http://www.jstor.org/stable/3175048 beijing 0.1374590080394053
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Nathan Kelber and Ted Lawless<br/>
        
          <div class="extra_footer">
            <div>
<a href="https://creativecommons.org/licenses/by/2.0/"><img class="license" alt="Creative Commons License" src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png" /></a> The tutorials and workshop materials are licensed under a <a href="https://creativecommons.org/licenses/by/2.0/">Creative Commons BY License</a>.
</div>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>