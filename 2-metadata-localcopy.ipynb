{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZtnwJbt6oJT"
   },
   "source": [
    "By <a href=\"https://nkelber.com\">Nathan Kelber</a> and Ted Lawless <br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.\n",
    "____\n",
    "\n",
    "## Explore the metadata for a JSTOR and/or Portico dataset\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Programming Knowledge Required:** \n",
    "This notebook can be run on a JSTOR/Portico [non-consumptive](./key-terms.ipynb#non-consumptive) [JSON Lines (.jsonl)](./key-terms.ipynb#jsonl) [dataset](./key-terms.ipynb#dataset) with little to no knowledge of [Python](./key-terms.ipynb#python). To have a full understanding of the code used in this [notebook](./key-terms.ipynb#jupyter-notebook), we recommend learning:\n",
    "* [Python Basics](https://automatetheboringstuff.com/2e/chapter1/)\n",
    "* [Flow Control](https://automatetheboringstuff.com/2e/chapter2/)\n",
    "* [Functions](https://automatetheboringstuff.com/2e/chapter3/)\n",
    "* [Lists](https://automatetheboringstuff.com/2e/chapter4/)\n",
    "* [Dictionaries](https://automatetheboringstuff.com/2e/chapter5/)\n",
    "\n",
    "A familiarity with [Pandas](./key-terms.ipynb#pandas) is helpful but not required.\n",
    "\n",
    "**Completion time:** 20 minutes\n",
    "\n",
    "**Data Format:** [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico) [non-consumptive](./key-terms.ipynb#non-consumptive) [JSON Lines (.jsonl)](./key-terms.ipynb#jsonl)\n",
    "\n",
    "**Libraries Used:**\n",
    "* [json](./key-terms.ipynb#json-python-library) to convert our dataset from json lines format to a Python list\n",
    "* [Pandas](./key-terms.ipynb#pandas) to help visualize the metadata\n",
    "\n",
    "**Description of methods in this notebook:**\n",
    "This [notebook](./key-terms.ipynb#jupyter-notebook) shows how to explore the [metadata](./key-terms.ipynb#metadata) of your [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico) [dataset](./key-terms.ipynb#dataset) using [Python](./key-terms.ipynb#python). The following processes are described:\n",
    "\n",
    "* Converting your [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico)[dataset](./key-terms.ipynb#dataset) into a Python list\n",
    "* Using the `.get` method to retrieve bibliographic metadata\n",
    "* Checking to see if items are in your [dataset](./key-terms.ipynb#dataset)\n",
    "* Visualizing the contents of your [dataset](./key-terms.ipynb#dataset) as a table and graph with [Pandas](./key-terms.ipynb#pandas)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can begin working with our [dataset](./key-terms.ipynb#dataset), we need to convert the [JSON lines](./key-terms.ipynb#jsonl) file written in [JavaScript](./key-terms.ipynb#javascript) into [Python](./key-terms.ipynb#python) so we can work with it. Remember that each line of our [JSON lines](./key-terms.ipynb#jsonl) file represents a single text, whether that is a journal article, book, or something else. We will create a [Python](./key-terms.ipynb#python) list that contains every document. Within each list item for each document, we will use a [Python dictionary](./key-terms.ipynb#python-dictionary) of [key/value pairs](./key-terms.ipynb#key-value-pair) to store information related to that document. \n",
    "\n",
    "Essentially we will have a [list](./key-terms.ipynb#python-list) of documents numbered, from zero to the last document. Each [list](./key-terms.ipynb#python-list) item then will be composed of a [dictionary](./key-terms.ipynb#python-dictionary) of [key/value pairs](./key-terms.ipynb#key-value-pair) that allows us to retrieve information from that particular document by number. The structure will look something like this:\n",
    "\n",
    "![Structure of the corpus, a list of dictionaries](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CorpusView.png)\n",
    "\n",
    "For each item in our list we will be able to use [key/value pairs](./key-terms.ipynb#key-value-pair) to get a **value** if we supply a **key**. We will call our [Python list](./key-terms.ipynb#python-list) variable `all_documents` since it will contain all of the documents in our [corpus](./key-terms.ipynb#corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'shakespeareQuarterly.jsonl' #Replace with your filename and be sure your file is in your datasets folder\n",
    "\n",
    "import json #import the json module\n",
    "all_documents = [] #create an empty new list variable named `all_documents`\n",
    "with open('./datasets/' + fileName) as dataset_file: #temporarily open the file `filename` in the datasets/ folder\n",
    "    for line in dataset_file: #for each line in the dataset file\n",
    "        # Read each line into a Python dictionary.\n",
    "        document = json.loads(line) #create a variable document that contains the line using json.loads to convert the json key/value pairs to a python dictionary\n",
    "        all_documents.append(document) #append a new list item to `all_documents` containing the dictionary we created "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of our documents have been converted from our original [JSON lines](./key-terms.ipynb#jsonl) file format (.jsonl) into a [python List](./key-terms.ipynb#python-list) variable named `all_documents`. Let's see what we can discover about our [corpus](./key-terms.ipynb#corpus) with a few simple methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can determine how many texts are in our [dataset](./key-terms.ipynb#dataset) by using the `len()` function to get the size of `all_documents`. \n",
    "\n",
    "We can also choose a single document and get [bibliographic metadata](./key-terms.ipynb#bibliographic-metadata) for that item. Let's try the first document. (In computer code, 0 is the first item, 1 is the second item, 2 is the third item, etc.) In this case, we use the `.get` method to retrieve the title for our the first item in our [list](./key-terms.ipynb#python-list) `all_documents[0]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosenDocument = all_documents[0] #define a new dictionary variable `chosenDocument` that is equal to the first item in our `all_documents` list\n",
    "chosenDocument.get('title') #get the corresponding value for the key 'title'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `.get` method to discover additional [bibliographic metadata](./key-terms.ipynb#bibliographic-metadata). Here are the most significant [bibliographic metadata](./key-terms.ipynb#bibliographic-metadata) items found with a [JSTOR](./key-terms.ipynb#jstor) item:\n",
    "* `title` returns the title\n",
    "* `creators` returns the authors in a Python list\n",
    "* `isPartOf` returns the journal title\n",
    "* `datePublished` returns the publication date\n",
    "* `id` returns the stable URL for a JSTOR item\n",
    "* `identifier` returns a Python list of dictionaries containing the ISSN #, OCLC #, and DOI #. \n",
    "* `volumeNumber` returns the journal volume number\n",
    "* `pageCount` returns the number of pages in the print article\n",
    "* `pagination` returns the page number range of the print article\n",
    "* `pageStart` returns the first print page\n",
    "* `pageEnd` returns the last print page\n",
    "* `wordCount` returns the number of words in the article\n",
    "* `docType` returns the type of document, usually `article` for journal article\n",
    "* `url` returns the stable url for the document\n",
    "* `provider` returns the source of the data, for JSTOR articles usually `jstor`\n",
    "\n",
    "Let's try all these on our `chosenDocument`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Title: \" + chosenDocument.get('title'))\n",
    "print(chosenDocument.get('creators'))\n",
    "print(chosenDocument.get('isPartOf'))\n",
    "print(chosenDocument.get('datePublished'))\n",
    "print(chosenDocument.get('publisher'))\n",
    "print(chosenDocument.get('id'))\n",
    "print(chosenDocument.get('identifier'))\n",
    "print(chosenDocument.get('volumeNumber'))\n",
    "print(chosenDocument.get('pageCount'))\n",
    "print(chosenDocument.get('pagination'))\n",
    "print(chosenDocument.get('pageStart'))\n",
    "print(chosenDocument.get('pageEnd'))\n",
    "print(chosenDocument.get('wordCount'))\n",
    "print(chosenDocument.get('docType'))\n",
    "print(chosenDocument.get('url'))\n",
    "print(chosenDocument.get('provider'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see every [Python dictionary](./key-terms.ipynb#python-dictionary) **key** in the [metadata](./key-terms.ipynb#metadata) by using the `.keys` method. We could use this in conjunction with the `print()` function, but we will use the `list()` function here to make it a little neater for reading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(chosenDocument.keys())# Uncomment the # in front of print to run this line of code\n",
    "list(chosenDocument.keys()) # Create a list of every Python dictionary key within `chosen_document`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we could also list all the [Python dictionary](./key-terms.ipynb#python-dictionary) **values**, but the output will be quite long since it includes the word counts for every word that is in the article. (In fact, it includes the count for every unique [string](./key-terms.ipynb#string) in the article. We'll address the distinction in the [word frequencies](./key-terms.ipynb#word-frequency) [notebooks](./key-terms.ipynb#jupyter-notebook).) The word counts are found within `unigramCount` which we'll address in the word frequencies notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(chosenDocument.values()) # Uncomment the # in front of list to run this line of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's return to our larger [corpus](./key-terms.ipynb#corpus) `all_documents` to do some exploratory analysis. What if we wanted to check if a particular item was in the [corpus](./key-terms.ipynb#corpus)?\n",
    "\n",
    "If we search out any journal article on jstor.org, the article description page will feature a stable url.\n",
    "\n",
    "![A JSTOR description page](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/jstorDescription.png)\n",
    "\n",
    "We already saw above that the stable URL is stored in both the `id` and `url` dictionaries, so we can check our whole [corpus](./key-terms.ipynb#corpsu) for a particular article if we know the stable URL. From the image above, we can see the article in question has a stable URL of: https://www.jstor.org/stable/2871420\n",
    "\n",
    "We can check whether the item above is in `all_documents` with the `in` or `not in` operators. First though, we need a list of all of the URLs in our corpus. We'll create a variable `list_of_urls` to hold all these values. Then we can check to see if our stable URL (http://www.jstor.org/stable/2871420) is in that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_urls = [] #we create a blank list that will contain all of the urls in our dataset\n",
    "for document in all_documents: #for every document in our dataset\n",
    "    url_value = document.get('url') #create a url_value variable to hold the URL for that document\n",
    "    list_of_urls.append(url_value) #append or add that URL to our Python list `list_of_urls`\n",
    "list(list_of_urls[0:5]) #show the first five items in our list of urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a [list](./key-terms.ipynb#python-list) of all the URLs in our [corpus](./key-terms.ipynb#corpus) in the `list_of_urls` variable, let's use the `in` operator to discover whether our text is in the [corpus](./key-terms.ipynb#corpus). If the article is in our [dataset](./key-terms.ipynb#dataset), we will receive `true`. If the article is not our [dataset](./key-terms.ipynb#dataset), we will receive `false`.\n",
    "\n",
    "*Note that the stable URL from jstor.org uses a secure address starting with \"https://\". Our dictionary values, however, use a standard address beginning with \"http://\".\n",
    "* http**s**://\n",
    "* http://\n",
    "\n",
    "You'll need to remove the \"s\" to run this test since our `list_of_urls` are not secure addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'http://www.jstor.org/stable/2871420' in list_of_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "toqiQRaZ6oJd"
   },
   "source": [
    "Now we have a good idea of what [metadata](./key-terms.ipynb#metadata) is in our [corpus](./key-terms.ipynb#corpus) and how we might retrieve it. We were able to use the `in` operator above to check if a particular article was in the [corpus](./key-terms.ipynb#corpus) using the URL. Of course, we could also check to see if a particular journal, author, publisher, or DOI # was in our corpus using a similar method. \n",
    "\n",
    "We'll finish this [notebook](./key-terms.ipynb#jupyter-notebook) by taking a big picture look at the [corpus](./key-terms.ipynb#corpus). What largescale patterns exist in this [corpus](./key-terms.ipynb#corpus) over the decades. We'll use [Pandas](./key-terms.ipynb#pandas) to help with our analysis. If you would like to learn more about [Pandas](./key-terms.ipynb#pandas), we recommend the [Python Pandas tutorial at learndatasci.com](https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/). For now, we will create a couple visualizations for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I9IRRS466oJU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd #imports pandas and allows us to call it with the phrase pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use [Pandas](./key-terms.ipynb#pandas), we need to import it first. The `as pd` lets us use the shorthand `pd` when we want to call [Pandas](./key-terms.ipynb#pandas) instead writing out the entire word `pandas`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHkQ1o7P6oJh"
   },
   "source": [
    "Now we can turn our [Python list](./key-terms.ipynb#python-list) `all_documents` into a [Pandas](./key-terms.ipynb#pandas) [dataframe](./key-terms.ipynb#pandas-dataframe). This will enable us to manipulate and view our data as a table or a graph. We will call our [dataframe](./key-terms.ipynb#pandas-dataframe) `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWaiiLtC6oJi"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rIDbi3SD6oJj"
   },
   "source": [
    "Let's see what our [corpus](./key-terms.ipynb#corpus) looks like in table form. We can use the `.head()` method to show us the first five rows of our data as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SIJ8H0pT6oJk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOkMFJ246oJl"
   },
   "source": [
    "Now we can see the first part of all the [metadata](./key-terms.ipynb#metadata) we have been discussing in table form. Much clearer! There's a lot of [metadata](./key-terms.ipynb#metadata) here so you will need to scroll right to see all the columns. Long items in this view are abbreviated with `...` to signify that they continue past what is shown.\n",
    "\n",
    "Let's use our new [Pandas](./key-terms.ipynb#pandas) [dataframe](./key-terms.ipynb#pandas-dataframe) to learn a little more about our [corpus](./key-terms.ipynb#corpus). Notice above there is column labeled `publicationYear`. Let's figure out the full year range of our [corpus](./key-terms.ipynb#corpus). We can do this by using the `.min` and `.max` methods. We'll create a variable to store each and then print them out.\n",
    "\n",
    "We can find the year range in our [Pandas](./key-terms.ipynb#pandas) [dataframe](./key-terms.ipynb#pandas-dataframe) by finding the minimum and maximum of `datePublished`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HkhgmqPo6oJm"
   },
   "outputs": [],
   "source": [
    "minYear = df['publicationYear'].min() #create variable `minYear` that is the minimum value from `publicationYear`\n",
    "maxYear = df['publicationYear'].max() #create variable `maxYear` that is the maximum value from `publicationYear`\n",
    "\n",
    "print(str(minYear) + ' to ' + str(maxYear)) #print a string showing \"minYear to maxYear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rtjvEi9n6oJn"
   },
   "source": [
    "Now we know the full year range of our [dataset](./key-terms.ipynb#dataset). Let's see if we can identify any trends across the decades.\n",
    "\n",
    "Since `decade` isn't a column in our [Pandas](./key-terms.ipynb#pandas) [dataframe](./key-terms.ipynb#pandas-dataframe), we'll need to create it. First though, we'll need to consider how to turn a date into a decade. Let's try an example.  To translate a year (1925) to a decade (1920), we need to subtract the final digit so it becomes a zero. Basically, we need a way to discover the final digit in each decade and then subtract it so the final digit of our date becomes a zero. Something like:\n",
    "\n",
    "`1925 - 5 = 0`\n",
    "\n",
    "We can find the value for the final digit in any particular case by using [modulo](./key-terms.ipynb#modulo) (which provides the remainder of a division). If we use `% 10` on a date, it should give us a remainder that is the ones digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1925 % 10 # What is the remainder of 1925 divided by 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will give us our ones digit. Now we subtract this calculation from our original date. The result gives us the decade number we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1925 - (1925 % 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can translate this example to the whole [dataframe](./key-terms.ipynb#pandas-dataframe) using the following code. We'll create a new function `add_decade` that takes a `value` from the `publicationYear` column and translates it into a decade column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWoq2gMt6oJn"
   },
   "outputs": [],
   "source": [
    "def add_decade(value): # create a function `add_decade` that takes an argument `value`\n",
    "    yr = int(value) # create a variable `yr` that turns value from a string into an integer\n",
    "    decade = yr - ( yr % 10 ) #create a variable `decade` that subtracts the ones digit as shown above\n",
    "    return decade #return the variable `decade` for the function `add_decade`\n",
    "\n",
    "df['decade'] = df['publicationYear'].apply(add_decade) \n",
    "# create a new column `decade` in our dataframe that is equal to \n",
    "# the column `publicationYear` after applying \n",
    "# the add_decade function we created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H79W5vIw6oJp"
   },
   "source": [
    "To see the new decade column we created in our data, let's use the `df.head()` method again to see how it changed the first 5 rows of the [dataframe](./key-terms.ipynb#pandas-dataframe). To see the decade column, you will need to scroll all the way to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKCjvDna6oJq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4lwdgRAf6oJr"
   },
   "source": [
    "For a final demonstration, we'll create two graphs of our [corpus](./key-terms.ipynb#corpus) using our new decade column. Let's look at the number of documents by decade first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upxLwnZr6oJs"
   },
   "outputs": [],
   "source": [
    "df.groupby(['decade', 'provider'])['id'].agg('count').unstack()\\\n",
    "    .plot.bar(title='Documents by decade', figsize=(20, 5), fontsize=12, stacked=True); ##There is a weird bug where this cell needs to be run twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNp2wN8I6oJt"
   },
   "source": [
    "And now let's look at the total page numbers by decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uifT8Ocy6oJu"
   },
   "outputs": [],
   "source": [
    "df.groupby(['decade', 'provider'])['pageCount'].agg('sum').unstack()\\\n",
    "    .plot.bar(title='Pages by decade', figsize=(20, 5), fontsize=12, stacked=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes our look at [bibliographic metadata](./key-terms.ipynb#bibliographic-metadata). Next we'll examine [word frequency](./key-terms.ipynb#word-frequency). \n",
    "___\n",
    "\n",
    "By <a href=\"https://nkelber.com\">Nathan Kelber</a> and Ted Lawless <br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2-metadata.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
