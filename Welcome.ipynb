{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdKRf0QMWlvK"
   },
   "source": [
    "By <a href=\"https://nkelber.com\">Nathan Kelber</a> <br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.\n",
    "___\n",
    "# Notebook Table of Contents\n",
    "\n",
    "* [About *JSTOR TDM*](#about)\n",
    "* [Why learn text mining](#why-learn)\n",
    "  * [Introduction](#introduction)\n",
    "* The questions text mining can help answer\n",
    "  * What's it about\n",
    "    * Word Frequency\n",
    "    * Collocation\n",
    "    * Topic Analysis\n",
    "    * TF-IDF\n",
    "  * How are they connected?\n",
    "    * Collocation\n",
    "    * Network Analysis\n",
    "  * How does it feel?\n",
    "    * Sentiment Analysis\n",
    "  * What names are in here?\n",
    "    * Named Entity Recognition\n",
    "  * How are they similar?\n",
    "    * Authorship Attribution\n",
    "    * Clustering\n",
    "    * Supervised Machine Learning\n",
    "* [Why Jupyter Notebooks](#intro-to-jupyter)\n",
    "* [Getting started with Jupyter Notebooks](#getting-started-with-jupyter)\n",
    "  * Creating and writing your first notebook\n",
    "  * Preparing a plain text dataset\n",
    "* [Text and Data Mining with Digital Scholar Workbench Datasets](#tdm-with-dsw)\n",
    "  * Importing Your Dataset into Jupyter Notebooks\n",
    "  * Metadata for your Dataset\n",
    "  * Word Frequencies\n",
    "  * Finding Significant Words (TF/IDF)\n",
    "  * Topic Modeling (LDA)\n",
    "* [TDM Key Terms and Concepts](#tdm-key-terms)\n",
    "* [Learn More](#learn-more)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEBCxfdVO1QQ"
   },
   "source": [
    "<a name =\"about\"></a>\n",
    "# About *JSTOR TDM*\n",
    "\n",
    "![JSTOR and Portico Logos](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/JSTORandPorticoLogo.png)\n",
    "\n",
    "Text mining, or the process of deriving new information from pattern and trend analysis of the written word, has the potential to revolutionize research across disciplines, but there is a massive hurdle facing those eager to unleash its power: the coding skills and statistical knowledge it requires can take years to develop.   All too often, researchers are shown the promise of text mining, but then told that promise can only be realized by the select few with the necessary technical skills.  Ted Underwood, Professor of English at the University of Illinois, analogizes this challenge by saying that analytics techniques are a “deceptively gentle welcome mat, followed by a trapdoor.”   \n",
    "\n",
    "JSTOR and Portico are addressing this problem by building a text and data mining platform aimed at teaching and enabling a generation of researchers to text mine.  The JSTOR & Portico text mining platform includes a user interface to allow researchers, students, and instructors to curate, visualize, and save custom datasets.  Researchers may download the extracted features of their curated datasets -- a non-consumptive “bag-of-words” where each journal issue or book in the custom dataset is represented with bibliographic metadata for the articles and chapters, the unique set of words on each page, the part of speech of each word, and the number of times the word occurs on the page. The platform includes a teaching and development environment (a Jupyter Hub) which will be populated with easy-to-use code tutorials and templates where new text miners can analyze their custom datasets and learn to modify the Python or R code to better suit their own research purposes.  Researchers may download and locally hold the extracted features of any content and the full-text of open content, while the full-text of rights restricted content will be available for analysis in a secure computing environment.\n",
    "\n",
    "The content in the text mining platform will at least include all of JSTOR and the content from those Portico publishers who choose to participate (currently, 30 publishers including John Wiley & Sons, Inc., Project Muse, Thieme Publishing Group, and Hindawi).  In addition, we are in discussions with third party content providers about participating with content and the service will include the ability for researchers to upload their own content for analysis.\n",
    "\n",
    "The JSTOR & Portico text mining service will provide both free tools and tools accessible exclusively for  institutional participants.  \n",
    "\n",
    "We are working with a set of ten reference institutions from late 2019 and through 2020 to identify and build all of the necessary features, with an aim to release the service in 2021.\n",
    "\n",
    "More information, including access to the prototype, may be found at: https://tdm-pilot.org.\n",
    "\n",
    "Any questions, discussion items, or requests for a demonstration may be sent to Amy Kirchhoff, Text and Data Mining Business Manager (amy.kirchhoff@ithaka.org).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a-t2J73JFY2b"
   },
   "source": [
    "# Why learn text mining? <a name=\"why-learn\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yd-T_5F9NSW1"
   },
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "You may have heard buzz on campus about text analysis, artificial intelligence, or big data. But if you’re a humanities scholar, librarian, or someone who has never used data in their research, it is not obvious why text analysis matters. Many disciplines have eschewed text analysis for decades (and we all know a few faculty members likely to continue for decades more), but there is no doubt that interest in the field is growing. Why should you learn text mining when you could be writing your next article?\n",
    "\n",
    "For researchers, the primary advantage that text and data mining offer is an ability to consider knowledge at non-human scales (both very big and very small). Text analysis can enable us to consider a million books across a thousand-dimensional space, revealing aspects of our records that are not obvious to human readers whether those aspects are imperceptibly small, diffused across centuries, or simply within records never read. What does that mean in practice though? The short answer is more evidence (and more kinds of evidence) for interrogating humanities problems. In [\"Searching for the Victorians\"](http://dancohen.org/2010/10/04/searching-for-the-victorians/) (2010), Dan Cohen asks, 'how much evidence is enough?': \n",
    "\n",
    ">Many humanities scholars have been satisfied, perhaps unconsciously, with the use of a limited number of cases or examples to prove a thesis. Shouldn’t we ask, like the Victorians, what can we do to be most certain about a theory or interpretation? If we use intuition based on close reading, for instance, is that enough?\n",
    ">\n",
    ">Should we be worrying that our scholarship might be anecdotally correct but comprehensively wrong? Is 1 or 10 or 100 or 1000 books an adequate sample to know the Victorians? What we might do with all of Victorian literature—not a sample, or a few canonical texts, as in Houghton’s work, but all of it.\n",
    "\n",
    "To operate as a researcher in the 21st century is to be confronted with the challenges and opportunities of data—at once both being overwhelmed by too much and yet not nearly enough of *the right kind*. As Miriam Posner has pointed out, \"...even if they don’t call their sources data, traditional humanists do have pretty pressing data-management needs\" ([\"Humanities Data: A Necessary Contradiction\"](https://miriamposner.com/blog/humanities-data-a-necessary-contradiction/) 2015). Tom Scheinfeldt suggests that data concerns are becoming the primary concern of the humanities:\n",
    "\n",
    ">The new technology of the Internet has shifted the work of a rapidly growing number of scholars away from thinking big thoughts to forging new tools, methods, materials, techniques, and modes or work that will enable us to harness the still unwieldy, but obviously game-changing, information technologies now sitting on our desktops and in our pockets. These concerns touch all scholars. [\"Sunset for Ideology, Sunrise for Methodology\"](http://dancohen.org/2010/10/04/searching-for-the-victorians/) (2008)\n",
    "\n",
    "Indeed, humanists cannot afford to ignore computational methods since they are, for better or worse, at the heart of modern culture and industry. Future humanists will not be able to study our digital present without becoming adept at reading and manipulating the burgeoning data of our historical record. Ted Underwood describes this new horizon:\n",
    "\n",
    ">It is becoming clear that we have narrated literary history as a sequence of discrete movements and periods because chunks of that size are about as much of the past as a single person could remember and discuss at one time. Apparently, longer arcs of change have been hidden from us by their sheer scale—just as you can drive across a continent noticing mountains and political boundaries but never the curvature of the earth. A single pair of eyes at ground level can't grasp the curve of the horizon, and arguments limited by a single reader's memory can't reveal the largest patterns organizing literary history. <br /> [*Distant Horizons: Digital Evidence and Literary Change*](https://www.press.uchicago.edu/ucp/books/book/chicago/D/bo35853783.html) (2019)\n",
    "\n",
    "For many scholars, text analysis sounds *potentially* powerful and useful, but the reality remains that learning text analysis is not a trivial task. Most humanities coursework does not prepare students to work with data. The good news is that text analysis, like any skill, can be learned to a greater or lesser degree. For historians to study the early modern period, it is very helpful to have a command of Latin. Still, there are plenty of successful early modern scholars that never learn the language (or learn enough to navigate the resources significant to their research).\n",
    "\n",
    "Depending on your research question, *you may not need to learn any coding* to do text analysis. The problem for many scholars is the possible applications for text analysis are not clear, so they are not in a good position to decide what to learn (and how much). At the same time, the sophistication needed for doing text analysis is a moving target. Topic modeling was once a very complicated task, requiring an understanding of the command line. Today, it can be accomplished in minutes using just a mouse. \n",
    "\n",
    "This notebook is intended to help researchers get started with text analysis by addressing the fundamental opportunity-cost question for doing this kind of research:\n",
    "\n",
    "* How can (and has) text analysis improve(d) scholarship?\n",
    "* What can I do with the knowledge I already have?\n",
    "* What method(s) could I learn quickly to advance my research?\n",
    "* What tools and resources are available to help?\n",
    "\n",
    "In this introduction, we explain the various kinds of text analysis for a general scholarly audience. What are they? Why would you use them? How long will it take to apply them? (The methods presented here are among the most well-known but certainly not exhaustive.) Afterward, you'll be prepared to decide how much or how little text analysis may be useful to your research. As you read about these methods, it will be helpful to keep in mind the current, intractable problems that face your field. Could you use one of these methods to address them? Along the way, we will reference recent scholarly arguments as examples and models. \n",
    "\n",
    "## What are these texts about?\n",
    "* **Word Frequency** *Little or no coding required* <br />\n",
    "Counting the frequency of a word in any given text. This includes Bag of Words and TF-IDF. **Example:** \"Which of these texts focus on women?\"\n",
    "\n",
    "* **Collocation** *Little or no coding required* <br />\n",
    "Examining where words occur close to one another. **Example:** \"Where are women mentioned in relation to home ownership?\"\n",
    "\n",
    "* **Topic Analysis (or Topic Modeling)** *Little or no coding required* <br />\n",
    "Discovering the topics within a group of texts. **Example:** \"What are the most frequent topics discussed in this newspaper?\"\n",
    "\n",
    "* **TF/IDF** *Little or no coding required* <br />\n",
    "Finding the significant words within a text. **Example:** \"What language is most significant within 1970s political speech?\"\n",
    "\n",
    "## How are these texts connected?\n",
    "* **Concordance** *Little or no coding required* <br />\n",
    "Where is this word or phrase used in these documents? **Example:** \"Which journal articles mention Maya Angelou's phrase, 'If you're for the right thing, then you do it without thinking.'\"\n",
    "* **Network Analysis** *Moderate coding required* <br />\n",
    "How are the authors of these texts connected? **Example:** \"What local communities formed around civil rights in 1963?\"\n",
    "\n",
    "## What emotions (or affects) are found within these texts?\n",
    "* **Sentiment Analysis** *Moderate coding required* <br />\n",
    "Does the author use positive or negative language? **Example:** \"How do presidents describe gun control?\"\n",
    "\n",
    "## What names are used in these texts?\n",
    "* **Named Entity Recognition** *Moderate coding required* <br />\n",
    "List every example of a kind of entity from these texts. **Example:** \"What are all of the geographic locations mentioned by Tolstoy?\"\n",
    "\n",
    "## Which of these texts are most similar?\n",
    "\n",
    "* **Authorship Attribution** *Moderate coding required* <br />\n",
    "Find the author of an anonymous document. **Example:** \"Who wrote The Federalist Papers?\"\n",
    "* **Clustering** *Moderate coding required* <br />\n",
    "Which texts are the most similar? **Example:** \"Is this play closer to comedy or tragedy?\"\n",
    "* **Supervised Machine Learning** *Moderate coding required* <br />\n",
    "Are there other texts similar to this? **Example:** \"Are there other Jim Crow laws like these we have already identified?\"\n",
    "\n",
    "The next section will examine each method in greater depth.\n",
    "___\n",
    "\n",
    "Cohen, Dan. [\"Searching for the Victorians.\"](http://dancohen.org/2010/10/04/searching-for-the-victorians/). (2010).\n",
    "\n",
    "Posner, Miriam. [\"Humanities Data: A Necessary Contradiction\"](https://miriamposner.com/blog/humanities-data-a-necessary-contradiction/). (2015).\n",
    "\n",
    "Scheinfeldt, Tom. [\"Sunset for Ideology, Sunrise for Methodology?\"](http://foundhistory.org/2008/03/sunset-for-ideology-sunrise-for-methodology/). (2008).\n",
    "\n",
    "Underwood, Ted. [*Distant Horizons: Digital Evidence and Literary Change*](https://www.press.uchicago.edu/ucp/books/book/chicago/D/bo35853783.html). (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8CPcwAuFPwXu"
   },
   "source": [
    "## What's it about?\n",
    "\n",
    "The most common reason to use text analysis is to summarize and/or describe the content of a collection of texts, also known as a [corpus](#corpus). The following methods can help researchers by:\n",
    "\n",
    "* Giving a broad overview of topics, themes, and language\n",
    "* Locating and ranking the significance of particular words and phrases\n",
    "* Discovering language in each document surrounding particular topics\n",
    "\n",
    "These methods are useful at getting a quick, high-level view of a large variety of materials. Assuming your dataset is ready to analyze (like those created by [JSTOR's Digital Scholar Workbench](https://tdm-pilot.org)), these methods can be executed easily with text analysis software and require little or no coding expertise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtdSja83IMIV"
   },
   "source": [
    "### Word Frequency<a name =\"wf-method\"></a>\n",
    "\n",
    "The [Word Frequency](#word-frequency) method counts the number of occurrences of individual words within a particular text. Each document is described as a set of words and their counts. [Word Frequency](#word-frequency) uses a [bag of words](#bag-of-words) model where the order of words is not significant. Just as the letters of a Scrabble game are tossed into a bag without order, word frequency merely records the number of occurences with no regard to where a particular word occurs within a document. \n",
    "\n",
    "For example, the ten most common words in Shakespeare's Hamlet can be represented in the following table:\n",
    "\n",
    "|  Word  | Count|\n",
    "| -------|:----:|\n",
    "| the    | 1148 |\n",
    "| and    | 970  | \n",
    "| to     | 764  |  \n",
    "| of     | 671  |\n",
    "| i      | 573  |\n",
    "| a      | 550  |\n",
    "| you    | 550  |\n",
    "| my     | 514  |\n",
    "| hamlet | 485  |\n",
    "| in     | 437  |\n",
    "\n",
    "To represent the whole text, our table would have to include all of the 4,728 unique words in the play. While these counts can be useful, the above list contains mostly [function words](#function-words), common words with little lexical meaning like articles, prepositions, and conjunctions. We can remove the [function words](#function-words) from our analysis using a [stop words](#stop-words) list. This is a list of common words we would like to filter out of our analysis. If we filter out common [function words](#function-words) in English, the result is:\n",
    "\n",
    "|   Word  | Count|\n",
    "| --------|:----:|\n",
    "| hamlet  | 485  |\n",
    "| lord    | 313  | \n",
    "| king    | 199  |  \n",
    "| horatio | 159  |\n",
    "| polonius| 124  |\n",
    "| claudius| 122  |\n",
    "| queen   | 121  |\n",
    "| shall   | 114  |\n",
    "| good    | 109  |\n",
    "| come    | 106  |\n",
    "\n",
    "Our new list contains more [content words](#content-words), yet it is dominated by character names. The reason for this is that a play-text contains speech headings before each line. The word \"hamlet\" is the most common word in the play because it is counted every time Hamlet has a speaking line. This may be useful information if we are interested in determining who has the most lines in the play. If that is not our goal, we can filter out character names by adding them to our [stop words](#stop-words) list. Then we get:\n",
    "\n",
    "|  Word |Count|\n",
    "| ------|:---:|\n",
    "| good  | 109 |\n",
    "| come  | 106 | \n",
    "| let   | 95  |  \n",
    "| like  | 85  |\n",
    "| sir   | 75  |\n",
    "| know  | 74  |\n",
    "| enter | 72  |\n",
    "| love  | 68  |\n",
    "| speak | 63  |\n",
    "| make  | 56  |\n",
    "\n",
    "Far from a truly objective viewpoint, text analysis is often about refining and tailoring your analysis. Refining a [stop words](#stop-words) list is an important part of obtaining useful results using [word frequencies]((#word-frequency).  The researcher must decide whether the filtering is adequate and appropriate. The answer always depends on the context of the argument. Is the above table appropriately filtered? The high frequency of the word \"enter\" is likely from stage directions instead of speaking lines. That may or may not be appropriate depending on the argument at hand.\n",
    "\n",
    "\n",
    "### Examples of Word Frequency\n",
    "\n",
    "One of the most popular kinds of visualization associated with the digital humanities is the tag cloud (or word cloud). A tag cloud visualizes word frequency by connecting the size of a word to its frequency. More common words are larger. \n",
    "![Word cloud of The Narrative of the Life of Frederick Douglass, An American Slave](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tagCloudDouglass.png)\n",
    "**Tag Cloud of *The Narrative of the Life of Frederick Douglass\n",
    "       An American Slave* created using Voyant.**\n",
    "In the visualization above, the ten most common words listed below are the largest words in the tag cloud.\n",
    "\n",
    "|Word   | Count |\n",
    "|----   |:-----:|\n",
    "| mr    |  168  |\n",
    "|slaves | 125   |\n",
    "|master | 124   |\n",
    "|slave  | 122   |\n",
    "|time   | 117   |\n",
    "|man    | 78    |\n",
    "|slavery| 69    |\n",
    "|covey  | 61    |\n",
    "|old    | 58    |\n",
    "|said   | 56    |\n",
    "\n",
    "Word frequencies can be counted within an individual work or compared across many works. In the visualization below, we can see the historical standardization of the spelling of the word \"love\" using the N-gram Browser of earlyprint.org. \n",
    "\n",
    "\n",
    "\n",
    "KEYWORD EXTRACTION WITH A CONSTRUCTED VOCABULARY\n",
    "How do we find publications that people aren't thinking about?\n",
    "\n",
    "### When should I use word frequency?\n",
    "\n",
    "One of the shortcomings of word frequency is that sometimes individual words are not useful units of analysis. For example, a social scientist may be interested in the relationship between nuclear families and drug offenses. Word frequency would allow us to search a thousand recent journal articles for the occurrences of \"nuclear,\" \"family,\" \"drug,\" and \"offense.\" But occurrences of \"nuclear\" could be about power plants and occurrences of \"offense\" may be about many other kinds of crime (or even sports). \n",
    "\n",
    "\"Nuclear family\" and \"drug offense\" are collocations. To address them, we'll need the method found in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1WwbXJ7JQVa"
   },
   "source": [
    "### Collocation\n",
    "\n",
    "An alternative to this approach is using [n-grams](#n-gram) which can capture phrases in addition to individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kegs0vqGJbYw"
   },
   "source": [
    "### Topic Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4aeJy3YJljK"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PQ1dQ1qRrm-"
   },
   "source": [
    "## How are they connected?\n",
    "\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATtZFWDPVXFs"
   },
   "source": [
    "### Concordance\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0CPpeThVaRN"
   },
   "source": [
    "### Network Analysis\n",
    " Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rMV0GzMRSCCc"
   },
   "source": [
    "## How does it feel?\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79X8F49tVnGE"
   },
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHvG69x1SQUu"
   },
   "source": [
    "## What names are in here? \n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DRzCO0K8Vz5H"
   },
   "source": [
    "### Named Entity Recognition\n",
    " Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pj90uywzSb5w"
   },
   "source": [
    "## How are they similar?\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDhqGL-0V8f8"
   },
   "source": [
    "### Authorship Attribution\n",
    "\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnB_EFD0V-qf"
   },
   "source": [
    "### Clustering\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQ1yEQeuWD65"
   },
   "source": [
    "### Supervised Machine Learning\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmtomRvRkf32"
   },
   "source": [
    "<a name =\"intro-to-jupyter\"></a>\n",
    "# Why learn Python?\n",
    "\n",
    "Lorem Ipsum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsaD_m83qEEm"
   },
   "source": [
    "## Why code in Jupyter Notebooks?\n",
    "\n",
    "Jupyter notebooks are documents that contain both computer code (e.g. python or R) and rich text elements (i.e. explanatory text, figures, links). Jupyter notebooks have two significant advantages for teaching and learning to code:\n",
    "* Minimal Setup\n",
    " * Traditional code editors may require students to become familiar with terminals, environments, libraries, etc. \n",
    " * With a Jupyter notebook, users can run code immediately.\n",
    "*\tRich Support Content\n",
    " * Code supports plain text commenting, but has a limited ability to include supporting explanation and information.\n",
    " * Jupyter notebooks allow instructors to embed many kinds of supporting content including text, images, equations, links, and videos.\n",
    "\n",
    "\"At present, one of [the] best ways of sharing or publishing these workflows might be with the free and open-source Jupyter Notebook.\" (Dobson 39)\n",
    "\n",
    "Examples from\n",
    "Programming Historian\n",
    "Ted Underwood's Book\n",
    "DHRI\n",
    "DHSI\n",
    "HILT\n",
    "\n",
    "\n",
    "\n",
    "Essentially, a Jupyter notebook is a file (.ipynb) that can be easily saved, uploaded, downloaded, and converted. (For example, a notebook file (.ipynb) can be converted into a python file (.py), HTML file (.html), or PDF file (.pdf).) Users edit Jupyter notebook files (.ipynb) with specialized software such as the Jupyter Notebook application. Yes, they unfortunately named the file type and the application \"Jupyter Notebook\" which is confusing. \n",
    "\n",
    "To clarify, *The* Jupyter Notebook is a browser application that runs *a* Jupyter Notebook. To clarify the difference, you may hear people refer to them as “the Jupyter Notebook app” and \"a Jupyter notebook.” Fortunately, future versions of the application will be called “JupyterLab” instead of “The Jupyter Notebook” which should alleviate some of this confusion.\n",
    "\n",
    "While Jupyter notebook files can be saved, viewed, and edited on your local machine using The Jupyter Notebook application, it is often helpful to connect Jupyter notebooks to a Jupyter server that contains the unique environment (e.g. the right kernel, dependencies) to execute the notebook’s code. Using a server ensures that the environment for executing the code is consistent and correct.\n",
    "\n",
    "If all of this is confusing, \n",
    "\n",
    "\n",
    "Other Jupyter servers include:\n",
    "\n",
    "* Jupyter Lab (Jupyter's new replacement for \"The Jupyter Notebook\")\n",
    "* Microsoft Azure Notebooks\n",
    "* Google Colab\n",
    "* Kaggle (popular with the data science community)\n",
    "\n",
    "The interfaces differ slightly between these Jupyter servers, but they are essentially the same software.\n",
    "\n",
    "How can I use a Jupyter notebook file?\n",
    "\n",
    "\n",
    "What computer languages do Jupyter notebooks support?\n",
    "The Jupyter system supports over 100 programming languages including Python, Java, R, Julia, Matlab, Octave, Scheme, Processing, Scala, and more. The most common languages used for text and data mining are python and R. \n",
    "How does the Digital Scholar Workbench connect to the TDM Jupyter notebooks?\n",
    "The Digital Scholar Workbench \n",
    "\n",
    "\n",
    "In order to use Jupyter notebooks (or any coding environment) for text and data mining, a user must have facility with the command line and install the appropriate dependencies. In order to make this work more accessible, we have deployed a JupyterHub server with kernels and depencies designed for text and data mining. Teachers and students simply sign in to the Digital Scholar Workbench using their JSTOR login, and they can write, edit, and run code immediately. Whether you are a novice or an advanced programmer, you can:\n",
    "●\tBuild replicable and shareable datasets in minutes\n",
    "●\tStart complex analyses like Topic Modeling or TF/IDF in minutes\n",
    "●\tCreate, edit, run, and share notebooks focused on particular TDM methods\n",
    "\n",
    "---\n",
    "Dobson, James E. [*Critical Digital Humanities*](https://www.press.uillinois.edu/books/catalog/48xfp2zp9780252042270.html). (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89btfps_lXEJ"
   },
   "source": [
    "<a name =\"tdm-key-terms\"></a>\n",
    "# TDM Key Terms and Concepts\n",
    "\n",
    "## Artificial Intelligence\n",
    "## Bag of Words (Model) <a name =\"bag-of-words\"></a>\n",
    "A model of texts that counts individual words without regard to grammatical location or phrases. Just as the letters of a Scrabble game are tossed into a bag without order, a \"bag of words\" model gathers all the words of a text into a \"bag\" with no regard to where a particular word occurs within the document. \n",
    "## Bigram <a name=\"bigram\"></a>\n",
    "An [n-gram](#n-gram) with a length of two. For example, \"chicken stock\" is a word bigram.\n",
    "## Bayesian Classification\n",
    "## Cleaning (Data)\n",
    "## Clustering\n",
    "## Collocation\n",
    "\n",
    "## Concordance\n",
    "## Content Words\n",
    "As opposed to [function words](#function-words) (e.g. articles, pronouns, conjuctions), content words (e.g. nouns, verbs, and adjectives) carry greater lexical meaning. Word frequency analysis typically attempts to filter out function words, in order to make content words more prominent. This filtering is accomplished with a [stop words](#stop-words) list.\n",
    "## Corpus <a name =\"corpus\"></a>\n",
    "A large (and often structured) collection of texts used for analysis. For example, all of the plays written by Shakespeare. A simple example might be a set of plain text files in a folder on your computer. A more complicated example may use XML, or another form of markup, to allow for deeper analysis. The plural form is corpora.\n",
    "\n",
    "See also [TEI XML](#tei-xml). \n",
    "## CSV (file) <a name=\"csv-file\"></a>\n",
    "A .csv file, or Comma-Separated Value file, is a simple format for storing structured data where each entry in the file is separated by a comma. Similarly, a [TSV file](#tsv-file) uses tabs to separate individual data entries. \n",
    "## Dataset\n",
    "A collection of information, usually computer files, used for statistical analysis. Most datasets are digital text (either numbers, words, or both), but they can also be other formats such as image, audio, and/or video content. Datasets are usually referred to as structured, semi-structured, or unstructured.\n",
    "Structured data fits into a predetermined format and can usually be represented by a table, spreadsheet, or relational database. \n",
    "Unstructured data is more freeform. For example, longform texts, audio, or video content are unstructured. \n",
    "Semi-structured data uses tags or elements to mark out structures within an unstructured data set. Email files, for example, have both structured aspects (Sender, Subject, etc.), but the body of an email is usually unstructured.\n",
    "## Discipline\n",
    "An academic field or body of knowledge taught and studied within colleges or universities. Generally academic disciplines are divided into three large groups: \n",
    "* The Humanities include disciplines like English, History, Law\n",
    "* The Sciences include disciplines like Physics, Biology, Mathematics\n",
    "* The Social Sciences include include disciplines like Anthropology, Economics, and Sociology\n",
    "\n",
    "Academic disciplines as divisions are matters of convenience for organizing departments, but many, if not most, professors research in two or more disciplines at a time. \n",
    "## Environment\n",
    "## Extracted Features\n",
    "## Function Words <a name=\"function-words\"></a>\n",
    "The words in a sentence that have little lexical meaning and express grammatical relationships. Function words include articles, pronouns, and conjunctions. When using a [word frequency](#word-frequency) approach, function words are often filtered out in favor of content words using a [stopwords](#stop-words) list. \n",
    "## Gensim\n",
    "## Google Colab <a name=\"google-colab\"></a>\n",
    "## HathiTrust\n",
    "## HathiTrust Research Center (HTRC)\n",
    "## JSTOR\n",
    "## JupyterHub <a name=\"jupyterhub\"></a>\n",
    "A multi-user version of [The Jupyter Notebook](#the-jupyter-notebook), ideal for teaching environments.\n",
    "## JupyterLab <a name=\"jupyterlab\"></a>\n",
    "The newest software from [Project Jupyter](#project-jupyter), intended to replace [The Jupyter Notebook](#the-jupyter-notebook), for executing and editing [Jupyter notebook](#jupyter-notebook) files.\n",
    "## Jupyter Notebook, The (software) <a name=\"the-jupyter-notebook\"></a>\n",
    "A single-user web application for executing and editing [Jupyter notebook files](#jupyter-notebook). Will be replaced by [JupyterLab](#jupyterlab).\n",
    "## Jupyter notebook (file) <a name=\"jupyter-notebook\"></a>\n",
    "A file with extension .ipynb that contains computer code (e.g. [Python](#python) or R) alongside other explanatory media (text, images, video). \n",
    "## Jupyter Server <a name=\"jupyter-serve\"></a>\n",
    "A server with the appropriate software environment (e.g. [JupyterHub](#jupyterhub), [JupyterLab](#jupyterlab), [Google Colab](#google-colab)) for running and editing [Jupyter notebooks](#jupyter-notebook).\n",
    "## Keyword Extraction\n",
    "## Latent Dirichlet Allocation (LDA)\n",
    "## Lemmatization <a name=\"lemmatization\"></a>\n",
    "## Library (in Python)\n",
    "A collections of methods and functions for achieving certain tasks (e.g. image manipulation, web scraping. This saves time since the code can be added quickly and all at once around a specific group of tasks. The [Natural Language Toolkit (NLTK)](#nltk) is a common library used in [natural language processing](#nlp).\n",
    "## Machine Learning\n",
    "## N-gram <a name =\"n-gram\"></a>\n",
    "A sequence of n items from a given sample of text or speech. Most often, this refers to a sequence of words, but it can also be used to analyze text at the level of syllables, letters, or phonemes. N-grams are often described by their length. For example, word n-grams might include:\n",
    "* stock (a 1-gram, or unigram)\n",
    "* chicken stock (a 2-gram, or [bigram](#bigram))\n",
    "* homemade chicken stock (a 3-gram, or [trigram](#trigram))\n",
    "A text analysis approach that looks only at unigrams at the word level will not be able to differentiate between the \"stock\" in \"stock market\" and \"chicken stock.\"\n",
    "\n",
    "One of the most popular examples of text analysis with n-grams is the [Google N-Gram Viewer](https://books.google.com/ngrams).\n",
    "\n",
    "See also [Natural Language Processing](#nlp). \n",
    "## Named Entity Recognition (NER)\n",
    "## Natural Language Processing (NLP) <a name=\"nlp\"></a>\n",
    "## Natural Language Toolkit (NLTK) <a name=\"nltk\"></a>\n",
    "A suite of libraries and programs for [Natural Language Processing](#nlp) written in [python](#python). NLTK includes libraries for tokening, collocation, n-grams, Part of Speech (POS) Tagging, and Named Entity Recognition (NER).\n",
    "\n",
    "See the [project documentation](https://www.nltk.org/) and book [Natural Language Processing with Python](http://www.nltk.org/book/).\n",
    "## Neural Net\n",
    "## Optical Character Recognition (OCR)\n",
    "## Package\n",
    "## Part of Speech (POS) Tagging <a name=\"pos-tagging\"></a>\n",
    "## Plain text\n",
    "## Portico\n",
    "## Parts of Speech (POS) Tagging <a name=\"pos-tagging\"></a>\n",
    "## Primary Source\n",
    "## Project Jupyter <a name=\"project-jupyter\"></a>\n",
    "A non-profit that develops open-source software, open standards, and services across many programming languages. They are most well-known for software such as [The Jupyter Notebook](#the-jupyter-notebook), [JupyterLab](#jupyterlab), and [JupyterHub](#jupyterhub). All three of these programs are used to create, edit, and share programming notebooks, known as [Jupyter notebooks](#jupyter-notebook).\n",
    "## Python (Programming Language) <a name=\"python\"></a>\n",
    "\n",
    "## R (Programming Language) <a name=\"r\"></a>\n",
    "## Secondary Source\n",
    "## Sentiment Analsis\n",
    "## Stop Words (List) <a name=\"stop-words\"></a>\n",
    "A stop words list is a set of words or phrases that are ignored in [word frequency](#word-frequency) analysis. It is common for a researcher who is interested in prominent nouns and verbs to remove [function words](#function-words) (e.g. the, and, I, to, of, a). A stop word list may also include other common words, such as character names which are usually the most common words in a play text.\n",
    "## Tag Cloud (or Word Cloud)<a name =\"tag-cloud\"></a>\n",
    "A tag cloud is a visualization of the relative word frequencies in a [corpus](#corpus). The relative size of each word in a tag cloud depends on its frequency within a text. Larger words occur more frequently.\n",
    "\n",
    "![Tag Cloud of The Narrative of the Life of Frederick Douglass\n",
    "       An American Slave](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tagCloudDouglass.png)\n",
    "**A Tag Cloud of *The Narrative of the Life of Frederick Douglass\n",
    "       An American Slave* generated using Voyant.**\n",
    "## TEI XML <a name =\"tei-xml\"></a>\n",
    "A form of [XML Markup](#xml), or tagging, created by the [Text Encoding Initiative](https://tei-c.org/) to describe digital documents. This markup can help computers recognize particular aspects of the text. Text analysis often requires explicit marking, even for textual aspects that a human reader can easily pick out:\n",
    "* Title\n",
    "* Author Name\n",
    "* Name of the speaker in a play\n",
    "* A paragraph\n",
    "* The speaker in a play\n",
    "* Stage directions\n",
    "* A stanza\n",
    "\n",
    "See also [Parts of Speech Tagging](#pos-tagging), [Lemmatization](#lemmatization), [Tokenization](#tokenization).\n",
    "## Term Frequency\n",
    "## Term Frequency-Inverse Document Frequency (TFIDF)\n",
    "## Text Extraction\n",
    "## Token\n",
    "## Tokenization <a name=\"tokenization\"></a>\n",
    "## Topic Modeling (or Topic Analysis)\n",
    "## Tree Map\n",
    "## Trigram <a name=\"trigram\"></a>\n",
    "An [n-gram](#n-gram) with a length of three. For example, \"homemade chicken stock\" is a word trigram.\n",
    "## TSV (file) <a name=\"tsv-file\"></a>\n",
    "A .tsv file, or Tab-Separated Value file, is a simple format for storing structured data where each entry in the file is separated by a tab. Similarly, a [CSV file](#csv-file) uses commas to separate individual data entries.\n",
    "## Unigram\n",
    "## Voyant\n",
    "## Word2vec\n",
    "## Word Cloud<a name=\"word-cloud\"></a>\n",
    "See [Tag Cloud](#tag-cloud).\n",
    "## Word Embedding\n",
    "## Word Frequency <a name=\"word-frequency\"></a>\n",
    "A text analysis method that counts the number of occurences of individual words within a particular text. Word frequency uses a [bag of words](#bag-of-words) model where the order of words is not significant. Just as the letters of a Scrabble game are tossed into a bag without order, word frequency merely records the number of occurences with no regard to where a particular word occurs within a document. \n",
    "\n",
    "An alternative to this approach is using [n-grams](#n-gram) which can capture phrases in addition to individual words.\n",
    "\n",
    "Read more about [Word Frequency](#wf-method). \n",
    "## XML <a name=\"xml\"></a>\n",
    "Short for (eXtensible markup language), XML uses tags to identify parts of a document for a machine to understand. Like HTML, these tags have an opening tag (e.g. <l>) and a closing tag marked by a forward slash (e.g. </l>). Unlike HTML, these tags can be freely created according to whatever standard the creator needs. One prominent example is the [Text Encoding Initiative](https://tei-c.org/). The example below uses [TEI-XML](#tei-xml) to describe Shakespeare's Sonnet 130 by labeling lines, quatrains, and the final couplet. This kind of markup enables computers to do complex analysis quickly such as comparing every couplet, quatrain, or line in Shakespeare's sonnets.\n",
    "```\n",
    "<text>\n",
    " <body>\n",
    "  <lg>\n",
    "   <lg type=\"quatrain\">\n",
    "    <l>My Mistres eyes are nothing like the Sunne,</l>\n",
    "    <l>Currall is farre more red, then her lips red</l>\n",
    "    <l>If snow be white, why then her brests are dun:</l>\n",
    "    <l>If haires be wiers, black wiers grown on her head:</l>\n",
    "   </lg>\n",
    "   <lg type=\"quatrain\">\n",
    "    <l>I have seene Roses damaskt, red and white,</l>\n",
    "    <l>But no such Roses see I in her cheekes,</l>\n",
    "    <l>And in some perfumes is there more delight,</l>\n",
    "    <l>Then in the breath that from my Mistres reekes.</l>\n",
    "   </lg>\n",
    "   <lg type=\"quatrain\">\n",
    "    <l>I love to heare her speake, yet well I know,</l>\n",
    "    <l>That Musicke hath a farre more pleasing sound:</l>\n",
    "    <l>I graunt I never saw a goddesse goe,</l>\n",
    "    <l>My Mistres when shee walkes treads on the ground.</l>\n",
    "   </lg>\n",
    "  </lg>\n",
    "  <lg type=\"couplet\">\n",
    "   <l>And yet by heaven I think my love as rare,</l>\n",
    "   <l>As any she beli'd with false compare.</l>\n",
    "  </lg>\n",
    " </body>\n",
    "</text>\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KrOd5K9p6kSB"
   },
   "source": [
    "<a name =\"learn-more\"></a>\n",
    "# Learn More\n",
    "\n",
    "### Intensive Institutes (Travel required)\n",
    "* [Digital Humanities Summer Institute](https://dhsi.org)\n",
    "* [Digital Humanities Research Institute](http://dhinstitutes.org/)\n",
    "* [Humanities Intensive Learning and Teaching](http://dhtraining.org/hilt/)\n",
    "* [Data Matters](http://datamatters.org)\n",
    "\n",
    "\n",
    "## Python Tutorials by Discipline\n",
    "### Humanities\n",
    "* [Python Programming for the Humanities (Folgert Karsdorp)](http://www.karsdorp.io/python-course/)\n",
    "* [Intro to Python Workshop (Digital Humanities Research Institute)](https://github.com/DHRI-Curriculum/python)\n",
    "* [Intro to Python I and II (University Libraries UNC Chapel Hill)](https://unc-libraries-data.github.io/Python/)\n",
    "* [Python Lessons (The Programming Historian)](https://programminghistorian.org/en/lessons/?topic=python)\n",
    "\n",
    "### Libraries\n",
    "* [Python Intro for Libraries (Library Carpentry)](https://librarycarpentry.org/lc-python-intro/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "WelcomeTDM",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
