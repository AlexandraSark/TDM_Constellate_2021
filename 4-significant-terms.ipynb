{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding significant words within a curated dataset\n",
    "\n",
    "This notebook demonstrates how to find the significant words in your dataset using [tf-idf](./key-terms.ipynb#tf-idf). The following processes are described:\n",
    "\n",
    "* Importing your [dataset](./key-terms.ipynb#dataset)\n",
    "* Finding your initial query within your [dataset's](./key-terms.ipynb#dataset) metadata\n",
    "* Writing a helper function to help clean up a single [token](./key-terms.ipynb#token)\n",
    "* Cleaning each document of your dataset, one [token](./key-terms.ipynb#token) at a time\n",
    "* Using a dictionary of English words to remove words with poor [OCR](./key-terms.ipynb#ocr)\n",
    "* Computing the most significant words in your [corpus](./key-terms.ipynb#corpus) using [TFIDF](./key-terms.ipynb#tf-idf) with the [gensim](./key-terms.ipynb#gensim) library\n",
    "\n",
    "A familiarity with gensim is helpful but not required.\n",
    "____\n",
    "We import the `Dataset` module from the `tdm_client` library. The tdm_client library contains functions for connecting to the JSTOR server containing our [corpus](./key-terms.ipynb#corpus) [dataset](./key-terms.ipynb#dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm_client import Dataset\n",
    "from tdm_client import htrc_corrections, htrc_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also import `warnings` to XXXXX and [gensim](https://radimrehurek.com/gensim/index.html), a Python library to help us calculate the significant words in our text using using the [TFIDF](./key-terms.ipynb#tf-idf) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze your dataset, use the [dataset ID](./key-terms.ipynb/#dataset-ID) provided when you created your [dataset](./key-terms.ipynb/#dataset). A copy of your [dataset ID](./key-terms.ipynb/#dataset-ID) was sent to your email when you created your [corpus](./key-terms.ipynb#corpus). It should look like a long series of characters surrounded by dashes.  \n",
    "\n",
    "We create a new variable **dset** and initialize its value using the **Dataset** function. A sample **dataset ID** of journals focused on Shakespeare is provided here ('a517ef1f-0794-48e4-bea1-ac4fb8b312b4'). Pasting your unique **dataset ID** here will import your dataset from the JSTOR server.\n",
    "\n",
    "**Note**: If you are curious what is in your dataset, there is a download link in the email you received. The format and content of the files is described in the notebook [Building a Dataset](./1-building-a-dataset.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset('a517ef1f-0794-48e4-bea1-ac4fb8b312b4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find total number of documents in the dataset using the `len()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if this is the correct dataset, we can look at the original query by using the query attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q=shakespeare&start=0&rows=20&fq=yearPublished%3A%5B1900%20TO%202019%5D&fq=category%3A(%22Literature%20(General)%22%20OR%20%22English%20literature%22)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This string is part of the URL used for your initial search. It is written in  It is normally interpreted by the computer, but we can parse it if we keep in mind a few rules:\n",
    "\n",
    "* Each part of the query is separated by an `&`\n",
    "* It uses URL enconding to represent characters. Where there is a `%`, a special character is being encoded:\n",
    "    * %20 is a single space ` `\n",
    "    * %3A is a `:`\n",
    "    * %5B is a ``[``\n",
    "    * %5D is a `]`\n",
    "\n",
    "Alternatively, we could decode the URL using `urllib` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q=shakespeare&start=0&rows=20&fq=yearPublished:[1900 TO 2019]&fq=category:(\"Literature (General)\" OR \"English literature\")'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.parse\n",
    "encodedStr = 'q=shakespeare&start=0&rows=20&fq=yearPublished%3A%5B1900%20TO%202019%5D&fq=category%3A(%22Literature%20(General)%22%20OR%20%22English%20literature%22)'\n",
    "urllib.parse.unquote(encodedStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example:\n",
    "* the original query was `shakespeare`\n",
    "* published from `1900 to 2019`\n",
    "* found within the categories `Literature (General)` or `English literature`\n",
    "___\n",
    "\n",
    "Now that we've verified that we have the correct corpus, let's create a helper function that can standardize and [clean](./key-terms.ipynb#clean-data) up the [tokens](./key-terms.ipynb#token) in our [dataset](./key-terms.ipynb#dataset). The function will:\n",
    "* lower cases all tokens\n",
    "* use an HTRC dictionary to correct common OCR problems\n",
    "* discard tokens less than 4 characters in length\n",
    "* discard tokens with non-alphabetical characters\n",
    "* remove stopwords based on an HTRC stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token(token): #define a function `process_token` that takes the argument `token`\n",
    "    token = token.lower() #set the string in token to a new string with all lowercase letters\n",
    "    corrected = htrc_corrections.get(token) #initialize a new variable `corrected` that runs toke through the `htrc_corrections.get()` function to fix common OCR errors\n",
    "    if corrected is not None: #if corrected has a value, set the `token` variable to the same value as `corrected`\n",
    "        token = corrected\n",
    "    if len(token) < 4: #if token is less than four characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    if not(token.isalpha()): #if token contains non-alphabetic characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    return token #return the `token` variable which has been set equal to the `corrected` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cycle through each document in the [corpus](./key-terms.ipynb#corpus) with our helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [] #Create a new variable `documents` that is a list\n",
    "\n",
    "for doc_n, volume in enumerate(dset.get_features()): #for each \n",
    "    this_doc = [] #create a new variable `this_doc` that is a list\n",
    "    try:\n",
    "        pages = volume['features']['pages']\n",
    "    except KeyError:\n",
    "        continue\n",
    "    for pn, page in enumerate(pages):\n",
    "        body = page.get('body')\n",
    "        if body is not None:\n",
    "            for token, pos_count in body.get('tokenPosCount', {}).items():\n",
    "                clean_token = process_token(token)\n",
    "                if clean_token is None:\n",
    "                    continue\n",
    "                for pos, n in pos_count.items():\n",
    "                    this_doc += [clean_token] * n\n",
    "    documents.append(this_doc)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most significant terms, by TFIDF, in the curated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = {\n",
    "        dictionary.get(_id): value for doc in corpus_tfidf\n",
    "        for _id, value in doc\n",
    "    }\n",
    "sorted_td = sorted(td.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rihll 0.8789730103989494\n",
      "ffarington 0.8687586142454262\n",
      "lācis 0.8243941724639476\n",
      "vennar 0.8163770278745967\n",
      "springthorpe 0.7756033880521054\n",
      "dowson 0.7319115494098776\n",
      "ansori 0.7275629612915525\n",
      "haggai 0.6938212666496392\n",
      "mordred 0.6741468905424344\n",
      "gambuh 0.6694600905272786\n",
      "hynde 0.6476993218928895\n",
      "mbti 0.6373307886940024\n",
      "naatsilanei 0.612118444994542\n",
      "pgends 0.6074158461930421\n",
      "pgvar 0.6074158461930421\n",
      "teena 0.6066638524325301\n",
      "bunting 0.6061238433470649\n",
      "londesbr 0.605012939081335\n",
      "ramlila 0.5986739623074735\n",
      "crimewatch 0.5864962542111738\n",
      "bulworth 0.5685071371122152\n",
      "mccrea 0.559153117887106\n",
      "ncox 0.5589374086582615\n",
      "edib 0.5561714547394812\n",
      "mayella 0.5523679319537927\n"
     ]
    }
   ],
   "source": [
    "for term, weight in sorted_td[:25]:\n",
    "    print(term, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the most significant word, by TFIDF, for the first 50 documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.jstor.org/stable/i40075057 sophie 0.31074312791735936\n",
      "http://www.jstor.org/stable/i40103856 hodgetts 0.3584969280241884\n",
      "http://www.jstor.org/stable/i40075051 tercentenary 0.25790641235753525\n",
      "http://www.jstor.org/stable/i40075048 harvey 0.2823835999735381\n",
      "http://www.jstor.org/stable/i40075029 siddons 0.4804214701488567\n",
      "http://www.jstor.org/stable/i40075043 mutran 0.4860344553470682\n",
      "http://www.jstor.org/stable/i40075049 hathaway 0.37880283509018037\n",
      "http://www.jstor.org/stable/i40103854 faucit 0.5626181311237378\n",
      "http://www.jstor.org/stable/i24712320 hawkes 0.5600912328304873\n",
      "http://www.jstor.org/stable/i40180516 péguy 0.8972513573836641\n",
      "http://www.jstor.org/stable/i40075016 blackmore 0.18185351011005418\n",
      "http://www.jstor.org/stable/i23917916 zarathustra 0.6152358652346409\n",
      "http://www.jstor.org/stable/i338528 designer 0.1496299927861339\n",
      "http://www.jstor.org/stable/i24712323 prufrock 0.3850736252470401\n",
      "http://www.jstor.org/stable/i40074765 shak 0.17726733854453525\n",
      "http://www.jstor.org/stable/i338415 festival 0.15795298784320438\n",
      "http://www.jstor.org/stable/i338524 winnie 0.36466455352663424\n",
      "http://www.jstor.org/stable/i24775424 favourites 0.17800868407584217\n",
      "http://www.jstor.org/stable/i338540 jenkins 0.16976539512895675\n",
      "http://www.jstor.org/stable/i338541 prospero 0.2701526488048237\n",
      "http://www.jstor.org/stable/i338502 darley 0.219385818565172\n",
      "http://www.jstor.org/stable/i338434 stints 0.25897120526560985\n",
      "http://www.jstor.org/stable/i338551 erasable 0.4798278399596801\n",
      "http://www.jstor.org/stable/i338425 mosby 0.4154072413313824\n",
      "http://www.jstor.org/stable/i371506 yvan 0.4751671908015018\n",
      "http://www.jstor.org/stable/i383532 clennam 0.3129099428760377\n",
      "http://www.jstor.org/stable/i40197775 banky 0.21844964417994747\n",
      "http://www.jstor.org/stable/i40074931 portrayal 0.258005764040775\n",
      "http://www.jstor.org/stable/i40197776 belinda 0.500309600133013\n",
      "http://www.jstor.org/stable/i338510 chatterton 0.6851339634262732\n",
      "http://www.jstor.org/stable/i40074947 teats 0.5770511035820868\n",
      "http://www.jstor.org/stable/i40103855 crimewatch 0.5864962542111738\n",
      "http://www.jstor.org/stable/i40074938 sobran 0.319318430769077\n",
      "http://www.jstor.org/stable/i40075027 gaunt 0.19800465677422135\n",
      "http://www.jstor.org/stable/i345579 primaleon 0.5726831196740602\n",
      "http://www.jstor.org/stable/i24777499 erath 0.474019148587382\n",
      "http://www.jstor.org/stable/i338559 lionheart 0.4318860204895387\n",
      "http://www.jstor.org/stable/i40194909 mouffet 0.35682273569724665\n",
      "http://www.jstor.org/stable/i40154819 vittoria 0.3142035075765928\n",
      "http://www.jstor.org/stable/i338428 keats 0.3600579602638076\n",
      "http://www.jstor.org/stable/i26350931 lucrece 0.3287635684949627\n",
      "http://www.jstor.org/stable/i30210587 schreber 0.41228609324232673\n",
      "http://www.jstor.org/stable/i40197764 strachey 0.4233560686778518\n",
      "http://www.jstor.org/stable/i338544 espearean 0.19793844926937468\n",
      "http://www.jstor.org/stable/i24775380 ahab 0.23786078280452203\n",
      "http://www.jstor.org/stable/i40007101 hoby 0.5590449009850061\n",
      "http://www.jstor.org/stable/i26350421 hidetora 0.2167189801271919\n",
      "http://www.jstor.org/stable/i338411 nedham 0.2942070173414242\n",
      "http://www.jstor.org/stable/i40075074 teled 0.27278524831836315\n",
      "http://www.jstor.org/stable/i26350426 shak 0.8743053293541777\n"
     ]
    }
   ],
   "source": [
    "for n, doc in enumerate(corpus_tfidf):\n",
    "    if len(doc) < 1:\n",
    "        continue\n",
    "    word_id, score = max(doc, key=lambda x: x[1])\n",
    "    print(dset.items[n], dictionary.get(word_id), score)\n",
    "    if n >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
