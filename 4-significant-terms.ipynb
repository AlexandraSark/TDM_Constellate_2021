{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding significant words within a curated dataset\n",
    "\n",
    "This notebook demonstrates how to find the significant words in your dataset using [tf-idf](./key-terms.ipynb#tf-idf). The following processes are described:\n",
    "\n",
    "* Importing your [dataset](./key-terms.ipynb#dataset)\n",
    "* Finding your initial query within your [dataset's](./key-terms.ipynb#dataset) metadata\n",
    "* Writing a helper function to help clean up a single [token](./key-terms.ipynb#token)\n",
    "* Cleaning each document of your dataset, one [token](./key-terms.ipynb#token) at a time\n",
    "* Using a dictionary of English words to remove words with poor [OCR](./key-terms.ipynb#ocr)\n",
    "* Computing the most significant words in your [corpus](./key-terms.ipynb#corpus) using [TFIDF](./key-terms.ipynb#tf-idf) with the [gensim](./key-terms.ipynb#gensim) library\n",
    "\n",
    "A familiarity with gensim is helpful but not required.\n",
    "____\n",
    "We import the `Dataset` module from the `tdm_client` library. The tdm_client library contains functions for connecting to the JSTOR server containing our [corpus](./key-terms.ipynb#corpus) [dataset](./key-terms.ipynb#dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim\n",
    "\n",
    "from tdm_client import Dataset\n",
    "from tdm_client import htrc_corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze your dataset, use the [dataset ID](./key-terms.ipynb/#dataset-ID) provided when you created your [dataset](./key-terms.ipynb/#dataset). A copy of your [dataset ID](./key-terms.ipynb/#dataset-ID) was sent to your email when you created your [corpus](./key-terms.ipynb#corpus). It should look like a long series of characters surrounded by dashes.  \n",
    "\n",
    "We create a new variable **dset** and initialize its value using the **Dataset** function. A sample **dataset ID** of journals focused on Shakespeare is provided here ('a517ef1f-0794-48e4-bea1-ac4fb8b312b4'). Pasting your unique **dataset ID** here will import your dataset from the JSTOR server.\n",
    "\n",
    "**Note**: If you are curious what is in your dataset, there is a download link in the email you received. The format and content of the files is described in the notebook [Building a Dataset](./1-building-a-dataset.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset('59c090b6-3851-3c65-e016-9181833b4a2c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find total number of documents in the dataset using the `len()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6687"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if this is the correct dataset, we can look at the original query by using the query attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q=*%3A*&fq=yearPublished%3A%5B1700%20TO%202019%5D&fq=-provider%3Aportico&fq=isPartOf%3A(%22Shakespeare%20Quarterly%22)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This string is part of the URL used for your initial search. It is written in  It is normally interpreted by the computer, but we can parse it if we keep in mind a few rules:\n",
    "\n",
    "* Each part of the query is separated by an `&`\n",
    "* It uses URL enconding to represent characters. Where there is a `%`, a special character is being encoded:\n",
    "    * %20 is a single space ` `\n",
    "    * %3A is a `:`\n",
    "    * %5B is a ``[``\n",
    "    * %5D is a `]`\n",
    "\n",
    "Alternatively, we could decode the URL using `urllib` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q=shakespeare&start=0&rows=20&fq=yearPublished:[1900 TO 2019]&fq=category:(\"Literature (General)\" OR \"English literature\")'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.parse\n",
    "encodedStr = 'q=shakespeare&start=0&rows=20&fq=yearPublished%3A%5B1900%20TO%202019%5D&fq=category%3A(%22Literature%20(General)%22%20OR%20%22English%20literature%22)'\n",
    "urllib.parse.unquote(encodedStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example:\n",
    "* the original query was `shakespeare`\n",
    "* published from `1900 to 2019`\n",
    "* found within the categories `Literature (General)` or `English literature`\n",
    "___\n",
    "\n",
    "Now that we've verified that we have the correct corpus, let's create a helper function that can standardize and [clean](./key-terms.ipynb#clean-data) up the [tokens](./key-terms.ipynb#token) in our [dataset](./key-terms.ipynb#dataset). The function will:\n",
    "* lower cases all tokens\n",
    "* use an HTRC dictionary to correct common OCR problems\n",
    "* discard tokens less than 4 characters in length\n",
    "* discard tokens with non-alphabetical characters\n",
    "* remove stopwords based on an HTRC stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token(token): #define a function `process_token` that takes the argument `token`\n",
    "    token = token.lower() #set the string in token to a new string with all lowercase letters\n",
    "    corrected = htrc_corrections.get(token) #initialize a new variable `corrected` that runs toke through the `htrc_corrections.get()` function to fix common OCR errors\n",
    "    if corrected is not None: #if corrected has a value, set the `token` variable to the same value as `corrected`\n",
    "        token = corrected\n",
    "    if len(token) < 4: #if token is less than four characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    if not(token.isalpha()): #if token contains non-alphabetic characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    return token #return the `token` variable which has been set equal to the `corrected` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cycle through each document in the [corpus](./key-terms.ipynb#corpus) with our helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [] #Create a new variable `documents` that is a list\n",
    "\n",
    "for n, unigram_count in enumerate(dset.get_features()):\n",
    "    this_doc = []\n",
    "    for token, count in unigram_count.items():\n",
    "        clean_token = process_token(token)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    documents.append(this_doc)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most significant terms, by TFIDF, in the curated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = {\n",
    "        dictionary.get(_id): value for doc in corpus_tfidf\n",
    "        for _id, value in doc\n",
    "    }\n",
    "sorted_td = sorted(td.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ofamiem 1.0\n",
      "cwtrnca 0.9816048327406713\n",
      "zamir 0.909664366407156\n",
      "reprint 0.8912794505341458\n",
      "ouderdom 0.8907547566168921\n",
      "worken 0.889452318558063\n",
      "falocco 0.8735400172020873\n",
      "werken 0.8619900346951035\n",
      "weingust 0.8366952585903492\n",
      "witmore 0.8101754277391725\n",
      "falco 0.7687425271759438\n",
      "womersley 0.7673648412212968\n",
      "wynkyn 0.7478190668986658\n",
      "honan 0.747047406836647\n",
      "unton 0.7389050311436763\n",
      "willeford 0.7313129489069663\n",
      "mebane 0.7205366793489603\n",
      "demea 0.717697038586963\n",
      "matz 0.7110635758552036\n",
      "alceste 0.7093984204685807\n",
      "whitefriars 0.7043184902613787\n",
      "athelstan 0.7040717360868357\n",
      "hrotsvits 0.70280612009222\n",
      "foulkes 0.7017590805228023\n",
      "playbookes 0.6942220407478248\n"
     ]
    }
   ],
   "source": [
    "for term, weight in sorted_td[:25]:\n",
    "    print(term, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the most significant word, by TFIDF, for the first 50 documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.jstor.org/stable/24778431 handkerchief 0.5104825155260231\n",
      "http://www.jstor.org/stable/24778442 altman 0.5338518939832136\n",
      "http://www.jstor.org/stable/24778441 shakespeareans 0.26752472146914286\n",
      "http://www.jstor.org/stable/44990760 zeeb 0.19243734202627377\n",
      "http://www.jstor.org/stable/44990258 folger 0.20695590614013684\n",
      "http://www.jstor.org/stable/44991483 item 0.6937914194847521\n",
      "http://www.jstor.org/stable/44990809 institute 0.2708117216896589\n",
      "http://www.jstor.org/stable/44990755 item 0.6338561247748161\n",
      "http://www.jstor.org/stable/44990251 jackson 0.20811283511274925\n",
      "http://www.jstor.org/stable/44990806 facsimile 0.15689657515431293\n",
      "http://www.jstor.org/stable/44990805 item 0.5122766995683712\n",
      "http://www.jstor.org/stable/2866476 entrance 0.5893884586749324\n",
      "http://www.jstor.org/stable/2866484 ducis 0.446803200335144\n",
      "http://www.jstor.org/stable/2866493 professor 0.35995179197163296\n",
      "http://www.jstor.org/stable/2868727 survey 0.4219999222625147\n",
      "http://www.jstor.org/stable/2868724 dissertations 0.22899351473515442\n",
      "http://www.jstor.org/stable/2868721 separately 0.28109344571395056\n",
      "http://www.jstor.org/stable/2868723 concordance 0.3918808930813103\n",
      "http://www.jstor.org/stable/2868722 klose 0.28742796638689994\n",
      "http://www.jstor.org/stable/2868716 menenius 0.3915740512968717\n",
      "http://www.jstor.org/stable/2868796 africke 0.19471550664959467\n",
      "http://www.jstor.org/stable/2868819 medallions 0.33036001775733687\n",
      "http://www.jstor.org/stable/24778474 transference 0.5490317574341341\n",
      "http://www.jstor.org/stable/24778487 johns 0.20733268638188268\n",
      "http://www.jstor.org/stable/24778476 anthony 0.47683047812804824\n",
      "http://www.jstor.org/stable/24778461 bevington 0.3590039480226712\n",
      "http://www.jstor.org/stable/24778464 healy 0.6284681272024956\n",
      "http://www.jstor.org/stable/24778437 herder 0.7709310110682129\n",
      "http://www.jstor.org/stable/2868734 serious 0.24315602801645053\n",
      "http://www.jstor.org/stable/2868743 catch 0.7061338488830697\n",
      "http://www.jstor.org/stable/2868754 department 0.45465610566838716\n",
      "http://www.jstor.org/stable/2868739 dream 0.4643518249264839\n",
      "http://www.jstor.org/stable/2868691 theses 0.561482625560402\n",
      "http://www.jstor.org/stable/2868680 baer 0.22857899297593898\n",
      "http://www.jstor.org/stable/2868686 meanings 0.14728482741023055\n",
      "http://www.jstor.org/stable/2867144 birdsey 0.35206281103557646\n",
      "http://www.jstor.org/stable/2867168 blackstone 0.39927547908359606\n",
      "http://www.jstor.org/stable/2867146 egae 0.5773502691896257\n",
      "http://www.jstor.org/stable/2867155 musgrove 0.5042749587129829\n",
      "http://www.jstor.org/stable/2867164 tune 0.24959912661805977\n",
      "http://www.jstor.org/stable/2867158 tourneur 0.3775340244411614\n",
      "http://www.jstor.org/stable/2867371 johnson 0.3442765779718542\n",
      "http://www.jstor.org/stable/2867346 gregory 0.27047154405059776\n",
      "http://www.jstor.org/stable/2867369 hall 0.37551754676948523\n",
      "http://www.jstor.org/stable/2867357 fordian 0.43222579524062565\n",
      "http://www.jstor.org/stable/2867345 brutus 0.3086712930655622\n",
      "http://www.jstor.org/stable/2867356 neilson 0.2246510044152212\n",
      "http://www.jstor.org/stable/2867359 chorus 0.2919335160024982\n",
      "http://www.jstor.org/stable/2867327 story 0.152578446723317\n",
      "http://www.jstor.org/stable/2869334 haugk 0.3336396104738204\n",
      "http://www.jstor.org/stable/2869299 saratoga 0.2929989962896435\n"
     ]
    }
   ],
   "source": [
    "for n, doc in enumerate(corpus_tfidf):\n",
    "    if len(doc) < 1:\n",
    "        continue\n",
    "    word_id, score = max(doc, key=lambda x: x[1])\n",
    "    print(dset.items[n], dictionary.get(word_id), score)\n",
    "    if n >= 50:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
