{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding significant words within a curated dataset\n",
    "\n",
    "This notebook demonstrates how to find the significant words in your dataset using [tf-idf](./key-terms.ipynb#tf-idf). The following processes are described:\n",
    "\n",
    "* Importing your [dataset](./key-terms.ipynb#dataset)\n",
    "* Finding your initial query within your [dataset's](./key-terms.ipynb#dataset) metadata\n",
    "* Writing a helper function to help clean up a single [token](./key-terms.ipynb#token)\n",
    "* Cleaning each document of your dataset, one [token](./key-terms.ipynb#token) at a time\n",
    "* Using a dictionary of English words to remove words with poor [OCR](./key-terms.ipynb#ocr)\n",
    "* Computing the most significant words in your [corpus](./key-terms.ipynb#corpus) using [TFIDF](./key-terms.ipynb#tf-idf) with the [gensim](./key-terms.ipynb#gensim) library\n",
    "\n",
    "A familiarity with gensim is helpful but not required.\n",
    "____\n",
    "We import the `Dataset` module from the `tdm_client` library. The tdm_client library contains functions for connecting to the JSTOR server containing our [corpus](./key-terms.ipynb#corpus) [dataset](./key-terms.ipynb#dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim\n",
    "\n",
    "from tdm_client import Dataset\n",
    "from tdm_client import htrc_corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze your dataset, use the [dataset ID](./key-terms.ipynb/#dataset-ID) provided when you created your [dataset](./key-terms.ipynb/#dataset). A copy of your [dataset ID](./key-terms.ipynb/#dataset-ID) was sent to your email when you created your [corpus](./key-terms.ipynb#corpus). It should look like a long series of characters surrounded by dashes.  \n",
    "\n",
    "We create a new variable **dset** and initialize its value using the **Dataset** function. A sample **dataset ID** of journals focused on Shakespeare is provided here ('a517ef1f-0794-48e4-bea1-ac4fb8b312b4'). Pasting your unique **dataset ID** here will import your dataset from the JSTOR server.\n",
    "\n",
    "**Note**: If you are curious what is in your dataset, there is a download link in the email you received. The format and content of the files is described in the notebook [Building a Dataset](./1-building-a-dataset.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset('59c090b6-3851-3c65-e016-9181833b4a2c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find total number of documents in the dataset using the `len()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6687"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if this is the correct dataset, we can look at the original query by using the query_text method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All documents from JSTOR published in Shakespeare Quarterly from 1700 - 2019'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.query_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've verified that we have the correct [corpus](./key-terms.ipynb#corpus), let's create a helper function that can standardize and [clean](./key-terms.ipynb#clean-data) up the [tokens](./key-terms.ipynb#token) in our [dataset](./key-terms.ipynb#dataset). The function will:\n",
    "* lower cases all [tokens](./key-terms.ipynb#token)\n",
    "* use a dictionary from [The HathiTrust Research Center](./key-terms.ipynb#htrc) to correct common [Optical Character Recognition](./key-terms.ipynb#ocr) problems\n",
    "* discard [tokens](./key-terms.ipynb#token) less than 4 characters in length\n",
    "* discard [tokens](./key-terms.ipynb#token) with non-alphabetical characters\n",
    "* remove [stopwords](./key-terms.ipynb#stop-words) based on an [The HathiTrust Research Center](./key-terms.ipynb#htrc) [stopword](./key-terms.ipynb#stop-words) list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token(token): #define a function `process_token` that takes the argument `token`\n",
    "    token = token.lower() #set the string in token to a new string with all lowercase letters\n",
    "    corrected = htrc_corrections.get(token) #initialize a new variable `corrected` that runs token through the `htrc_corrections.get()` function to fix common OCR errors\n",
    "    if corrected is not None: #if corrected has a value, set the `token` variable to the same value as `corrected`\n",
    "        token = corrected\n",
    "    if len(token) < 4: #if token is less than four characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    if not(token.isalpha()): #if token contains non-alphabetic characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    return token #return the `token` variable which has been set equal to the `corrected` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cycle through each document in the [corpus](./key-terms.ipynb#corpus) with our helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "875"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [] #Create a new variable `documents` that is a list that will contain all of our documents.\n",
    "\n",
    "for n, unigram_count in enumerate(dset.get_features()):\n",
    "    this_doc = []\n",
    "    for token, count in unigram_count.items():\n",
    "        clean_token = process_token(token)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    documents.append(this_doc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most significant terms, by TFIDF, in the curated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = {\n",
    "        dictionary.get(_id): value for doc in corpus_tfidf\n",
    "        for _id, value in doc\n",
    "    }\n",
    "sorted_td = sorted(td.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ofamiem 1.0\n",
      "cwtrnca 0.9863483873087012\n",
      "chartres 0.9273378589673646\n",
      "worken 0.9207300498619446\n",
      "sobran 0.9075508762980733\n",
      "nuimber 0.8775001624677011\n",
      "weingust 0.8755826466137229\n",
      "rudanko 0.86000135238765\n",
      "enbiemata 0.8563716462679598\n",
      "weils 0.8472587947507879\n",
      "sliv 0.8394611060327918\n",
      "snuggs 0.8381178515481901\n",
      "ouderdom 0.8316113874601273\n",
      "habib 0.8308118578625007\n",
      "buzacott 0.8303061621922353\n",
      "gaiicanus 0.8294403338236659\n",
      "holmer 0.8201937618215803\n",
      "spectogram 0.817057687336797\n",
      "reproducedfrmtefgr 0.8139670501922074\n",
      "womersley 0.8080655409128259\n",
      "dulcitius 0.8048781892048817\n",
      "margolies 0.7961508372216022\n",
      "dugas 0.7868223742710501\n",
      "willeford 0.7859219459401604\n",
      "tcad 0.7819493652557465\n"
     ]
    }
   ],
   "source": [
    "for term, weight in sorted_td[:25]:\n",
    "    print(term, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the most significant word, by TFIDF, for the first 50 documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.jstor.org/stable/2869980 henslowe 0.32498040467805084\n",
      "http://www.jstor.org/stable/2870198 nimrod 0.1616432176708553\n",
      "http://www.jstor.org/stable/2870199 beatrice 0.22776869095345914\n",
      "http://www.jstor.org/stable/2870209 donaldson 0.47661799586847836\n",
      "http://www.jstor.org/stable/2870208 cheng 0.5913759877754204\n",
      "http://www.jstor.org/stable/2870189 antonio 0.44365366900689435\n",
      "http://www.jstor.org/stable/2870193 painting 0.4091485612863863\n",
      "http://www.jstor.org/stable/2870188 edgar 0.3258325027350309\n",
      "http://www.jstor.org/stable/2870203 hartwig 0.6147359515246972\n",
      "http://www.jstor.org/stable/2870194 hall 0.5121228500237642\n",
      "http://www.jstor.org/stable/2870206 novy 0.5473522309642914\n",
      "http://www.jstor.org/stable/2870202 booth 0.3863680373528867\n",
      "http://www.jstor.org/stable/2870313 vizcaya 0.601154198547381\n",
      "http://www.jstor.org/stable/2870307 dennis 0.3266904513150024\n",
      "http://www.jstor.org/stable/2870327 hollar 0.6487395808866938\n",
      "http://www.jstor.org/stable/2870308 longleat 0.5023342170474715\n",
      "http://www.jstor.org/stable/2870304 welles 0.6010247674772539\n",
      "http://www.jstor.org/stable/2869730 andidentifies 0.17159914957892092\n",
      "http://www.jstor.org/stable/2869726 rubinstein 0.5362488350329068\n",
      "http://www.jstor.org/stable/2871208 goals 0.23001427827326723\n",
      "http://www.jstor.org/stable/2871196 carded 0.29724672445968886\n",
      "http://www.jstor.org/stable/2871206 jaggard 0.27305742735731064\n",
      "http://www.jstor.org/stable/2870092 kentucky 0.23729556330149262\n",
      "http://www.jstor.org/stable/2870086 roger 0.22204664860545675\n",
      "http://www.jstor.org/stable/2870090 arena 0.18335304148692239\n",
      "http://www.jstor.org/stable/2870080 foster 0.3553404112536266\n",
      "http://www.jstor.org/stable/2870094 taylor 0.33191619292482416\n",
      "http://www.jstor.org/stable/2869774 reviewer 0.5117030660367732\n",
      "http://www.jstor.org/stable/2869777 production 0.20845224766777756\n",
      "http://www.jstor.org/stable/2869765 reviewing 0.19192121094797993\n",
      "http://www.jstor.org/stable/2869775 theatre 0.28943418086170686\n",
      "http://www.jstor.org/stable/2870172 wedor 0.33478083317514656\n",
      "http://www.jstor.org/stable/2870161 lear 0.26236016042521487\n",
      "http://www.jstor.org/stable/2870157 italian 0.21204005550294144\n",
      "http://www.jstor.org/stable/2870177 jardine 0.5479900617282173\n",
      "http://www.jstor.org/stable/2870185 mailing 0.19113866281258757\n",
      "http://www.jstor.org/stable/2870383 laan 0.7010845484536087\n",
      "http://www.jstor.org/stable/2870386 brown 0.24750846521211095\n",
      "http://www.jstor.org/stable/2870376 television 0.2401880358550755\n",
      "http://www.jstor.org/stable/2870394 simonds 0.25567208995060603\n",
      "http://www.jstor.org/stable/2870395 cloth 0.268351744105865\n",
      "http://www.jstor.org/stable/2870384 vice 0.16223647555933257\n",
      "http://www.jstor.org/stable/2870379 utah 0.2908187767136855\n",
      "http://www.jstor.org/stable/2869930 suetonius 0.22402970366558822\n",
      "http://www.jstor.org/stable/2869935 boston 0.2900993545994711\n",
      "http://www.jstor.org/stable/2869949 dissertation 0.2929867243070928\n",
      "http://www.jstor.org/stable/2869929 tail 0.5079683689976742\n",
      "http://www.jstor.org/stable/2869932 prague 0.24510894149698817\n",
      "http://www.jstor.org/stable/2869947 bliss 0.32079684342023224\n",
      "http://www.jstor.org/stable/2869942 driscoll 0.2859064137696673\n",
      "http://www.jstor.org/stable/2869920 universitn 0.2748416895015885\n"
     ]
    }
   ],
   "source": [
    "for n, doc in enumerate(corpus_tfidf):\n",
    "    if len(doc) < 1:\n",
    "        continue\n",
    "    word_id, score = max(doc, key=lambda x: x[1])\n",
    "    print(dset.items[n], dictionary.get(word_id), score)\n",
    "    if n >= 50:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
