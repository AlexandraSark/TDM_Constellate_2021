{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By <a href=\"https://nkelber.com\">Nathan Kelber</a> and Ted Lawless <br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.\n",
    "____\n",
    "\n",
    "## Finding Significant Words within a Dataset Using TF/IDF\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Programming Knowledge Required:** \n",
    "This notebook can be run on a JSTOR/Portico [non-consumptive](./key-terms.ipynb#non-consumptive) [JSON Lines (.jsonl)](./key-terms.ipynb#jsonl) [dataset](./key-terms.ipynb#dataset) with little to no knowledge of [Python](./key-terms.ipynb#python). To have a full understanding of the code used in this [notebook](./key-terms.ipynb#jupyter-notebook), we recommend learning:\n",
    "* [Python Basics](https://automatetheboringstuff.com/2e/chapter1/)\n",
    "* [Flow Control](https://automatetheboringstuff.com/2e/chapter2/)\n",
    "* [Functions](https://automatetheboringstuff.com/2e/chapter3/)\n",
    "* [Lists](https://automatetheboringstuff.com/2e/chapter4/)\n",
    "* [Dictionaries](https://automatetheboringstuff.com/2e/chapter5/)\n",
    "\n",
    "**Completion time:** 35 minutes\n",
    "\n",
    "**Data Format:** [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico) [non-consumptive](./key-terms.ipynb#non-consumptive) [JSON Lines (.jsonl)](./key-terms.ipynb#jsonl)\n",
    "\n",
    "**Libraries Used:**\n",
    "* **[json](./key-terms.ipynb#json-python-library)** to convert our dataset from json lines format to a Python list\n",
    "* **[gensim](./key-terms.ipynb#gensim)** to help compute the [tf-idf](./key-terms.ipynb#tf-idf) calculation\n",
    "\n",
    "**Description of methods in this notebook:**\n",
    "This [notebook](./key-terms.ipynb#jupyter-notebook) shows how to discover significant words in your [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico) [dataset](./key-terms.ipynb#dataset) using [Python](./key-terms.ipynb#python). The method for finding significant terms is [tf-idf](./key-terms.ipynb#tf-idf).  The following processes are described:\n",
    "\n",
    "* Converting your [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico)[dataset](./key-terms.ipynb#dataset) into a Python list\n",
    "* Writing a helper function to help clean up a single [token](./key-terms.ipynb#token)\n",
    "* Cleaning each document of your dataset, one [token](./key-terms.ipynb#token) at a time\n",
    "* Using a dictionary of English words to remove words with poor [OCR](./key-terms.ipynb#ocr)\n",
    "* Computing the most significant words in your [corpus](./key-terms.ipynb#corpus) using [TFIDF](./key-terms.ipynb#tf-idf) with the [gensim](./key-terms.ipynb#gensim) library\n",
    "\n",
    "A familiarity with [gensim](./key-terms.ipynb#gensim) is helpful but not required.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing your dataset\n",
    "\n",
    "You have two options for bringing your dataset into the local environment:\n",
    "\n",
    "1. Manually download and upload your dataset\n",
    "2. Use a dataset id to automatically upload a dataset\n",
    "\n",
    "### Option one: Manually download and upload your dataset\n",
    "\n",
    "You can download your dataset from the corpus builder in the link shown below. (You may also have a link to your dataset in your email.) If you wish, you can modify your dataset on your local machine before the next upload phase. This gives you some more flexibility than automatically pulling in your dataset using a dataset ID using option 2 below.\n",
    "\n",
    "![The link for downloading your dataset](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/downloadDataset.png)\n",
    "\n",
    "Once you have your dataset ready on your local machine, you can then upload your dataset into JupyterLab by clicking the upload button in the file pane on the left.\n",
    "\n",
    "![The upload button in the file pane](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/uploadDataset.png)\n",
    "\n",
    "Make sure to upload your dataset to the \"datasets\" folder. \n",
    "\n",
    "### Option Two: Use a Dataset ID to automatically upload a dataset\n",
    "\n",
    "You'll use the tdm_client library to automatically upload your dataset. We import the `Dataset` module from the `tdm_client` library. The tdm_client library contains functions for connecting to the JSTOR server containing our [corpus](./key-terms.ipynb#corpus) [dataset](./key-terms.ipynb#dataset). To analyze your dataset, use the [dataset ID](./key-terms.ipynb/#dataset-ID) provided when you created your [dataset](./key-terms.ipynb/#dataset). A copy of your [dataset ID](./key-terms.ipynb/#dataset-ID) was sent to your email when you created your [corpus](./key-terms.ipynb#corpus). It should look like a long series of characters surrounded by dashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/sampleJournalAnalysis.jsonl'"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing your dataset with a dataset ID\n",
    "import tdm_client\n",
    "tdm_client.get_dataset(\"f6ae29d4-3a70-36ee-d601-20a8c0311273\", \"sampleJournalAnalysis\") #Load the sample dataset, the full run of Shakespeare Quarterly from 1950-2013.\n",
    "\n",
    "# Other humanities datasets:\n",
    "\n",
    "#English\n",
    "# Negro American Literature Forum (1967-1976) + Black American Literature Forum (1976-1991) + African American Review (1992-2016) (b4668c50-a970-c4d7-eb2c-bb6d04313542)\n",
    "# Shakespeare Quarterly (1950-2013) (f6ae29d4-3a70-36ee-d601-20a8c0311273)\n",
    "# ELH (1934-2014) (4999901a-fa17-31da-cfe5-2abf3a429df7)\n",
    "# College English (1939-2016) (a161f384-720b-b6bf-a0cc-4d7d3b857e1c)\n",
    "# PMLA (1889-2014) (1aea53b9-26d5-fe54-e35c-8259156ce6cd)\n",
    "\n",
    "#History\n",
    "\n",
    "#Philosophy\n",
    "\n",
    "#Anthropology\n",
    "\n",
    "#Law\n",
    "\n",
    "#Art\n",
    "\n",
    "#Classics\n",
    "#Classical Quarterly (1907-2014) (82014740-8ed9-3c34-5716-d0879b8317f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can begin working with our [dataset](./key-terms.ipynb#dataset), we need to convert the [JSON lines](./key-terms.ipynb#jsonl) file written in [JavaScript](./key-terms.ipynb#javascript) into [Python](./key-terms.ipynb#python) so we can work with it. Remember that each line of our [JSON lines](./key-terms.ipynb#jsonl) file represents a single text, whether that is a journal article, book, or something else. We will create a [Python](./key-terms.ipynb#python) list that contains every document. Within each list item for each document, we will use a [Python dictionary](./key-terms.ipynb#python-dictionary) of [key/value pairs](./key-terms.ipynb#key-value-pair) to store information related to that document. \n",
    "\n",
    "Essentially we will have a [list](./key-terms.ipynb#python-list) of documents numbered, from zero to the last document. Each [list](./key-terms.ipynb#python-list) item then will be composed of a [dictionary](./key-terms.ipynb#python-dictionary) of [key/value pairs](./key-terms.ipynb#key-value-pair) that allows us to retrieve information from that particular document by number. The structure will look something like this:\n",
    "\n",
    "![Structure of the corpus, a list of dictionaries](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CorpusView.png)\n",
    "\n",
    "For each item in our list we will be able to use [key/value pairs](./key-terms.ipynb#key-value-pair) to get a **value** if we supply a **key**. We will call our [Python list](./key-terms.ipynb#python-list) variable `all_documents` since it will contain all of the documents in our [corpus](./key-terms.ipynb#corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your filename and be sure your file is in your datasets folder\n",
    "file_name = 'sampleJournalAnalysis.jsonl' \n",
    "\n",
    "# Import the json module\n",
    "import json\n",
    "# Create an empty new list variable named `all_documents`\n",
    "all_documents = [] \n",
    "# Temporarily open the file `filename` in the datasets/ folder\n",
    "with open('./datasets/' + file_name) as dataset_file: \n",
    "    #for each line in the dataset file\n",
    "    for line in dataset_file: \n",
    "        # Read each line into a Python dictionary.\n",
    "        # Create a variable document that contains the line using json.loads to convert the json key/value pairs to a python dictionary\n",
    "        document = json.loads(line) \n",
    "        # Append a new list item to `all_documents` containing the dictionary we created.\n",
    "        all_documents.append(document) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of our documents have been converted from our original [JSON lines](./key-terms.ipynb#jsonl) file format (.jsonl) into a [python List](./key-terms.ipynb#python-list) variable named `all_documents`. Let's see what we can discover about our [corpus](./key-terms.ipynb#corpus) with a few simple methods.\n",
    "\n",
    "First, we can determine how many texts are in our [dataset](./key-terms.ipynb#dataset) by using the `len()` function to get the size of `all_documents`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6687"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of documents: 6687\n",
      "After removing \"Review Articles\": 4573\n",
      "After removing articles labeled \"Front Matter\": 4324\n",
      "After removing articles labeled \"Back Matter\": 4152\n",
      "After removing short articles: 3008\n"
     ]
    }
   ],
   "source": [
    "print('Original number of documents: ' + str(len(all_documents)))\n",
    "reduced_list = [all_documents[x] for x in range(len(all_documents)) if all_documents[x].get('title') != 'Review Article']\n",
    "print('After removing \"Review Articles\": ' + str(len(reduced_list)))\n",
    "reduced_list = [all_documents[x] for x in range(len(reduced_list)) if reduced_list[x].get('title') != 'Front Matter']\n",
    "print('After removing articles labeled \"Front Matter\": ' + str(len(reduced_list)))\n",
    "reduced_list = [all_documents[x] for x in range(len(reduced_list)) if reduced_list[x].get('title') != 'Back Matter']\n",
    "print('After removing articles labeled \"Back Matter\": ' + str(len(reduced_list)))\n",
    "reduced_list = [all_documents[x] for x in range(len(reduced_list)) if reduced_list[x].get('wordCount') < 3000]\n",
    "print('After removing short articles: ' + str(len(reduced_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\n",
      "Title: Review Article\n",
      "URL: http://www.jstor.org/stable/2869980\n",
      "Status: Removed--Review Article\n",
      "Article 1:\n",
      "Title: Shakespeare in Sydney\n",
      "URL: http://www.jstor.org/stable/2870198\n",
      "Status: Removed--Too short at 2032 words\n",
      "Article 2:\n",
      "Title: Shakespeare in the Berkshires, 1985\n",
      "URL: http://www.jstor.org/stable/2870199\n",
      "Status: Removed--Too short at 1805 words\n",
      "Article 3:\n",
      "Title: Review Article\n",
      "URL: http://www.jstor.org/stable/2870209\n",
      "Status: Removed--Review Article\n",
      "Article 4:\n",
      "Title: Review Article\n",
      "URL: http://www.jstor.org/stable/2870208\n",
      "Status: Removed--Review Article\n"
     ]
    }
   ],
   "source": [
    "def remove_non_articles(test_doc):\n",
    "    print('Article ' + str(i) + ':')\n",
    "    print('Title: ' + test_doc.get('title'))\n",
    "    print('URL: ' + test_doc.get('id'))\n",
    "    print('Status: ', end='')\n",
    "    if test_doc.get('creators') == None:\n",
    "        print('Removed--No author')\n",
    "    elif test_doc.get('title') == 'Review Article':\n",
    "        print('Removed--Review Article')\n",
    "    elif test_doc.get('title') == 'Front Matter':\n",
    "        print('Removed--Front Matter')\n",
    "    elif test_doc.get('title') == 'Back Matter':\n",
    "        print('Removed--Back Matter')  \n",
    "    elif test_doc.get('wordCount') < 3000:\n",
    "        print('Removed--Too short at ' + str(test_doc.get('wordCount')) + ' words')\n",
    "    else:\n",
    "        print('GOOD ARTICLE')      \n",
    "\n",
    "articles_to_show = 5\n",
    "#articles_to_show = len(all_documents)\n",
    "for i in range(articles_to_show):\n",
    "    remove_non_articles(all_documents[i])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function that can standardize and [clean](./key-terms.ipynb#clean-data) up the [tokens](./key-terms.ipynb#token) in our [dataset](./key-terms.ipynb#dataset). The function will:\n",
    "* lower cases all [tokens](./key-terms.ipynb#token)\n",
    "* use a dictionary from [The HathiTrust Research Center](./key-terms.ipynb#htrc) to correct common [Optical Character Recognition](./key-terms.ipynb#ocr) problems\n",
    "* discard [tokens](./key-terms.ipynb#token) less than 4 characters in length\n",
    "* discard [tokens](./key-terms.ipynb#token) with non-alphabetical characters\n",
    "* remove [stopwords](./key-terms.ipynb#stop-words) based on [The HathiTrust Research Center](./key-terms.ipynb#htrc) [stopword](./key-terms.ipynb#stop-words) list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm_client import htrc_corrections\n",
    "\n",
    "def process_token(token): #define a function `process_token` that takes the argument `token`\n",
    "    token = token.lower() #set the string in token to a new string with all lowercase letters\n",
    "    corrected = htrc_corrections.get(token) #initialize a new variable `corrected` that runs token through the `htrc_corrections.get()` function to fix common OCR errors\n",
    "    if corrected is not None: #if corrected has a value, set the `token` variable to the same value as `corrected`\n",
    "        token = corrected\n",
    "    if len(token) < 4: #if token is less than four characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    if not(token.isalpha()): #if token contains non-alphabetic characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    return token #return the `token` variable which has been set equal to the `corrected` variable\n",
    "\n",
    "def process_document(chosen_document):\n",
    "    this_doc = []\n",
    "    singleDoc = chosen_document.get('unigramCount')\n",
    "    for token, count in singleDoc.items():\n",
    "        clean_token = process_token(token)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    documents.append(this_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(len(reduced_list)):\n",
    "    process_document(reduced_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cycle through each document in the [corpus](./key-terms.ipynb#corpus) with our helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most significant terms, by TFIDF, in the curated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = {\n",
    "        dictionary.get(_id): value for doc in corpus_tfidf\n",
    "        for _id, value in doc\n",
    "    }\n",
    "sorted_td = sorted(td.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ofamiem 1.0\n",
      "ouderdom 0.9127008148928485\n",
      "sturgess 0.9086024303923905\n",
      "zamir 0.8776651519018113\n",
      "santayana 0.8665736199609847\n",
      "falocco 0.8562928124271615\n",
      "weingust 0.8547899776401692\n",
      "chinese 0.8462001652815331\n",
      "weils 0.8445705864452131\n",
      "rudanko 0.8390830868389877\n",
      "enbiemata 0.8280102498481464\n",
      "daileader 0.8171301955562135\n",
      "nodier 0.8168018882816901\n",
      "usury 0.8005782510346803\n",
      "menas 0.7909230276348473\n",
      "beaurline 0.7905965479055058\n",
      "spectogram 0.7879121817261375\n",
      "franciscus 0.7771865284620213\n",
      "soellner 0.77327831058108\n",
      "bastarde 0.7712490973605648\n",
      "unton 0.7682807524450844\n",
      "cohens 0.7677867862204198\n",
      "falco 0.7635813973078707\n",
      "callimachus 0.7604638066069844\n",
      "wynkyn 0.7583409894849417\n"
     ]
    }
   ],
   "source": [
    "for term, weight in sorted_td[:25]:\n",
    "    print(term, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the most significant word, by TFIDF, for the first 50 documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.jstor.org/stable/2869980 henslowe 0.33564303350040253\n",
      "http://www.jstor.org/stable/2870198 stairs 0.15472663369841613\n",
      "http://www.jstor.org/stable/2870199 beatrice 0.23846910220032727\n",
      "http://www.jstor.org/stable/2870209 donaldson 0.4977445938192701\n",
      "http://www.jstor.org/stable/2870208 cheng 0.6067690431054176\n",
      "http://www.jstor.org/stable/2870203 hartwig 0.6244428220115\n",
      "http://www.jstor.org/stable/2870194 hall 0.5136214288878345\n",
      "http://www.jstor.org/stable/2870206 novy 0.5645876593477805\n",
      "http://www.jstor.org/stable/2870202 booth 0.3864621355873546\n",
      "http://www.jstor.org/stable/2870313 vizcaya 0.6046299393365276\n",
      "http://www.jstor.org/stable/2870327 hollar 0.639842687006522\n",
      "http://www.jstor.org/stable/2870308 longleat 0.5039894018479222\n",
      "http://www.jstor.org/stable/2869730 andidentifies 0.1635030038209591\n",
      "http://www.jstor.org/stable/2869726 rubinstein 0.5497022223906934\n",
      "http://www.jstor.org/stable/2871196 carded 0.2820633806904763\n",
      "http://www.jstor.org/stable/2871206 jaggard 0.2704892925363586\n",
      "http://www.jstor.org/stable/2870086 roger 0.22407927452908946\n",
      "http://www.jstor.org/stable/2870094 taylor 0.3310153818503154\n",
      "http://www.jstor.org/stable/2869777 production 0.20771778787771467\n",
      "http://www.jstor.org/stable/2869765 reviewing 0.19442107347889528\n",
      "http://www.jstor.org/stable/2869775 theatre 0.29861889467571856\n",
      "http://www.jstor.org/stable/2870161 lear 0.2660105431143734\n",
      "http://www.jstor.org/stable/2870157 italian 0.21087240690666048\n",
      "http://www.jstor.org/stable/2870177 jardine 0.5611803888072969\n",
      "http://www.jstor.org/stable/2870185 mailing 0.1992704223595866\n",
      "http://www.jstor.org/stable/2870383 laan 0.6921640509219383\n",
      "http://www.jstor.org/stable/2870386 brown 0.25477440033888865\n",
      "http://www.jstor.org/stable/2870376 television 0.24732995544933492\n",
      "http://www.jstor.org/stable/2870394 simonds 0.2739133793708261\n",
      "http://www.jstor.org/stable/2870395 cloth 0.27133610060734786\n",
      "http://www.jstor.org/stable/2870384 vice 0.1615457820296335\n",
      "http://www.jstor.org/stable/2870379 utah 0.30647004664738303\n",
      "http://www.jstor.org/stable/2869930 suetonius 0.2277299687699213\n",
      "http://www.jstor.org/stable/2869949 dissertation 0.3083116834796744\n",
      "http://www.jstor.org/stable/2869929 tail 0.5099418545362527\n",
      "http://www.jstor.org/stable/2869947 bliss 0.3311906461731288\n",
      "http://www.jstor.org/stable/2869942 driscoll 0.2718201396975177\n",
      "http://www.jstor.org/stable/2869920 universitn 0.26078064915896176\n",
      "http://www.jstor.org/stable/2869938 stair 0.25089921713600366\n",
      "http://www.jstor.org/stable/2869704 capitol 0.5955814030640157\n",
      "http://www.jstor.org/stable/2869682 bullingbrook 0.20883169101665788\n",
      "http://www.jstor.org/stable/2869695 mcanuff 0.2313300162117224\n",
      "http://www.jstor.org/stable/2869703 dash 0.5728806682575506\n",
      "http://www.jstor.org/stable/2870062 miller 0.4990204703075245\n",
      "http://www.jstor.org/stable/2870055 teaching 0.26188675285977076\n",
      "http://www.jstor.org/stable/2870068 students 0.22910712319925738\n",
      "http://www.jstor.org/stable/2869860 judah 0.5459643576202144\n",
      "http://www.jstor.org/stable/2869859 magician 0.5056121470707868\n",
      "http://www.jstor.org/stable/2869911 facsimile 0.25122626649949076\n",
      "http://www.jstor.org/stable/2869915 gyman 0.1411382206138676\n",
      "http://www.jstor.org/stable/2869912 distribution 0.3138351376547982\n"
     ]
    }
   ],
   "source": [
    "for n, doc in enumerate(corpus_tfidf):\n",
    "    if len(doc) < 1:\n",
    "        continue\n",
    "    word_id, score = max(doc, key=lambda x: x[1])\n",
    "    print(reduced_list[n].get('id'), dictionary.get(word_id), score)\n",
    "    if n >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
