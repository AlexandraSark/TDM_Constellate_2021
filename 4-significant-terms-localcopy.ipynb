{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By <a href=\"https://nkelber.com\">Nathan Kelber</a> and Ted Lawless <br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.\n",
    "____\n",
    "\n",
    "# Finding Significant Words within a Dataset Using TF/IDF\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Programming Knowledge Required:** \n",
    "This notebook can be run on a JSTOR/Portico [non-consumptive](./key-terms.ipynb#non-consumptive) [JSON Lines (.jsonl)](./key-terms.ipynb#jsonl) [dataset](./key-terms.ipynb#dataset) with little to no knowledge of [Python](./key-terms.ipynb#python). To have a full understanding of the code used in this [notebook](./key-terms.ipynb#jupyter-notebook), we recommend learning:\n",
    "* [Python Basics](https://automatetheboringstuff.com/2e/chapter1/)\n",
    "* [Flow Control](https://automatetheboringstuff.com/2e/chapter2/)\n",
    "* [Functions](https://automatetheboringstuff.com/2e/chapter3/)\n",
    "* [Lists](https://automatetheboringstuff.com/2e/chapter4/)\n",
    "* [Dictionaries](https://automatetheboringstuff.com/2e/chapter5/)\n",
    "\n",
    "**Completion time:** 75 minutes\n",
    "\n",
    "**Data Format:** [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico) [non-consumptive](./key-terms.ipynb#non-consumptive) [JSON Lines (.jsonl)](./key-terms.ipynb#jsonl)\n",
    "\n",
    "**Libraries Used:**\n",
    "* **[json](./key-terms.ipynb#json-python-library)** to convert our dataset from json lines format to a Python list\n",
    "* **[gensim](./key-terms.ipynb#gensim)** to help compute the [tf-idf](./key-terms.ipynb#tf-idf) calculation\n",
    "\n",
    "**Description of methods in this notebook:**\n",
    "This [notebook](./key-terms.ipynb#jupyter-notebook) shows how to discover significant words in your [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico) [dataset](./key-terms.ipynb#dataset) using [Python](./key-terms.ipynb#python). The method for finding significant terms is [tf-idf](./key-terms.ipynb#tf-idf).  The following processes are described:\n",
    "\n",
    "* Converting your [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico)[dataset](./key-terms.ipynb#dataset) into a Python list\n",
    "* Writing a helper function to help clean up a single [token](./key-terms.ipynb#token)\n",
    "* Cleaning each document of your dataset, one [token](./key-terms.ipynb#token) at a time\n",
    "* Using a dictionary of English words to remove words with poor [OCR](./key-terms.ipynb#ocr)\n",
    "* Computing the most significant words in your [corpus](./key-terms.ipynb#corpus) using [TFIDF](./key-terms.ipynb#tf-idf) with the [gensim](./key-terms.ipynb#gensim) library\n",
    "\n",
    "A familiarity with [gensim](./key-terms.ipynb#gensim) is helpful but not required.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding \"Term Frequency- Inverse Document Frequency\" (TF-IDF)\n",
    "\n",
    "TF-IDF is used in machine learning and text mining for measuring the significance of particular terms for a given document. It consists of two parts that are multiplied together:\n",
    "\n",
    "1. Term Frequency- A measure of how many times a given word appears in a document\n",
    "2. Inverse Document Frequency- A measure of how many times the same word occurs in other documents within the corpus\n",
    "\n",
    "If we were to merely consider word frequency, the most frequent words would be common function words like: \"the\", \"and\", \"of\". We could use a stopwords list to remove the common function words, but that still may not give us results that describe the unique terms in the document since the uniqueness of terms depends on the context of a larger body of documents. In other words, the same term could be significant or insignificant depending on the context. Consider these examples:\n",
    "\n",
    "* Given a set of scientific journal articles in biology, the term \"lab\" may not be significant since biologists often rely on and mention labs in their research. However, if the term \"lab\" were to occur frequently in a history or English article, then it is likely to be significant since humanities articles rarely discuss labs. \n",
    "* If we were to look at thousands of articles in literary studies, then the term \"postcolonial\" may be significant for any given article. However, if were to look at a few hundred articles on the topic of \"the global south,\" then the term \"postcolonial\" may occur so frequently that it is not a significant way to differentiate between the articles.\n",
    "\n",
    "The TF-IDF calculation reveals the words that are frequent in this document **yet rare in other documents**. The goal is to find out what is unique or remarkable about a document given the context and that context can change the results of the analysis. \n",
    "\n",
    "Here is how the calculation is mathematically written:\n",
    "\n",
    "$$tfidf_{t,d} = tf_{t,d} \\cdot idf_{t,D}$$\n",
    "\n",
    "In plain English, this means: **The value of TF-IDF is the product (or multiplication) of a given term's frequency multiplied by its inverse document frequency.** Let's unpack these terms one at a time.\n",
    "\n",
    "### Term Frequency Function\n",
    "\n",
    "$$tf_{t,d}$$\n",
    "The number of times (t) a term occurs in a given document (d)\n",
    "\n",
    "### Inverse Document Frequency Function\n",
    "\n",
    "$$idf_i = \\mbox{log} \\frac{N}{|{d : t_i \\in d}|}$$\n",
    "The inverse document frequency can be expanded to the calculation on the right. In plain English, this means: **The log of the total number of documents (N) divided by the number of documents that contain the term**\n",
    "\n",
    "### TF-IDF Calculation in Plain English\n",
    "\n",
    "$$ tf-idf = (Number-of-times-the-word-occurs-in-given-document) \\cdot \\mbox{log} \\frac{(Total-number-of-documents)}{(Total-number-of-documents-containing-the-word)}$$\n",
    "\n",
    "There are variations on the TF-IDF formula, but this is the most widely-used version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example Calculation of TF-IDF\n",
    "\n",
    "Let's take a look at an example to illustrate the fundamentals of TF-IDF. First, we need several texts to compare. Our texts will be very simple.\n",
    "\n",
    "* text1 = 'The grass was green and spread out the distance like the sea.'\n",
    "* text2 = 'Green eggs and ham were spread out like the book.'\n",
    "* text3 = 'Green sailors were met like the sea met troubles.'\n",
    "* text4 = 'The grass was green.'\n",
    "\n",
    "The first step is we need to discover how many unique words are in each text. \n",
    "\n",
    "|text1|text2|text3|text4|\n",
    "|    ---    | ---| --- | --- |\n",
    "|the|green|green|the|\n",
    "|grass|eggs|sailors|grass|\n",
    "|was|and|were|was|\n",
    "|green|ham|met|green|\n",
    "|and|were|like| |\n",
    "|spread|spread|the| |\n",
    "|out|out|sea| |\n",
    "|into|like|met| |\n",
    "|distance|the|troubles| |\n",
    "|like|book| | |\n",
    "|sea| | | |\n",
    "\n",
    "\n",
    "Our four texts share some similar words. Next, we create a single list of unique words that occur across all three texts.\n",
    "\n",
    "|Unique Words|\n",
    "| --- |\n",
    "|and|\n",
    "|book|\n",
    "|distance|\n",
    "|eggs|\n",
    "|grass|\n",
    "|green|\n",
    "|ham|\n",
    "|like|\n",
    "|met|\n",
    "|out|\n",
    "|sailors|\n",
    "|sea|\n",
    "|spread|\n",
    "|the|\n",
    "|troubles|\n",
    "|was|\n",
    "|were|\n",
    "\n",
    "Now let's count the occurences of each unique word in each sentence\n",
    "\n",
    "|word|text1|text2|text3|text4|\n",
    "|---|---|---|---|---|\n",
    "|and|1|1|0|0|\n",
    "|book|0|1|0|0|\n",
    "|distance|1|0|0|0|\n",
    "|eggs|0|1|0|0|\n",
    "|grass|1|0|0|1|\n",
    "|green|1|1|1|1|\n",
    "|ham|0|1|0|0|\n",
    "|like|1|1|1|0|\n",
    "|met|0|0|2|0|\n",
    "|out|1|1|0|0|\n",
    "|sailors|0|0|1|0|\n",
    "|sea|1|0|1|0|\n",
    "|spread|1|1|0|0|\n",
    "|the|3|1|1|1|\n",
    "|troubles|0|0|1|0|\n",
    "|was|1|0|0|1|\n",
    "|were|0|1|1|0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing TF-IDF (Example 1)\n",
    "\n",
    "We have enough information now to compute TF-IDF for every word in our corpus. Recall the plain English formula.\n",
    "\n",
    "$$ tf-idf = (Number-of-times-the-word-occurs-in-given-document) \\cdot \\mbox{log} \\frac{(Total-number-of-documents)}{(Total-number-of-documents-containing-the-word)}$$\n",
    "\n",
    "We can use the formula to compute TF-IDF for the most common word in our corpus: 'the'. We will compute TF-IDF four times, once for each of our texts. \n",
    "\n",
    "|word|text1|text2|text3|text4|\n",
    "|---|---|---|---|---|\n",
    "|the|3|1|1|1|\n",
    "\n",
    "text1: $$ tf-idf = 3 \\cdot \\mbox{log} \\frac{4}{(4)} = 3 \\cdot \\mbox{log} 1 = 3 \\cdot 0 = 0$$\n",
    "text2: $$ tf-idf = 1 \\cdot \\mbox{log} \\frac{4}{(4)} = 1 \\cdot \\mbox{log} 1 = 1 \\cdot 0 = 0$$\n",
    "text3: $$ tf-idf = 1 \\cdot \\mbox{log} \\frac{4}{(4)} = 1 \\cdot \\mbox{log} 1 = 1 \\cdot 0 = 0$$\n",
    "text4: $$ tf-idf = 1 \\cdot \\mbox{log} \\frac{4}{(4)} = 1 \\cdot \\mbox{log} 1 = 1 \\cdot 0 = 0$$\n",
    "\n",
    "The results of our analysis suggest 'the' has a weight of 0 in every document. The word 'the' exists in all of our documents, and therefore it is not a significant term to differentiate one document from another.\n",
    "\n",
    "Given that idf is\n",
    "\n",
    "$$\\mbox{log} \\frac{(Total-number-of-documents)}{(Total-number-of-documents-containing-the-word)}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\mbox{log} 1 = 0$$\n",
    "we can see that TF-IDF will be 0 for any word that occurs in every document. That is, if a word occurs in every document, then it is not a significant term for any individual document.\n",
    "\n",
    "\n",
    "\n",
    "### Computing TF-IDF (Example 2)\n",
    "\n",
    "Let's try a second example with the word 'out'. Recall the plain English formula.\n",
    "\n",
    "$$ tf-idf = (Number-of-times-the-word-occurs-in-given-document) \\cdot \\mbox{log} \\frac{(Total-number-of-documents)}{(Total-number-of-documents-containing-the-word)}$$\n",
    "\n",
    "We will compute TF-IDF four times, once for each of our texts. \n",
    "\n",
    "|word|text1|text2|text3|text4|\n",
    "|---|---|---|---|---|\n",
    "|out|1|1|0|0|\n",
    "\n",
    "text1: $$ tf-idf = 1 \\cdot \\mbox{log} \\frac{4}{(2)} = 1 \\cdot \\mbox{log} 2 = 1 \\cdot .3010 = .3010$$\n",
    "text2: $$ tf-idf = 1 \\cdot \\mbox{log} \\frac{4}{(2)} = 1 \\cdot \\mbox{log} 2 = 1 \\cdot .3010 = .3010$$\n",
    "text3: $$ tf-idf = 0 \\cdot \\mbox{log} \\frac{4}{(2)} = 0 \\cdot \\mbox{log} 2 = 0 \\cdot .3010 = 0$$\n",
    "text4: $$ tf-idf = 0 \\cdot \\mbox{log} \\frac{4}{(2)} = 0 \\cdot \\mbox{log} 2 = 0 \\cdot .3010 = 0$$\n",
    "\n",
    "The results of our analysis suggest 'out' has some significance in text1 and text2, but no significance for text3 and text4 where the word does not occur.\n",
    "\n",
    "### Computing TF-IDF (Example 3)\n",
    "\n",
    "Let's try one last example with the word 'met'. Here's the tf-idf formula again:\n",
    "\n",
    "$$ tf-idf = (Number-of-times-the-word-occurs-in-given-document) \\cdot \\mbox{log} \\frac{(Total-number-of-documents)}{(Total-number-of-documents-containing-the-word)}$$\n",
    "\n",
    "And here's how many times the word 'met' occurs in each text.\n",
    "\n",
    "|word|text1|text2|text3|text4|\n",
    "|---|---|---|---|---|\n",
    "|met|0|0|2|0|\n",
    "\n",
    "text1: $$ tf-idf = 0 \\cdot \\mbox{log} \\frac{4}{(1)} = 0 \\cdot \\mbox{log} 4 = 1 \\cdot .6021 = 0$$\n",
    "text2: $$ tf-idf = 0 \\cdot \\mbox{log} \\frac{4}{(1)} = 0 \\cdot \\mbox{log} 4 = 1 \\cdot .6021 = 0$$\n",
    "text3: $$ tf-idf = 2 \\cdot \\mbox{log} \\frac{4}{(1)} = 2 \\cdot \\mbox{log} 4 = 1 \\cdot .6021 = 1.2042$$\n",
    "text4: $$ tf-idf = 0 \\cdot \\mbox{log} \\frac{4}{(1)} = 0 \\cdot \\mbox{log} 4 = 1 \\cdot .6021 = 0$$\n",
    "\n",
    "As should be expected, we can see that the word 'met' is very significant in text3 but not significant in any other text since it does not occur in any other text. \n",
    "\n",
    "## The Full TF-IDF Example Table\n",
    "\n",
    "Here are the original sentences for each text:\n",
    "\n",
    "* text1 = 'The grass was green and spread out the distance like the sea.'\n",
    "* text2 = 'Green eggs and ham were spread out like the book.'\n",
    "* text3 = 'Green sailors were met like the sea met troubles.'\n",
    "* text4 = 'The grass was green.'\n",
    "\n",
    "And here's the corresponding TF-IDF scores for each word in each text:\n",
    "\n",
    "|word|text1|text2|text3|text4|\n",
    "|---|---|---|---|---|\n",
    "|and|.3010|.3010|0|0|\n",
    "|book|0|.6021|0|0|\n",
    "|distance|.6021|0|0|0|\n",
    "|eggs|0|.6021|0|0|\n",
    "|grass|.3010|0|0|.3010|\n",
    "|green|0|0|0|0|\n",
    "|ham|0|.6021|0|0|\n",
    "|like|.1249|.1249|.1249|0|\n",
    "|met|0|0|.6021|0|\n",
    "|out|.3010|.3010|0|0|\n",
    "|sailors|0|0|.6021|0|\n",
    "|sea|.3010|0|.3010|0|\n",
    "|spread|.3010|.3010|0|0|\n",
    "|the|0|0|0|0|\n",
    "|troubles|0|0|.6021|0|\n",
    "|was|.3010|0|0|.3010|\n",
    "|were|0|.3010|.3010|0|\n",
    "\n",
    "There are a few noteworthy things in this data. \n",
    "\n",
    "* The TF-IDF score for any word that does not occur in a text is 0.\n",
    "* The scores for almost every word in text4 are 0 since it is a shorter version of text1. There are no unique words in text4 since text1 contains all the same words. It is also a short text which means that there are only four words to consider. The words 'the' and 'green' occur in every text, leaving only 'was' and 'grass' which are also found in text1.\n",
    "* The words 'book', 'eggs', and 'ham' are significant in text2 since they only occur in that text.\n",
    "\n",
    "Now that you have a basic understanding of how TF-IDF is computed at a small scale, let's try computing TF-IDF on a JSTOR/Portico corpus which could contain millions of words.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing TF-IDF with your JSTOR/Portico Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing your dataset\n",
    "\n",
    "You have two options for bringing your dataset into the local environment:\n",
    "\n",
    "1. Manually download and upload your dataset\n",
    "2. Use a dataset id to automatically upload a dataset\n",
    "\n",
    "### Option one: Manually download and upload your dataset\n",
    "\n",
    "You can download your dataset from the corpus builder in the link shown below. (You may also have a link to your dataset in your email.) If you wish, you can modify your dataset on your local machine before the next upload phase. This gives you some more flexibility than automatically pulling in your dataset using a dataset ID using option 2 below.\n",
    "\n",
    "![The link for downloading your dataset](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/downloadDataset.png)\n",
    "\n",
    "Once you have your dataset ready on your local machine, you can then upload your dataset into JupyterLab by clicking the upload button in the file pane on the left.\n",
    "\n",
    "![The upload button in the file pane](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/uploadDataset.png)\n",
    "\n",
    "Make sure to upload your dataset to the \"datasets\" folder. \n",
    "\n",
    "### Option Two: Use a Dataset ID to automatically upload a dataset\n",
    "\n",
    "You'll use the tdm_client library to automatically upload your dataset. We import the `Dataset` module from the `tdm_client` library. The tdm_client library contains functions for connecting to the JSTOR server containing our [corpus](./key-terms.ipynb#corpus) [dataset](./key-terms.ipynb#dataset). To analyze your dataset, use the [dataset ID](./key-terms.ipynb/#dataset-ID) provided when you created your [dataset](./key-terms.ipynb/#dataset). A copy of your [dataset ID](./key-terms.ipynb/#dataset-ID) was sent to your email when you created your [corpus](./key-terms.ipynb#corpus). It should look like a long series of characters surrounded by dashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/sampleJournalAnalysis.jsonl'"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing your dataset with a dataset ID\n",
    "import tdm_client\n",
    "tdm_client.get_dataset(\"f6ae29d4-3a70-36ee-d601-20a8c0311273\", \"sampleJournalAnalysis\") #Load the sample dataset, the full run of Shakespeare Quarterly from 1950-2013.\n",
    "\n",
    "# Other humanities datasets:\n",
    "\n",
    "#English\n",
    "# Negro American Literature Forum (1967-1976) + Black American Literature Forum (1976-1991) + African American Review (1992-2016) (b4668c50-a970-c4d7-eb2c-bb6d04313542)\n",
    "# Shakespeare Quarterly (1950-2013) (f6ae29d4-3a70-36ee-d601-20a8c0311273)\n",
    "# ELH (1934-2014) (4999901a-fa17-31da-cfe5-2abf3a429df7)\n",
    "# College English (1939-2016) (a161f384-720b-b6bf-a0cc-4d7d3b857e1c)\n",
    "# PMLA (1889-2014) (1aea53b9-26d5-fe54-e35c-8259156ce6cd)\n",
    "\n",
    "#History\n",
    "\n",
    "#Philosophy\n",
    "\n",
    "#Anthropology\n",
    "\n",
    "#Law\n",
    "\n",
    "#Art\n",
    "\n",
    "#Classics\n",
    "#Classical Quarterly (1907-2014) (82014740-8ed9-3c34-5716-d0879b8317f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can begin working with our [dataset](./key-terms.ipynb#dataset), we need to convert the [JSON lines](./key-terms.ipynb#jsonl) file written in [JavaScript](./key-terms.ipynb#javascript) into [Python](./key-terms.ipynb#python) so we can work with it. Remember that each line of our [JSON lines](./key-terms.ipynb#jsonl) file represents a single text, whether that is a journal article, book, or something else. We will create a [Python](./key-terms.ipynb#python) list that contains every document. Within each list item for each document, we will use a [Python dictionary](./key-terms.ipynb#python-dictionary) of [key/value pairs](./key-terms.ipynb#key-value-pair) to store information related to that document. \n",
    "\n",
    "Essentially we will have a [list](./key-terms.ipynb#python-list) of documents numbered, from zero to the last document. Each [list](./key-terms.ipynb#python-list) item then will be composed of a [dictionary](./key-terms.ipynb#python-dictionary) of [key/value pairs](./key-terms.ipynb#key-value-pair) that allows us to retrieve information from that particular document by number. The structure will look something like this:\n",
    "\n",
    "![Structure of the corpus, a list of dictionaries](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CorpusView.png)\n",
    "\n",
    "For each item in our list we will be able to use [key/value pairs](./key-terms.ipynb#key-value-pair) to get a **value** if we supply a **key**. We will call our [Python list](./key-terms.ipynb#python-list) variable `all_documents` since it will contain all of the documents in our [corpus](./key-terms.ipynb#corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your filename and be sure your file is in your datasets folder\n",
    "file_name = 'sampleJournalAnalysis.jsonl' \n",
    "\n",
    "# Import the json module\n",
    "import json\n",
    "# Create an empty new list variable named `all_documents`\n",
    "all_documents = [] \n",
    "# Temporarily open the file `filename` in the datasets/ folder\n",
    "with open('./datasets/' + file_name) as dataset_file: \n",
    "    #for each line in the dataset file\n",
    "    for line in dataset_file: \n",
    "        # Read each line into a Python dictionary.\n",
    "        # Create a variable document that contains the line using json.loads to convert the json key/value pairs to a python dictionary\n",
    "        document = json.loads(line) \n",
    "        # Append a new list item to `all_documents` containing the dictionary we created.\n",
    "        all_documents.append(document) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of our documents have been converted from our original [JSON lines](./key-terms.ipynb#jsonl) file format (.jsonl) into a [python List](./key-terms.ipynb#python-list) variable named `all_documents`. Let's see what we can discover about our [corpus](./key-terms.ipynb#corpus) with a few simple methods.\n",
    "\n",
    "First, we can determine how many texts are in our [dataset](./key-terms.ipynb#dataset) by using the `len()` function to get the size of `all_documents`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6687"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Removing Articles That Are Not Full-Length\n",
    "\n",
    "When journal articles are added to JSTOR, they are broken up into chunks called articles. If we want to analyze every word in every issue, this approach works well. However, if we only want to analyze the full-length articles, we may want to remove some articles. These articles could be things like:\n",
    "\n",
    "* Tables of Contents\n",
    "* Indices\n",
    "* Unauthored Materials\n",
    "* Short Notes\n",
    "* Book Reviews\n",
    "\n",
    "We can design a set of assessments that will remove these materials. Depending on the journal and the field, shorter articles may still be relevant. The example code below demonstrates a set of rules for filtering using the article metadata.\n",
    "\n",
    "### Articles To Be Removed\n",
    "* Articles with no authors\n",
    "* Articles with title: \"Review Article\"\n",
    "* Articles with title: \"Front Matter\"\n",
    "* Articles with title: \"Back Matter\"\n",
    "* Articles with a word count less than 3000 words\n",
    "\n",
    "The function below outputs a list of the first ten articles followed by the reason each article would or would not be kept. This can be a useful exploratory tool for getting the best starting corpus. You may need to adjust the word count number up or down, particularly for distinguishing between say short notes and full-length articles. You might also consider writing other metadata field tests to narrow your corpus. **Note: The Article Assessment Exploration code below does not change your corpus found within ``all_documents``. It merely serves as a convenient way to consider the logic of your filtering.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7bac316ad111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#articles_to_show = len(all_documents) # Uncomment to show all articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles_to_show\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Repeat this process the number of times as the value of ``articles_to_show`` + 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mremove_non_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_documents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Run the remove_non_articles function on a single document with list index value of ``i``\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_documents' is not defined"
     ]
    }
   ],
   "source": [
    "#Article Assessment Exploration\n",
    "\n",
    "## Define a function ``remove_non_articles`` that will test a single document\n",
    "def remove_non_articles(test_doc):\n",
    "    print('Article ' + str(i) + ':') # Print the list index for each article so they can be easily referenced\n",
    "    print('Title: ' + test_doc.get('title')) # Print the title for the article in question\n",
    "    print('URL: ' + test_doc.get('id')) # Print the URL for the article so it can be quickly reviewed\n",
    "    print('Status: ', end='') # Print the phrase 'Status' without a following line-break\n",
    "    if test_doc.get('creators') == None: # Get the value for the key 'creators' in the test document and check if it is equal to none\n",
    "        print('Removed--No author') # If the value for 'creators' is none, print 'Removed--No author'\n",
    "    elif test_doc.get('title') == 'Review Article': # Get the value for the key 'title' in the test document and check if it is equal to 'Review Article'\n",
    "        print('Removed--Review Article') # If the value for 'title' is 'Review Article', print 'Removed--Review Article'\n",
    "    elif test_doc.get('title') == 'Front Matter': # Get the value for the key 'title' in the test document and check if it is equal to 'Front Matter'\n",
    "        print('Removed--Front Matter') # If the value for 'title' is 'Front Matter', print 'Removed--Front Matter'\n",
    "    elif test_doc.get('title') == 'Back Matter': # Get the value for the key 'title' in the test document and check if it is equal to 'Back Matter'\n",
    "        print('Removed--Back Matter')  # If the value for 'title' is 'Back Matter', print 'Removed--Back Matter'\n",
    "    elif test_doc.get('wordCount') < 3000: # Get the value for 'wordCount' in the test document and check if the integer is less than 3000 words (Change this number if you want more or less words)\n",
    "        print('Removed--Too short at ' + str(test_doc.get('wordCount')) + ' words') # If the value for wordCount is less than 3000, print 'Removed--Too short at' followed by the actual word count\n",
    "    else:\n",
    "        print('GOOD ARTICLE at '+ str(test_doc.get('wordCount')) + ' words') # If the article passes all the above tests, print 'GOOD ARTICLE at ' with the article word count\n",
    "\n",
    "articles_to_show = 10 # Show the first ten articles (Change this number to show more or fewer articles)\n",
    "#articles_to_show = len(all_documents) # Uncomment to show all articles\n",
    "for i in range(articles_to_show): # Repeat this process the number of times as the value of ``articles_to_show`` + 1\n",
    "    remove_non_articles(all_documents[i])  # Run the remove_non_articles function on a single document with list index value of ``i``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done some exploratory analysis to figure out the right parameters for filtering our corpus, let's put them into practice. The following set of list comprehensions creates a new list called ``reduced_list`` from our original corpus ``all_documents``. After each filtering, the number of kept articles is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of documents: 6687\n",
      "After removing articles with no authors: 5303\n",
      "After removing \"Review Articles\": 3610\n",
      "After removing articles labeled \"Front Matter\": 3413\n",
      "After removing articles labeled \"Back Matter\": 3276\n",
      "After removing short articles: 2399\n"
     ]
    }
   ],
   "source": [
    "print('Original number of documents: ' + str(len(all_documents))) # Print the original number of documents in ``all_documents``\n",
    "reduced_list = [all_documents[x] for x in range(len(all_documents)) if all_documents[x].get('creators') != None] # Copy each list item from ``all_documents`` to ``reduced_list`` if the ``creators`` key does not have a value pair of None\n",
    "print('After removing articles with no authors: ' + str(len(reduced_list))) # Print the current size of ``reduced_list``\n",
    "reduced_list = [all_documents[x] for x in range(len(reduced_list)) if all_documents[x].get('title') != 'Review Article'] # Copy each list item from ``reduced_list`` to ``reduced_list`` if the ``title`` key does not have a value pair of 'Review Article'\n",
    "print('After removing \"Review Articles\": ' + str(len(reduced_list))) # Print the current size of ``reduced_list``\n",
    "reduced_list = [all_documents[x] for x in range(len(reduced_list)) if reduced_list[x].get('title') != 'Front Matter'] # Copy each list item from ``reduced_list`` to ``reduced_list`` if the ``title`` key does not have a value pair of 'Front Matter'\n",
    "print('After removing articles labeled \"Front Matter\": ' + str(len(reduced_list))) # Print the current size of ``reduced_list``\n",
    "reduced_list = [all_documents[x] for x in range(len(reduced_list)) if reduced_list[x].get('title') != 'Back Matter'] # Copy each list item from ``reduced_list`` to ``reduced_list`` if the ``title`` key does not have a value pair of 'Back Matter'\n",
    "print('After removing articles labeled \"Back Matter\": ' + str(len(reduced_list))) # Print the current size of ``reduced_list``\n",
    "reduced_list = [all_documents[x] for x in range(len(reduced_list)) if reduced_list[x].get('wordCount') < 3000] # Copy each list item from ``reduced_list`` to ``reduced_list`` if the ``wordCount`` has a value pair less than 3000\n",
    "print('After removing short articles: ' + str(len(reduced_list))) # Print the current size of ``reduced_list``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up the Tokens in the Corpus\n",
    "\n",
    "Let's create a helper function that can standardize and [clean](./key-terms.ipynb#clean-data) up the [tokens](./key-terms.ipynb#token) in our [dataset](./key-terms.ipynb#dataset). The function will:\n",
    "* lower cases all [tokens](./key-terms.ipynb#token)\n",
    "* use a dictionary from [The HathiTrust Research Center](./key-terms.ipynb#htrc) to correct common [Optical Character Recognition](./key-terms.ipynb#ocr) problems\n",
    "* discard [tokens](./key-terms.ipynb#token) less than 4 characters in length\n",
    "* discard [tokens](./key-terms.ipynb#token) with non-alphabetical characters\n",
    "* remove [stopwords](./key-terms.ipynb#stop-words) based on [The HathiTrust Research Center](./key-terms.ipynb#htrc) [stopword](./key-terms.ipynb#stop-words) list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm_client import htrc_corrections # Import the htrc_corrections that helps correct common OCR problems\n",
    "\n",
    "def process_token(token): #define a function `process_token` that takes the argument `token`\n",
    "    token = token.lower() #set the string in token to a new string with all lowercase letters\n",
    "    corrected = htrc_corrections.get(token) #initialize a new variable `corrected` that runs token through the `htrc_corrections.get()` function to fix common OCR errors\n",
    "    if corrected is not None: #if corrected has a value, set the `token` variable to the same value as `corrected`\n",
    "        token = corrected\n",
    "    if len(token) < 4: #if token is less than four characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    if not(token.isalpha()): #if token contains non-alphabetic characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    return token #return the `token` variable which has been set equal to the `corrected` variable\n",
    "\n",
    "def process_document(chosen_document): # Create a new function ``process_document`` that takes the argument chosen_document\n",
    "    this_doc = [] # Create a new list ``this_doc`` that will hold the contents of the current document\n",
    "    singleDoc = chosen_document.get('unigramCount') # Create a list variable ``singleDoc` that will contain the contents of `unigramCount` for the current document\n",
    "    for token, count in singleDoc.items(): # For each token in the document, \n",
    "        clean_token = process_token(token) # Use the ``process_token`` function above to clean that token\n",
    "        if clean_token is None: # If there is no token returned, proceed\n",
    "            continue\n",
    "        this_doc += [clean_token] * count # Add to ``this_doc`` list the number of token occurences\n",
    "    documents.append(this_doc) # Add the token count results for ``this_doc`` to the ``documents`` list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cycle through each document in the [corpus](./key-terms.ipynb#corpus) with our helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [] # An empty variable ``documents`` that will contain all our documents with cleaned tokens\n",
    "for i in range(len(reduced_list)): # Repeat this process once for every document in ``reduced_list``\n",
    "    process_document(reduced_list[i]) # Run the ``process_token`` function on the single article by reference to its index number of i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Using Gensim to Compute \"Term Frequency- Inverse Document Frequency\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most significant terms, by TFIDF, in the curated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = {\n",
    "        dictionary.get(_id): value for doc in corpus_tfidf\n",
    "        for _id, value in doc\n",
    "    }\n",
    "sorted_td = sorted(td.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ofamiem 1.0\n",
      "ouderdom 0.9127008148928485\n",
      "sturgess 0.9086024303923905\n",
      "zamir 0.8776651519018113\n",
      "santayana 0.8665736199609847\n",
      "falocco 0.8562928124271615\n",
      "weingust 0.8547899776401692\n",
      "chinese 0.8462001652815331\n",
      "weils 0.8445705864452131\n",
      "rudanko 0.8390830868389877\n",
      "enbiemata 0.8280102498481464\n",
      "daileader 0.8171301955562135\n",
      "nodier 0.8168018882816901\n",
      "usury 0.8005782510346803\n",
      "menas 0.7909230276348473\n",
      "beaurline 0.7905965479055058\n",
      "spectogram 0.7879121817261375\n",
      "franciscus 0.7771865284620213\n",
      "soellner 0.77327831058108\n",
      "bastarde 0.7712490973605648\n",
      "unton 0.7682807524450844\n",
      "cohens 0.7677867862204198\n",
      "falco 0.7635813973078707\n",
      "callimachus 0.7604638066069844\n",
      "wynkyn 0.7583409894849417\n"
     ]
    }
   ],
   "source": [
    "for term, weight in sorted_td[:25]:\n",
    "    print(term, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the most significant word, by TFIDF, for the first 50 documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.jstor.org/stable/2869980 henslowe 0.33564303350040253\n",
      "http://www.jstor.org/stable/2870198 stairs 0.15472663369841613\n",
      "http://www.jstor.org/stable/2870199 beatrice 0.23846910220032727\n",
      "http://www.jstor.org/stable/2870209 donaldson 0.4977445938192701\n",
      "http://www.jstor.org/stable/2870208 cheng 0.6067690431054176\n",
      "http://www.jstor.org/stable/2870203 hartwig 0.6244428220115\n",
      "http://www.jstor.org/stable/2870194 hall 0.5136214288878345\n",
      "http://www.jstor.org/stable/2870206 novy 0.5645876593477805\n",
      "http://www.jstor.org/stable/2870202 booth 0.3864621355873546\n",
      "http://www.jstor.org/stable/2870313 vizcaya 0.6046299393365276\n",
      "http://www.jstor.org/stable/2870327 hollar 0.639842687006522\n",
      "http://www.jstor.org/stable/2870308 longleat 0.5039894018479222\n",
      "http://www.jstor.org/stable/2869730 andidentifies 0.1635030038209591\n",
      "http://www.jstor.org/stable/2869726 rubinstein 0.5497022223906934\n",
      "http://www.jstor.org/stable/2871196 carded 0.2820633806904763\n",
      "http://www.jstor.org/stable/2871206 jaggard 0.2704892925363586\n",
      "http://www.jstor.org/stable/2870086 roger 0.22407927452908946\n",
      "http://www.jstor.org/stable/2870094 taylor 0.3310153818503154\n",
      "http://www.jstor.org/stable/2869777 production 0.20771778787771467\n",
      "http://www.jstor.org/stable/2869765 reviewing 0.19442107347889528\n",
      "http://www.jstor.org/stable/2869775 theatre 0.29861889467571856\n",
      "http://www.jstor.org/stable/2870161 lear 0.2660105431143734\n",
      "http://www.jstor.org/stable/2870157 italian 0.21087240690666048\n",
      "http://www.jstor.org/stable/2870177 jardine 0.5611803888072969\n",
      "http://www.jstor.org/stable/2870185 mailing 0.1992704223595866\n",
      "http://www.jstor.org/stable/2870383 laan 0.6921640509219383\n",
      "http://www.jstor.org/stable/2870386 brown 0.25477440033888865\n",
      "http://www.jstor.org/stable/2870376 television 0.24732995544933492\n",
      "http://www.jstor.org/stable/2870394 simonds 0.2739133793708261\n",
      "http://www.jstor.org/stable/2870395 cloth 0.27133610060734786\n",
      "http://www.jstor.org/stable/2870384 vice 0.1615457820296335\n",
      "http://www.jstor.org/stable/2870379 utah 0.30647004664738303\n",
      "http://www.jstor.org/stable/2869930 suetonius 0.2277299687699213\n",
      "http://www.jstor.org/stable/2869949 dissertation 0.3083116834796744\n",
      "http://www.jstor.org/stable/2869929 tail 0.5099418545362527\n",
      "http://www.jstor.org/stable/2869947 bliss 0.3311906461731288\n",
      "http://www.jstor.org/stable/2869942 driscoll 0.2718201396975177\n",
      "http://www.jstor.org/stable/2869920 universitn 0.26078064915896176\n",
      "http://www.jstor.org/stable/2869938 stair 0.25089921713600366\n",
      "http://www.jstor.org/stable/2869704 capitol 0.5955814030640157\n",
      "http://www.jstor.org/stable/2869682 bullingbrook 0.20883169101665788\n",
      "http://www.jstor.org/stable/2869695 mcanuff 0.2313300162117224\n",
      "http://www.jstor.org/stable/2869703 dash 0.5728806682575506\n",
      "http://www.jstor.org/stable/2870062 miller 0.4990204703075245\n",
      "http://www.jstor.org/stable/2870055 teaching 0.26188675285977076\n",
      "http://www.jstor.org/stable/2870068 students 0.22910712319925738\n",
      "http://www.jstor.org/stable/2869860 judah 0.5459643576202144\n",
      "http://www.jstor.org/stable/2869859 magician 0.5056121470707868\n",
      "http://www.jstor.org/stable/2869911 facsimile 0.25122626649949076\n",
      "http://www.jstor.org/stable/2869915 gyman 0.1411382206138676\n",
      "http://www.jstor.org/stable/2869912 distribution 0.3138351376547982\n"
     ]
    }
   ],
   "source": [
    "for n, doc in enumerate(corpus_tfidf):\n",
    "    if len(doc) < 1:\n",
    "        continue\n",
    "    word_id, score = max(doc, key=lambda x: x[1])\n",
    "    print(reduced_list[n].get('id'), dictionary.get(word_id), score)\n",
    "    if n >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
