{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By <a href=\"https://nkelber.com\">Nathan Kelber</a> and Ted Lawless <br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.\n",
    "____\n",
    "\n",
    "## Finding Significant Words within a Dataset Using TF/IDF\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Programming Knowledge Required:** \n",
    "This notebook can be run on a JSTOR/Portico [non-consumptive](./key-terms.ipynb#non-consumptive) [JSON Lines (.jsonl)](./key-terms.ipynb#jsonl) [dataset](./key-terms.ipynb#dataset) with little to no knowledge of [Python](./key-terms.ipynb#python). To have a full understanding of the code used in this [notebook](./key-terms.ipynb#jupyter-notebook), we recommend learning:\n",
    "* [Python Basics](https://automatetheboringstuff.com/2e/chapter1/)\n",
    "* [Flow Control](https://automatetheboringstuff.com/2e/chapter2/)\n",
    "* [Functions](https://automatetheboringstuff.com/2e/chapter3/)\n",
    "* [Lists](https://automatetheboringstuff.com/2e/chapter4/)\n",
    "* [Dictionaries](https://automatetheboringstuff.com/2e/chapter5/)\n",
    "\n",
    "**Completion time:** 35 minutes\n",
    "\n",
    "**Data Format:** [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico) [non-consumptive](./key-terms.ipynb#non-consumptive) [JSON Lines (.jsonl)](./key-terms.ipynb#jsonl)\n",
    "\n",
    "**Libraries Used:**\n",
    "* **[json](./key-terms.ipynb#json-python-library)** to convert our dataset from json lines format to a Python list\n",
    "* **[gensim](./key-terms.ipynb#gensim)** to help compute the [tf-idf](./key-terms.ipynb#tf-idf) calculation\n",
    "\n",
    "**Description of methods in this notebook:**\n",
    "This [notebook](./key-terms.ipynb#jupyter-notebook) shows how to discover significant words in your [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico) [dataset](./key-terms.ipynb#dataset) using [Python](./key-terms.ipynb#python). The method for finding significant terms is [tf-idf](./key-terms.ipynb#tf-idf).  The following processes are described:\n",
    "\n",
    "* Converting your [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico)[dataset](./key-terms.ipynb#dataset) into a Python list\n",
    "* Writing a helper function to help clean up a single [token](./key-terms.ipynb#token)\n",
    "* Cleaning each document of your dataset, one [token](./key-terms.ipynb#token) at a time\n",
    "* Using a dictionary of English words to remove words with poor [OCR](./key-terms.ipynb#ocr)\n",
    "* Computing the most significant words in your [corpus](./key-terms.ipynb#corpus) using [TFIDF](./key-terms.ipynb#tf-idf) with the [gensim](./key-terms.ipynb#gensim) library\n",
    "\n",
    "A familiarity with [gensim](./key-terms.ipynb#gensim) is helpful but not required.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing your dataset\n",
    "\n",
    "You have two options for bringing your dataset into the local environment:\n",
    "\n",
    "1. Manually download and upload your dataset\n",
    "2. Use a dataset id to automatically upload a dataset\n",
    "\n",
    "### Option one: Manually download and upload your dataset\n",
    "\n",
    "You can download your dataset from the corpus builder in the link shown below. (You may also have a link to your dataset in your email.) If you wish, you can modify your dataset on your local machine before the next upload phase. This gives you some more flexibility than automatically pulling in your dataset using a dataset ID using option 2 below.\n",
    "\n",
    "![The link for downloading your dataset](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/downloadDataset.png)\n",
    "\n",
    "Once you have your dataset ready on your local machine, you can then upload your dataset into JupyterLab by clicking the upload button in the file pane on the left.\n",
    "\n",
    "![The upload button in the file pane](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/uploadDataset.png)\n",
    "\n",
    "Make sure to upload your dataset to the \"datasets\" folder. \n",
    "\n",
    "### Option Two: Use a Dataset ID to automatically upload a dataset\n",
    "\n",
    "You'll use the tdm_client library to automatically upload your dataset. We import the `Dataset` module from the `tdm_client` library. The tdm_client library contains functions for connecting to the JSTOR server containing our [corpus](./key-terms.ipynb#corpus) [dataset](./key-terms.ipynb#dataset). To analyze your dataset, use the [dataset ID](./key-terms.ipynb/#dataset-ID) provided when you created your [dataset](./key-terms.ipynb/#dataset). A copy of your [dataset ID](./key-terms.ipynb/#dataset-ID) was sent to your email when you created your [corpus](./key-terms.ipynb#corpus). It should look like a long series of characters surrounded by dashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/sampleJournalAnalysis.jsonl'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing your dataset with a dataset ID\n",
    "import tdm_client\n",
    "tdm_client.get_dataset(\"f6ae29d4-3a70-36ee-d601-20a8c0311273\", \"sampleJournalAnalysis\") #Load the sample dataset, the full run of Shakespeare Quarterly from 1950-2013.\n",
    "\n",
    "# Other humanities datasets:\n",
    "\n",
    "#English\n",
    "# Negro American Literature Forum (1967-1976) + Black American Literature Forum (1976-1991) + African American Review (1992-2016) (b4668c50-a970-c4d7-eb2c-bb6d04313542)\n",
    "# Shakespeare Quarterly (1950-2013) (f6ae29d4-3a70-36ee-d601-20a8c0311273)\n",
    "# ELH (1934-2014) (4999901a-fa17-31da-cfe5-2abf3a429df7)\n",
    "# College English (1939-2016) (a161f384-720b-b6bf-a0cc-4d7d3b857e1c)\n",
    "# PMLA (1889-2014) (1aea53b9-26d5-fe54-e35c-8259156ce6cd)\n",
    "\n",
    "#History\n",
    "\n",
    "#Philosophy\n",
    "\n",
    "#Anthropology\n",
    "\n",
    "#Law\n",
    "\n",
    "#Art\n",
    "\n",
    "#Classics\n",
    "#Classical Quarterly (1907-2014) (82014740-8ed9-3c34-5716-d0879b8317f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can begin working with our [dataset](./key-terms.ipynb#dataset), we need to convert the [JSON lines](./key-terms.ipynb#jsonl) file written in [JavaScript](./key-terms.ipynb#javascript) into [Python](./key-terms.ipynb#python) so we can work with it. Remember that each line of our [JSON lines](./key-terms.ipynb#jsonl) file represents a single text, whether that is a journal article, book, or something else. We will create a [Python](./key-terms.ipynb#python) list that contains every document. Within each list item for each document, we will use a [Python dictionary](./key-terms.ipynb#python-dictionary) of [key/value pairs](./key-terms.ipynb#key-value-pair) to store information related to that document. \n",
    "\n",
    "Essentially we will have a [list](./key-terms.ipynb#python-list) of documents numbered, from zero to the last document. Each [list](./key-terms.ipynb#python-list) item then will be composed of a [dictionary](./key-terms.ipynb#python-dictionary) of [key/value pairs](./key-terms.ipynb#key-value-pair) that allows us to retrieve information from that particular document by number. The structure will look something like this:\n",
    "\n",
    "![Structure of the corpus, a list of dictionaries](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CorpusView.png)\n",
    "\n",
    "For each item in our list we will be able to use [key/value pairs](./key-terms.ipynb#key-value-pair) to get a **value** if we supply a **key**. We will call our [Python list](./key-terms.ipynb#python-list) variable `all_documents` since it will contain all of the documents in our [corpus](./key-terms.ipynb#corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your filename and be sure your file is in your datasets folder\n",
    "file_name = 'sampleJournalAnalysis.jsonl' \n",
    "\n",
    "# Import the json module\n",
    "import json\n",
    "# Create an empty new list variable named `all_documents`\n",
    "all_documents = [] \n",
    "# Temporarily open the file `filename` in the datasets/ folder\n",
    "with open('./datasets/' + file_name) as dataset_file: \n",
    "    #for each line in the dataset file\n",
    "    for line in dataset_file: \n",
    "        # Read each line into a Python dictionary.\n",
    "        # Create a variable document that contains the line using json.loads to convert the json key/value pairs to a python dictionary\n",
    "        document = json.loads(line) \n",
    "        # Append a new list item to `all_documents` containing the dictionary we created.\n",
    "        all_documents.append(document) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of our documents have been converted from our original [JSON lines](./key-terms.ipynb#jsonl) file format (.jsonl) into a [python List](./key-terms.ipynb#python-list) variable named `all_documents`. Let's see what we can discover about our [corpus](./key-terms.ipynb#corpus) with a few simple methods.\n",
    "\n",
    "First, we can determine how many texts are in our [dataset](./key-terms.ipynb#dataset) by using the `len()` function to get the size of `all_documents`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6687"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function that can standardize and [clean](./key-terms.ipynb#clean-data) up the [tokens](./key-terms.ipynb#token) in our [dataset](./key-terms.ipynb#dataset). The function will:\n",
    "* lower cases all [tokens](./key-terms.ipynb#token)\n",
    "* use a dictionary from [The HathiTrust Research Center](./key-terms.ipynb#htrc) to correct common [Optical Character Recognition](./key-terms.ipynb#ocr) problems\n",
    "* discard [tokens](./key-terms.ipynb#token) less than 4 characters in length\n",
    "* discard [tokens](./key-terms.ipynb#token) with non-alphabetical characters\n",
    "* remove [stopwords](./key-terms.ipynb#stop-words) based on [The HathiTrust Research Center](./key-terms.ipynb#htrc) [stopword](./key-terms.ipynb#stop-words) list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm_client import htrc_corrections\n",
    "\n",
    "def process_token(token): #define a function `process_token` that takes the argument `token`\n",
    "    token = token.lower() #set the string in token to a new string with all lowercase letters\n",
    "    corrected = htrc_corrections.get(token) #initialize a new variable `corrected` that runs token through the `htrc_corrections.get()` function to fix common OCR errors\n",
    "    if corrected is not None: #if corrected has a value, set the `token` variable to the same value as `corrected`\n",
    "        token = corrected\n",
    "    if len(token) < 4: #if token is less than four characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    if not(token.isalpha()): #if token contains non-alphabetic characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    return token #return the `token` variable which has been set equal to the `corrected` variable\n",
    "\n",
    "def process_document(chosen_document):\n",
    "    this_doc = []\n",
    "    singleDoc = chosen_document.get('unigramCount')\n",
    "    for token, count in singleDoc.items():\n",
    "        clean_token = process_token(token)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    documents.append(this_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(len(all_documents)):\n",
    "    process_document(all_documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cycle through each document in the [corpus](./key-terms.ipynb#corpus) with our helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most significant terms, by TFIDF, in the curated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = {\n",
    "        dictionary.get(_id): value for doc in corpus_tfidf\n",
    "        for _id, value in doc\n",
    "    }\n",
    "sorted_td = sorted(td.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ofamiem 1.0\n",
      "cwtrnca 0.9863483873087012\n",
      "chartres 0.9273378589673646\n",
      "worken 0.9207300498619446\n",
      "sobran 0.9075508762980733\n",
      "nuimber 0.8775001624677011\n",
      "weingust 0.8755826466137229\n",
      "rudanko 0.86000135238765\n",
      "enbiemata 0.8563716462679598\n",
      "weils 0.8472587947507879\n",
      "sliv 0.8394611060327918\n",
      "snuggs 0.8381178515481901\n",
      "ouderdom 0.8316113874601273\n",
      "habib 0.8308118578625007\n",
      "buzacott 0.8303061621922353\n",
      "gaiicanus 0.8294403338236659\n",
      "holmer 0.8201937618215803\n",
      "spectogram 0.817057687336797\n",
      "reproducedfrmtefgr 0.8139670501922074\n",
      "womersley 0.8080655409128259\n",
      "dulcitius 0.8048781892048817\n",
      "margolies 0.7961508372216022\n",
      "dugas 0.7868223742710501\n",
      "willeford 0.7859219459401604\n",
      "tcad 0.7819493652557465\n"
     ]
    }
   ],
   "source": [
    "for term, weight in sorted_td[:25]:\n",
    "    print(term, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the most significant word, by TFIDF, for the first 50 documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.jstor.org/stable/2869980 henslowe 0.32498040467805084\n",
      "http://www.jstor.org/stable/2870198 nimrod 0.1616432176708553\n",
      "http://www.jstor.org/stable/2870199 beatrice 0.22776869095345914\n",
      "http://www.jstor.org/stable/2870209 donaldson 0.47661799586847836\n",
      "http://www.jstor.org/stable/2870208 cheng 0.5913759877754204\n",
      "http://www.jstor.org/stable/2870189 antonio 0.44365366900689435\n",
      "http://www.jstor.org/stable/2870193 painting 0.4091485612863863\n",
      "http://www.jstor.org/stable/2870188 edgar 0.3258325027350309\n",
      "http://www.jstor.org/stable/2870203 hartwig 0.6147359515246972\n",
      "http://www.jstor.org/stable/2870194 hall 0.5121228500237642\n",
      "http://www.jstor.org/stable/2870206 novy 0.5473522309642914\n",
      "http://www.jstor.org/stable/2870202 booth 0.3863680373528867\n",
      "http://www.jstor.org/stable/2870313 vizcaya 0.601154198547381\n",
      "http://www.jstor.org/stable/2870307 dennis 0.3266904513150024\n",
      "http://www.jstor.org/stable/2870327 hollar 0.6487395808866938\n",
      "http://www.jstor.org/stable/2870308 longleat 0.5023342170474715\n",
      "http://www.jstor.org/stable/2870304 welles 0.6010247674772539\n",
      "http://www.jstor.org/stable/2869730 andidentifies 0.17159914957892092\n",
      "http://www.jstor.org/stable/2869726 rubinstein 0.5362488350329068\n",
      "http://www.jstor.org/stable/2871208 goals 0.23001427827326723\n",
      "http://www.jstor.org/stable/2871196 carded 0.29724672445968886\n",
      "http://www.jstor.org/stable/2871206 jaggard 0.27305742735731064\n",
      "http://www.jstor.org/stable/2870092 kentucky 0.23729556330149262\n",
      "http://www.jstor.org/stable/2870086 roger 0.22204664860545675\n",
      "http://www.jstor.org/stable/2870090 arena 0.18335304148692239\n",
      "http://www.jstor.org/stable/2870080 foster 0.3553404112536266\n",
      "http://www.jstor.org/stable/2870094 taylor 0.33191619292482416\n",
      "http://www.jstor.org/stable/2869774 reviewer 0.5117030660367732\n",
      "http://www.jstor.org/stable/2869777 production 0.20845224766777756\n",
      "http://www.jstor.org/stable/2869765 reviewing 0.19192121094797993\n",
      "http://www.jstor.org/stable/2869775 theatre 0.28943418086170686\n",
      "http://www.jstor.org/stable/2870172 wedor 0.33478083317514656\n",
      "http://www.jstor.org/stable/2870161 lear 0.26236016042521487\n",
      "http://www.jstor.org/stable/2870157 italian 0.21204005550294144\n",
      "http://www.jstor.org/stable/2870177 jardine 0.5479900617282173\n",
      "http://www.jstor.org/stable/2870185 mailing 0.19113866281258757\n",
      "http://www.jstor.org/stable/2870383 laan 0.7010845484536087\n",
      "http://www.jstor.org/stable/2870386 brown 0.24750846521211095\n",
      "http://www.jstor.org/stable/2870376 television 0.2401880358550755\n",
      "http://www.jstor.org/stable/2870394 simonds 0.25567208995060603\n",
      "http://www.jstor.org/stable/2870395 cloth 0.268351744105865\n",
      "http://www.jstor.org/stable/2870384 vice 0.16223647555933257\n",
      "http://www.jstor.org/stable/2870379 utah 0.2908187767136855\n",
      "http://www.jstor.org/stable/2869930 suetonius 0.22402970366558822\n",
      "http://www.jstor.org/stable/2869935 boston 0.2900993545994711\n",
      "http://www.jstor.org/stable/2869949 dissertation 0.2929867243070928\n",
      "http://www.jstor.org/stable/2869929 tail 0.5079683689976742\n",
      "http://www.jstor.org/stable/2869932 prague 0.24510894149698817\n",
      "http://www.jstor.org/stable/2869947 bliss 0.32079684342023224\n",
      "http://www.jstor.org/stable/2869942 driscoll 0.2859064137696673\n",
      "http://www.jstor.org/stable/2869920 universitn 0.2748416895015885\n"
     ]
    }
   ],
   "source": [
    "for n, doc in enumerate(corpus_tfidf):\n",
    "    if len(doc) < 1:\n",
    "        continue\n",
    "    word_id, score = max(doc, key=lambda x: x[1])\n",
    "    print(all_documents[n].get('id'), dictionary.get(word_id), score)\n",
    "    if n >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
