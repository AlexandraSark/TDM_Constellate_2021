{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By <a href=\"https://nkelber.com\">Nathan Kelber</a> and Ted Lawless <br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.\n",
    "____\n",
    "## Explore the metadata for a JSTOR and/or Portico dataset\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Programming Knowledge Required:** \n",
    "This notebook can be run on a JSTOR/Portico [non-consumptive](./key-terms.ipynb#non-consumptive) [JSON Lines (.jsonl)](./key-terms.ipynb#jsonl) [dataset](./key-terms.ipynb#dataset) with little to no knowledge of [Python](./key-terms.ipynb#python). To have a full understanding of the code used in this [notebook](./key-terms.ipynb#jupyter-notebook), we recommend learning:\n",
    "* [Python Basics](https://automatetheboringstuff.com/2e/chapter1/)\n",
    "* [Flow Control](https://automatetheboringstuff.com/2e/chapter2/)\n",
    "* [Functions](https://automatetheboringstuff.com/2e/chapter3/)\n",
    "* [Lists](https://automatetheboringstuff.com/2e/chapter4/)\n",
    "* [Dictionaries](https://automatetheboringstuff.com/2e/chapter5/)\n",
    "\n",
    "A familiarity with [The Natural Language Toolkit](./key-terms.ipynb#nltk) and [Counter objects](./key-terms.ipynb#python-counter) is helpful but not required.\n",
    "\n",
    "**Completion time:** 35 minutes\n",
    "\n",
    "**Data Format:** [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico) [non-consumptive](./key-terms.ipynb#non-consumptive) [JSON Lines (.jsonl)](./key-terms.ipynb#jsonl)\n",
    "\n",
    "**Libraries Used:**\n",
    "* **[json](./key-terms.ipynb#json-python-library)** to convert our dataset from json lines format to a Python list\n",
    "* **[NLTK](./key-terms.ipynb#nltk)** to help [clean](./key-terms.ipynb#clean-data) up our dataset\n",
    "* **Counter** from the **Collections Module** to help sum up our word frequencies\n",
    "\n",
    "**Description of methods in this notebook:**\n",
    "This [notebook](./key-terms.ipynb#jupyter-notebook) shows how to explore the [word frequencies](./key-terms.ipynb#word-frequency) of your [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico) [dataset](./key-terms.ipynb#dataset) using [Python](./key-terms.ipynb#python). The following processes are described:\n",
    "\n",
    "* Converting your [JSTOR](./key-terms.ipynb#jstor) and/or [Portico](./key-terms.ipynb#portico)[dataset](./key-terms.ipynb#dataset) into a Python list\n",
    "* Creating a raw word frequency count\n",
    "* Creating and modifying a [stop words list](./key-terms.ipynb#stop-words)\n",
    "* Cleaning up the [corpus](./key-terms.ipynb#corpus)\n",
    "* Create a new word frequency list focused on [content words](./key-terms.ipynb#content-words)\n",
    "\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing your dataset\n",
    "\n",
    "You have two options for bringing your dataset into the local environment:\n",
    "\n",
    "1. Manually download and upload your dataset\n",
    "2. Use a dataset id to automatically upload a dataset\n",
    "\n",
    "### Option one: Manually download and upload your dataset\n",
    "\n",
    "You can download your dataset from the corpus builder in the link shown below. (You may also have a link to your dataset in your email.) If you wish, you can modify your dataset on your local machine before the next upload phase. This gives you some more flexibility than automatically pulling in your dataset using a dataset ID using option 2 below.\n",
    "\n",
    "![The link for downloading your dataset](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/downloadDataset.png)\n",
    "\n",
    "Once you have your dataset ready on your local machine, you can then upload your dataset into JupyterLab by clicking the upload button in the file pane on the left.\n",
    "\n",
    "![The upload button in the file pane](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/uploadDataset.png)\n",
    "\n",
    "Make sure to upload your dataset to the \"datasets\" folder. \n",
    "\n",
    "### Option Two: Use a Dataset ID to automatically upload a dataset\n",
    "\n",
    "You'll use the tdm_client library to automatically upload your dataset. We import the `Dataset` module from the `tdm_client` library. The tdm_client library contains functions for connecting to the JSTOR server containing our [corpus](./key-terms.ipynb#corpus) [dataset](./key-terms.ipynb#dataset). To analyze your dataset, use the [dataset ID](./key-terms.ipynb/#dataset-ID) provided when you created your [dataset](./key-terms.ipynb/#dataset). A copy of your [dataset ID](./key-terms.ipynb/#dataset-ID) was sent to your email when you created your [corpus](./key-terms.ipynb#corpus). It should look like a long series of characters surrounded by dashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/sampleJournalAnalysis.jsonl'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing your dataset with a dataset ID\n",
    "import tdm_client\n",
    "tdm_client.get_dataset(\"82014740-8ed9-3c34-5716-d0879b8317f6\", \"sampleJournalAnalysis\") #Load the sample dataset, the full run of Shakespeare Quarterly from 1950-2013.\n",
    "\n",
    "# Other humanities datasets:\n",
    "\n",
    "#English\n",
    "# Negro American Literature Forum (1967-1976) + Black American Literature Forum (1976-1991) + African American Review (1992-2016) (b4668c50-a970-c4d7-eb2c-bb6d04313542)\n",
    "# Shakespeare Quarterly (1950-2013) (f6ae29d4-3a70-36ee-d601-20a8c0311273)\n",
    "# ELH (1934-2014) (4999901a-fa17-31da-cfe5-2abf3a429df7)\n",
    "# College English (1939-2016) (a161f384-720b-b6bf-a0cc-4d7d3b857e1c)\n",
    "# PMLA (1889-2014) (1aea53b9-26d5-fe54-e35c-8259156ce6cd)\n",
    "\n",
    "#History\n",
    "\n",
    "#Philosophy\n",
    "\n",
    "#Anthropology\n",
    "\n",
    "#Law\n",
    "\n",
    "#Art\n",
    "\n",
    "#Classics\n",
    "#Classical Quarterly (1907-2014) (82014740-8ed9-3c34-5716-d0879b8317f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can begin working with our [dataset](./key-terms.ipynb#dataset), we need to convert the [JSON lines](./key-terms.ipynb#jsonl) file written in [JavaScript](./key-terms.ipynb#javascript) into [Python](./key-terms.ipynb#python) so we can work with it. Remember that each line of our [JSON lines](./key-terms.ipynb#jsonl) file represents a single text, whether that is a journal article, book, or something else. We will create a [Python](./key-terms.ipynb#python) list that contains every document. Within each list item for each document, we will use a [Python dictionary](./key-terms.ipynb#python-dictionary) of [key/value pairs](./key-terms.ipynb#key-value-pair) to store information related to that document. \n",
    "\n",
    "Essentially we will have a [list](./key-terms.ipynb#python-list) of documents numbered, from zero to the last document. Each [list](./key-terms.ipynb#python-list) item then will be composed of a [dictionary](./key-terms.ipynb#python-dictionary) of [key/value pairs](./key-terms.ipynb#key-value-pair) that allows us to retrieve information from that particular document by number. The structure will look something like this:\n",
    "\n",
    "![Structure of the corpus, a list of dictionaries](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CorpusView.png)\n",
    "\n",
    "For each item in our list we will be able to use [key/value pairs](./key-terms.ipynb#key-value-pair) to get a **value** if we supply a **key**. We will call our [Python list](./key-terms.ipynb#python-list) variable `all_documents` since it will contain all of the documents in our [corpus](./key-terms.ipynb#corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can begin working with our [dataset](./key-terms.ipynb#dataset), we need to convert the [JSON lines](./key-terms.ipynb#jsonl) file format into [Python](./key-terms.ipynb#python) so we can work with it. Remember that each line of our [JSON lines](./key-terms.ipynb#jsonl) file represents a single text, whether that is a journal article, book, or something else. We will create a [Python](./key-terms.ipynb#python) list that contains every document. Within each list item for each document, we will use a [Python dictionary](./key-terms.ipynb#python-dictionary) of [key/value pairs](./key-terms.ipynb#key-value-pair) to store information related to that document. \n",
    "\n",
    "Essentially we will have a [list](./key-terms.ipynb#python-list) of documents numbered, from zero to the last document. Each [list](./key-terms.ipynb#python-list) item then will be composed of a [dictionary](./key-terms.ipynb#python-dictionary) of [key/value pairs](./key-terms.ipynb#key-value-pair) that allows us to retrieve information from that particular document by number. The structure will look something like this:\n",
    "\n",
    "![Structure of the corpus, a list of dictionaries](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CorpusView.png)\n",
    "\n",
    "For each item in our list we will be able to use [key/value pairs](./key-terms.ipynb#key-value-pair) to get a **value** if we supply a **key**. We will call our [Python list](./key-terms.ipynb#python-list) variable `all_documents` since it will contain all of the documents in our [corpus](./key-terms.ipynb#corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your filename and be sure your file is in your datasets folder\n",
    "file_name = 'sampleJournalAnalysis.jsonl' \n",
    "\n",
    "# Import the json module\n",
    "import json\n",
    "# Create an empty new list variable named `all_documents`\n",
    "all_documents = [] \n",
    "# Temporarily open the file `filename` in the datasets/ folder\n",
    "with open('./datasets/' + file_name) as dataset_file: \n",
    "    #for each line in the dataset file\n",
    "    for line in dataset_file: \n",
    "        # Read each line into a Python dictionary.\n",
    "        # Create a variable document that contains the line using json.loads to convert the json key/value pairs to a python dictionary\n",
    "        document = json.loads(line) \n",
    "        # Append a new list item to `all_documents` containing the dictionary we created.\n",
    "        all_documents.append(document) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of our documents have been converted from our original [JSON lines](./key-terms.ipynb#jsonl) file format (.jsonl) into a [python List](./key-terms.ipynb#python-list) variable named `all_documents`. Let's see what we can discover about our [corpus](./key-terms.ipynb#corpus) with a few simple methods.\n",
    "\n",
    "First, we can determine how many texts are in our [dataset](./key-terms.ipynb#dataset) by using the `len()` function to get the size of `all_documents`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5428"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Choosing a Document to Explore Word Frequencies\n",
    "We will create a new variable called `chosenDocument` and set it equal to first [list](./key-terms.ipynb#python-list) item in `all_documents`. (Remember, in computer code, 0 is the first item, 1 is the second item, 2 is the third item, etc.)\n",
    "\n",
    "We'll also use the .get method to retrieve some information about the item and print it here. Check to make sure this is a suitable article. It it is front matter or back matter, for example, you may want to select another article. You can achieve that by changing the index number in the first line of code. For example, you might change `all_documents[0]` (the first article in the list) to `all_documents[5]` (the sixth article in the list). Remember, in Python, counting starts with 0. \n",
    "\n",
    "If you're not sure if the JSTOR document you selected is a good example, follow the URL to preview it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Names in Horace's Satires\n",
      "written in The Classical Quarterly\n",
      "1960, Volume 2\n",
      "URL is: http://www.jstor.org/stable/638047\n"
     ]
    }
   ],
   "source": [
    "chosenDocument = all_documents[0] # Create a dictionary variable that contains the first document from all_documents. Change 0\n",
    "print(chosenDocument.get('title')) # Get the value for the key title for `chosenDocument` and print it\n",
    "print('written in ' + str(chosenDocument.get('isPartOf'))) # Print 'written in' and the journal value stored in the key 'isPartOf'\n",
    "print(str(chosenDocument.get('publicationYear')) + ', Volume ' + chosenDocument.get('volumeNumber')) # Print the value of the key `publicationYear` and `volumeNumber`\n",
    "print('URL is: ' + chosenDocument.get('url')) # Print 'URL is: ' and the value for the key 'url' in `chosenDocument`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the word counts from the `chosenDocument`. First, we create a new variable `wordCounts` that will contain the word counts from our `chosenDocument`. These are stored as [Python dictionary variables](./key-terms.ipynb#python-dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 528,\n",
       " 'of': 328,\n",
       " 'and': 236,\n",
       " 'to': 233,\n",
       " 'in': 226,\n",
       " 'a': 224,\n",
       " 'is': 210,\n",
       " 'that': 146,\n",
       " 'be': 98,\n",
       " 'I.': 98}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts = chosenDocument.get('unigramCount')\n",
    "#dict(list(wordCounts.items())[:10]) #This code previews the first 10 items in your dictionary\n",
    "#It does this by turning the `wordCounts` dictionary into a list and then shows 10 items (and then turns it back into a dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## An Explanation of the Counter Container Datatype\n",
    "\n",
    "In order to help analyze our dictionary, we are going to use a special container datatype called a Counter. A Counter is like a dictionary. In fact, it uses brackets `{}` like a dictionary. Here's an example where we turn a dictionary (`dictionaryDemo`) into a Counter (`counterDemo`) in order to explore the difference between the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({\"Othello's\": 23,\n",
       "         'Black': 3,\n",
       "         'Handkerchief': 4,\n",
       "         'cover': 4,\n",
       "         'of': 553})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter # Import Counter datatype\n",
    "dictionaryDemo = {\"Othello's\": 23,\n",
    "                  'Black': 3,\n",
    "                  'Handkerchief': 4,\n",
    "                  'cover': 4,\n",
    "                  'of': 553} # Create example dictionary with key/value pairs of words and numbers\n",
    "counterDemo = Counter(dictionaryDemo) # Turn the dictionary into a counter\n",
    "counterDemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Counter type looks identical to a dictionary with key/value pairs within {} that is surrounded by `Counter()`. Both dictionaries and counters can return a **value** from a **key**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(dictionaryDemo['cover']) # Using the Python dictionary `dictionaryDemo`, return the value for the key 'cover'\n",
    "print(counterDemo['cover']) #Using the Python counter `counterDemo`, return the value for the key 'cover'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the Counter has some differences that will help us sum up our word tallies. One key difference is that a Counter returns a 0 for items that are not keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(counterDemo['nosuchkeyexists']) # With a Counter, the value of the made-up key `nosuchkeyexists` is 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this with a dictionary. If a key is not in a dictionary, it returns a key error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nosuchkeyexists'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d9caa3dd9186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionaryDemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nosuchkeyexists'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# With a dictionary, the value of the made-up key `nosuchkeyexists` causes a KeyError in Python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'nosuchkeyexists'"
     ]
    }
   ],
   "source": [
    "print(dictionaryDemo['nosuchkeyexists']) # With a dictionary, the value of the made-up key `nosuchkeyexists` causes a KeyError in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, the most useful aspect of the counter datatype is that it lets us easily return the most common items through the `most_common()` method. We can specify an argument with this method to receive a specified number of results. Let's try it on our example `counterDemo`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 553), (\"Othello's\", 23), ('Handkerchief', 4)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterDemo.most_common(3) # List the top 3 most common items in `counterDemo`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using Counter to Sum the Words in a Single Article\n",
    "\n",
    "Let's return then to the dictionary we created to hold all the words in our article. We called that variable `wordCounts`. We can get a preview of the first 10 words in our dictionary using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 528,\n",
       " 'of': 328,\n",
       " 'and': 236,\n",
       " 'to': 233,\n",
       " 'in': 226,\n",
       " 'a': 224,\n",
       " 'is': 210,\n",
       " 'that': 146,\n",
       " 'be': 98,\n",
       " 'I.': 98}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(wordCounts.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, these are not in order of which words are most frequent. We can discover the most frequent words by turning our dictionary `wordCounts` into a Counter. We will call this Counter `word_freq` and then print out the top 25 most common words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             528\n",
      "of              328\n",
      "and             236\n",
      "to              233\n",
      "in              226\n",
      "a               224\n",
      "is              210\n",
      "that            146\n",
      "be              98\n",
      "I.              98\n",
      "2.              82\n",
      "was             79\n",
      "as              77\n",
      "are             76\n",
      "his             74\n",
      "by              71\n",
      "have            67\n",
      "The             65\n",
      "not             65\n",
      "it              65\n",
      "with            63\n",
      "we              60\n",
      "for             59\n",
      "on              51\n",
      "this            48\n"
     ]
    }
   ],
   "source": [
    "word_freq = Counter(wordCounts) # Create `word_freq` that will be Counter datatype version of our original `wordCounts` dictionary\n",
    "for key, value in word_freq.most_common(25): # For each key/value pair in word_freq Counter's top 25 most common words\n",
    "    print(key.ljust(15), value) #print the `key` left justified 15 characters from the `value` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully created a word frequency list. There are a couple small issues, however, that we still need to address:\n",
    "1. There are many [function words](./key-terms.ipynb#function-words), words like \"the\", \"in\", and \"of\" that are grammatically important but do not carry as much semantic meaning like [content words](./key-terms.ipynb#content-words), such as nouns and verbs. \n",
    "2. The words represented here are actually case-sensitive [strings](./key-terms.ipynb#string). That means that the string \"the\" is a different from the string \"The\". You may notice this in your results above.\n",
    "\n",
    "To solve these issues, we need to find a way remove to remove common [function words](./key-terms.ipynb#function-words) and combine [strings](./key-terms.ipynb#string) that may have capital letters in them. We can solve these issues by:\n",
    "\n",
    "1. Using a [stopwords](./key-terms.ipynb#stop-words) list to remove common [function words](./key-terms.ipynb#function-words)\n",
    "2. Lowercasing all the characters in each string to combine our counts\n",
    "\n",
    "We could create our own stopwords list, but luckily there are many examples out there already. We'll use NLTK's [stopwords](./key-terms.ipynb#stop-words) list to get started.\n",
    "\n",
    "First, we create a new list variable `stop_words` and initialize it with the common English [stopwords](./key-terms.ipynb#stop-words) from the [Natural Language Toolkit](./key-terms.ipynb#nltk) library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #import stopwords from nltk.corpus\n",
    "stop_words = stopwords.words('english') #create a list `stop_words` that contains the English stop words list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're curious what in our stopwords list, we can print a slice of the first ten words in our list to get a preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[:10] #print the first 10 stop words in the list\n",
    "#list(stop_words) #show the whole stopwords list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be that we want to add additional words to our stoplist. For example, we may want to remove character names. We can add items to the list by using the append method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.append(\"hamlet\")\n",
    "stop_words[-10:] #evaluate and show me a slice of the last 10 items in the `stop_words` list variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add multiple words to our stoplist by using the extend() method. Notice that this method requires using a set of brackets `[]` to clarify that we are adding \"gertrude\" and \"horatio\" as list items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend([\"gertrude\", \"horatio\"])\n",
    "stop_words[-10:] #evaluate and show me a slice of the last 10 items in the `stop_words` list variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove words from our list with the remove() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.remove(\"hamlet\")\n",
    "stop_words.remove(\"gertrude\")\n",
    "stop_words.remove(\"horatio\")\n",
    "stop_words[-10:] #evaluate and show me a slice of the last 10 items in the `stop_words` list variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Stopwords in a CSV File\n",
    "We could also store our stop words in a CSV file. A CSV, or \"Comma-Separated Values\" file, is a plain-text file with commas separating each entry. The file could be opened and modified with a text editor or spreadsheet software such as Excel or Google Sheets. Here's what our NLTK stopwords list will look like as a CSV file opened in a plain text editor.\n",
    "\n",
    "![The csv file as an image](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/stopwordsCSV.png)\n",
    "\n",
    "Let's create an example CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv #import the csv library to work with csv files\n",
    "outputFile = open('stopWords.csv', 'w', newline='') #create a variable `outputFile` that will be linked to a new csv file called stopWords.csv\n",
    "outputWriter = csv.writer(outputFile) #create a writer object to add to our `outputFile`\n",
    "outputWriter.writerow(stop_words) #add our list `stop_words` to the CSV file\n",
    "outputFile.close() #close the CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a new file called stopWords.csv that you can open to modify. Go ahead and make a change to your stopWords.csv (either adding or subtracting words). Remember, there are no spaces between words in the CSV file. If you want to edit the CSV right inside Jupyter Lab, right-click on the file and select \"Open With > Editor.\" \n",
    "\n",
    "![Selecting \"Open With > Editor\" in Jupyter Lab](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/editCSV.png)\n",
    "\n",
    "Now go ahead and add in a new word. Remember a few things:\n",
    "\n",
    "* Each word is separated from the next word by a comma.\n",
    "* There are no spaces between the words.\n",
    "* You must save changes to the file if you're using a text editor, Excel, or the Jupyter Lab editor.\n",
    "* You can reopen the file to make sure your changes were saved.\n",
    "\n",
    "Now let's read our CSV file back and overwrite our original `stop_words` list variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newStopwordsFile = open('stopWords.csv') # Open `stopWords.csv` as the variable newStopwordsFile\n",
    "newStopwordsReader = csv.reader(newStopwordsFile) # Create newStopwordsReader variable to open the newStopwordsFile in Reader Mode\n",
    "stop_words = list(newStopwordsReader)[0] # Define the stop_words variable as a list to the contents of newStopwordsReader\n",
    "stop_words # Return the contents of the list variable stop_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refining a stopwords list for your analysis can take time. It depends on:\n",
    "\n",
    "* What you are hoping to discover (for example, are function words important?)\n",
    "* The material you are analyzing (a play text, for example, will repeat the speakers' names many times)\n",
    "\n",
    "If your results are not satisfactory, you can always come back and adjust the stopwords. You may need to run your analysis many times to create a good stopword list.\n",
    "___\n",
    "## Cleaning and Standardizing Tokens\n",
    "\n",
    "We can standardize and [clean](./key-terms.ipynb#clean-data) up the [tokens](./key-terms.ipynb#token) in our [dataset](./key-terms.ipynb#dataset) by creating a function with several steps. The function will:\n",
    "* discard [tokens](./key-terms.ipynb#token) less than 4 characters in length\n",
    "* discard [tokens](./key-terms.ipynb#token) with non-alphabetical characters\n",
    "* lowercase all characters in each [token](./key-terms.ipynb#token)\n",
    "* remove [stopwords](./key-terms.ipynb#stop-words) based on the list we created in `stop_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_word_freq = Counter() # define a new variable `clean_word_freq` that is an empty counter type\n",
    "for token, count in word_freq.items(): # for each token(`key`), value(`count`) pair in our word_freq Counter variable\n",
    "    if len(token) < 4: # require tokens to be 4+ characters\n",
    "        continue\n",
    "    if not token.isalpha(): # require tokens to only contain alphabetical characters\n",
    "        continue\n",
    "    t = token.lower() # lowercase the token\n",
    "    if t in stop_words: # don't include stopwords\n",
    "        continue\n",
    "    clean_word_freq[t] += count # add the token to `clean_word_freq if it passes each test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dictionary `clean_word_freq` contains only function words, lowercased, and greater than four characters. We can print the top 25 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names           43\n",
      "horace          42\n",
      "would           35\n",
      "satires         32\n",
      "also            26\n",
      "like            23\n",
      "name            19\n",
      "since           16\n",
      "must            15\n",
      "could           14\n",
      "book            14\n",
      "first           14\n",
      "rather          14\n",
      "personal        14\n",
      "lucilian        13\n",
      "real            13\n",
      "characters      13\n",
      "mentioned       12\n",
      "much            11\n",
      "probably        11\n",
      "certainly       11\n",
      "satire          11\n",
      "rudd            10\n",
      "type            10\n",
      "case            10\n"
     ]
    }
   ],
   "source": [
    "for key, value in clean_word_freq.most_common(25): # For the top 25 most common key/value pairs in `clean_word_freq`\n",
    "    print(key.ljust(15), value) # print the key (left-justified by 15 characters) followed by the value\n",
    "    # Remember that the key above corresponds to the word and the value corresponds to the number of times that word occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
