{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore word frequencies for a curated dataset\n",
    "\n",
    "This notebook shows how to explore the word frequencies in your dataset. The following processes are described:\n",
    "\n",
    "* Importing your dataset\n",
    "* Discovering the size and contents of your dataset\n",
    "* Turning your dataset into a pandas dataframe\n",
    "* Visualizing the contents of your dataset as a graph with pandas\n",
    "\n",
    "A familiarity with pandas is helpful but not required.\n",
    "____\n",
    "We import the `Dataset` module from the `tdm_client` library. The tdm_client library contains functions for connecting to the JSTOR server containing our [corpus](./key-terms.ipynb#corpus) [dataset](./key-terms.ipynb#dataset). To analyze your dataset, use the [dataset ID](./key-terms.ipynb/#dataset-ID) provided when you created your [dataset](./key-terms.ipynb/#dataset). A copy of your [dataset ID](./key-terms.ipynb/#dataset-ID) was sent to your email when you created your [corpus](./key-terms.ipynb#corpus). It should look like a long series of characters surrounded by dashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm_client import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the [pandas](./key-terms.ipynb#pandas) module to help visualize and manipulate our data. Importing `as pd` allows us to call pandas' functions using the short phrase `pd` instead of typing out `pandas` each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we import `Counter` from `collections` library and `stopwords` from the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new variable **dset** and initialize its value using the **Dataset** function. A sample **dataset ID** of journals focused on Shakespeare is provided here ('a517ef1f-0794-48e4-bea1-ac4fb8b312b4'). Pasting your unique **dataset ID** here will import your dataset from the JSTOR server.\n",
    "\n",
    "**Note**: If you are curious what is in your dataset, there is a download link in the email you received. The format and content of the files is described in the notebook [Building a Dataset](./1-building-a-dataset.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset('a517ef1f-0794-48e4-bea1-ac4fb8b312b4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find total number of documents in the dataset using the `len()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select a single volume from the dataset and examine its word frequencies. We'll start with the first item in our dataset.\n",
    "\n",
    "We create a new variable `my_doc` and initialize it to item 0 of our dataset. \n",
    "We can also return `my_doc` to see what get a stable JSTOR link that describes the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.jstor.org/stable/i40075057'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_doc = dset.items[0]\n",
    "my_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the individual words in the volume. First, we create a new variable `volume_features` that will contain the extracted features from the dataset object. We can accomplish this using the `get_feature` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_features = dset.get_feature(my_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter()\n",
    "for page in volume_features['features']['pages']:\n",
    "    body = page['body']\n",
    "    if body is None:\n",
    "        continue\n",
    "    for token, pos_count in body['tokenPosCount'].items():\n",
    "        for pos, count in pos_count.items():\n",
    "            word_freq[token] += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the                            3661\n",
      ",                              3553\n",
      "of                             2528\n",
      ".                              2286\n",
      "and                            1709\n",
      "to                             1515\n",
      "'                              1294\n",
      "in                             1217\n",
      "a                              1208\n",
      "is                             1078\n",
      "that                           716\n",
      "'s                             642\n",
      "as                             512\n",
      "for                            490\n",
      "it                             468\n",
      "his                            443\n",
      ")                              436\n",
      "(                              436\n",
      "with                           396\n",
      "The                            370\n",
      "which                          354\n",
      "be                             344\n",
      "by                             333\n",
      "I                              325\n",
      "are                            315\n"
     ]
    }
   ],
   "source": [
    "for token, count in word_freq.most_common(25):\n",
    "    print(token.ljust(30), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of noise in these unigrams - mixed case, punctuation, and very common words. Let's use NLTK's stopwords and a couple of simple transformations to make a cleaner word frequency list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter()\n",
    "for page in volume_features['features']['pages']:\n",
    "    body = page['body']\n",
    "    if body is None:\n",
    "        continue\n",
    "    for token, pos_count in body['tokenPosCount'].items():\n",
    "        # require tokens to be 4+ characters\n",
    "        if len(token) < 4:\n",
    "            continue\n",
    "        # require tokens to be alphabetical\n",
    "        if not token.isalpha():\n",
    "            continue\n",
    "        # lower case\n",
    "        t = token.lower()\n",
    "        if t in stop_words:\n",
    "            continue\n",
    "        for pos, count in pos_count.items():\n",
    "            word_freq[t] += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, count in word_freq.most_common(25):\n",
    "    print(token.ljust(30), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
