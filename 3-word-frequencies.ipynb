{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore word frequencies for a curated dataset\n",
    "\n",
    "This notebook shows how to explore the word frequencies in your dataset. The following processes are described:\n",
    "\n",
    "* Importing your dataset\n",
    "* Discovering the size and contents of your dataset\n",
    "* Turning your dataset into a pandas dataframe\n",
    "* Visualizing the contents of your dataset as a graph with pandas\n",
    "\n",
    "A familiarity with pandas is helpful but not required.\n",
    "____\n",
    "We import the `Dataset` module from the `tdm_client` library. The tdm_client library contains functions for connecting to the JSTOR server containing our [corpus](./key-terms.ipynb#corpus) [dataset](./key-terms.ipynb#dataset). To analyze your dataset, use the [dataset ID](./key-terms.ipynb/#dataset-ID) provided when you created your [dataset](./key-terms.ipynb/#dataset). A copy of your [dataset ID](./key-terms.ipynb/#dataset-ID) was sent to your email when you created your [corpus](./key-terms.ipynb#corpus). It should look like a long series of characters surrounded by dashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm_client import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the [pandas](./key-terms.ipynb#pandas) module to help visualize and manipulate our data. Importing `as pd` allows us to call pandas' functions using the short phrase `pd` instead of typing out `pandas` each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we import `Counter` from `collections` library and `stopwords` from the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new variable **dset** and initialize its value using the **Dataset** function. A sample **dataset ID** featuring Shakespeare Quarterly (1950-2014) is provided here ('59c090b6-3851-3c65-e016-9181833b4a2c'). Pasting your unique **dataset ID** here will import your dataset from the JSTOR server.\n",
    "\n",
    "**Note**: If you are curious what is in your dataset, there is a download link in the email you received. The format and content of the files is described in the notebook [Building a Dataset](./1-building-a-dataset.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset('59c090b6-3851-3c65-e016-9181833b4a2c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find total number of documents in the dataset using the `len()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6687"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select a single volume from the dataset and examine its word frequencies. We'll start with the first item in our dataset.\n",
    "\n",
    "We create a new variable `my_doc` and initialize it to item 0 of our dataset. \n",
    "Here, we also return `my_doc` to view a stable JSTOR link that describes the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.jstor.org/stable/2871420'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_doc = dset.items[2278]\n",
    "my_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we copy this URL into a search bar, we can see the article is \"Shakespeare and the Middling Sort\" by Theodore B. Leinwand. \n",
    "\n",
    "___\n",
    "Now, let's inspect the individual words from the article. First, we create a new variable `article_features` that will contain the extracted features from the dataset object. We can accomplish this using the `get_feature` method. This will copy the dictionary of terms and their frequencies for the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_features = dset.get_feature(my_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the `Counter` function from `collections` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(article_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the                            617\n",
      "of                             504\n",
      "and                            426\n",
      "in                             311\n",
      "to                             276\n",
      "a                              191\n",
      "that                           135\n",
      "with                           75\n",
      "The                            73\n",
      "for                            72\n",
      "as                             69\n",
      "his                            67\n",
      "pp.                            65\n",
      "is                             62\n",
      "middling                       59\n",
      "not                            58\n",
      "or                             56\n",
      "their                          55\n",
      "was                            54\n",
      "have                           53\n",
      "we                             48\n",
      "were                           48\n",
      "on                             45\n",
      "p.                             45\n",
      "In                             43\n"
     ]
    }
   ],
   "source": [
    "for token, count in word_freq.most_common(25):\n",
    "    print(token.ljust(30), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of noise in these unigrams - mixed case, punctuation, and very common words. Let's use NLTK's stopwords and a couple of simple transformations to make a cleaner word frequency list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_word_freq = Counter()\n",
    "for token, count in word_freq.items():\n",
    "    # require tokens to be 4+ characters\n",
    "    if len(token) < 4:\n",
    "        continue\n",
    "    # require tokens to be alphabetical\n",
    "    if not token.isalpha():\n",
    "        continue\n",
    "    # lower case\n",
    "    t = token.lower()\n",
    "    # don't include stopwords\n",
    "    if t in stop_words:\n",
    "        continue\n",
    "    clean_word_freq[t] += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "middling                       71\n",
      "sort                           48\n",
      "shakespeare                    47\n",
      "london                         36\n",
      "poor                           29\n",
      "social                         29\n",
      "would                          29\n",
      "among                          25\n",
      "citizens                       25\n",
      "also                           24\n",
      "early                          23\n",
      "might                          22\n",
      "english                        20\n",
      "even                           20\n",
      "coriolanus                     19\n",
      "modern                         16\n",
      "middle                         15\n",
      "people                         15\n",
      "much                           15\n",
      "england                        14\n",
      "terms                          14\n",
      "citizen                        13\n",
      "city                           13\n",
      "urban                          13\n",
      "percent                        13\n"
     ]
    }
   ],
   "source": [
    "for token, count in clean_word_freq.most_common(25):\n",
    "    print(token.ljust(30), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
