{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore word frequencies for a curated dataset\n",
    "\n",
    "This notebook shows how to connect to a curated TDM dataset and explore the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tdm_client import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a TDM dataset object with the collection ID provided en the email you received after curating your collection in the Digital Scholars Workbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset('a517ef1f-0794-48e4-bea1-ac4fb8b312b4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find total number of documents in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine frequent words for a volume in the collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.jstor.org/stable/i40075051'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_doc = dset.items[2]\n",
    "my_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_features = dset.get_feature(my_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'http://www.jstor.org/stable/i40075051',\n",
       " 'journalTitle': 'Critical Survey',\n",
       " 'pageCount': 127,\n",
       " 'provider': 'jstor',\n",
       " 'title': 'Shakespeare and the Cultures of Commemoration',\n",
       " 'wordCount': 57179,\n",
       " 'yearPublished': 2010}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_metadata = [m for m in dset.get_metadata() if m['id'] == my_doc][0]\n",
    "volume_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter()\n",
    "for page in volume['features']['pages']:\n",
    "    body = page['body']\n",
    "    if body is None:\n",
    "        continue\n",
    "    for token, pos in body['tokenPosCount'].items():\n",
    "        word_freq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",                              123\n",
      ".                              122\n",
      "the                            120\n",
      "a                              119\n",
      "and                            118\n",
      "in                             118\n",
      "of                             118\n",
      "to                             115\n",
      "is                             111\n",
      "that                           111\n",
      "The                            107\n",
      "as                             106\n",
      "by                             102\n",
      "with                           100\n",
      "it                             99\n",
      "which                          98\n",
      "\"                              96\n",
      "not                            94\n",
      "this                           93\n",
      "from                           91\n",
      "for                            90\n",
      "'s                             90\n",
      "or                             87\n",
      "was                            86\n",
      "his                            85\n"
     ]
    }
   ],
   "source": [
    "for token, count in word_freq.most_common(25):\n",
    "    print(token.ljust(30), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of noise in these unigrams - mixed case, punctuation, and very common words. Let's use NLTK's stopwords and a couple of simple transformations to make a cleaner word frequency list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter()\n",
    "for page in volume['features']['pages']:\n",
    "    body = page['body']\n",
    "    if body is None:\n",
    "        continue\n",
    "    for token, pos in body['tokenPosCount'].items():\n",
    "        # require tokens to be 4+ characters\n",
    "        if len(token) < 4:\n",
    "            continue\n",
    "        # require tokens to be alphabetical\n",
    "        if not token.isalpha():\n",
    "            continue\n",
    "        # lower case\n",
    "        t = token.lower()\n",
    "        if t in stop_words:\n",
    "            continue\n",
    "        word_freq[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life                           52\n",
      "time                           49\n",
      "also                           47\n",
      "would                          45\n",
      "many                           41\n",
      "dream                          39\n",
      "unconscious                    37\n",
      "like                           36\n",
      "mother                         35\n",
      "first                          34\n",
      "dreams                         33\n",
      "must                           32\n",
      "could                          31\n",
      "woman                          30\n",
      "freud                          29\n",
      "thus                           28\n",
      "even                           28\n",
      "mind                           27\n",
      "child                          27\n",
      "story                          27\n",
      "father                         27\n",
      "little                         27\n",
      "symbolic                       27\n",
      "early                          26\n",
      "death                          26\n"
     ]
    }
   ],
   "source": [
    "for token, count in word_freq.most_common(25):\n",
    "    print(token.ljust(30), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
