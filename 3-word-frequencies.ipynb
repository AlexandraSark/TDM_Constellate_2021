{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore word frequencies for a curated dataset\n",
    "\n",
    "This notebook shows how to explore the word frequencies in your dataset. The following processes are described:\n",
    "\n",
    "* Importing your dataset\n",
    "* Discovering the size and contents of your dataset\n",
    "* Getting raw frequency counts of the document using `Counter` from `Collections`\n",
    "* Cleaning up the corpus using `Stopwords` from `nltk.corpus.corpus`, a part of the Natural Language Toolkit\n",
    "* Creating a new list of the most common words by frequency\n",
    "\n",
    "A familiarity with the `Counter` datatype is helpful for understanding how this notebook sums word frequencies.\n",
    "____\n",
    "We import the `Dataset` module from the `tdm_client` library. The tdm_client library contains functions for connecting to the JSTOR server containing our [corpus](./key-terms.ipynb#corpus) [dataset](./key-terms.ipynb#dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm_client import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we import `Counter` from `collections` library and `stopwords` from the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze your dataset, use the [dataset ID](./key-terms.ipynb/#dataset-ID) provided when you created your [dataset](./key-terms.ipynb/#dataset). A copy of your [dataset ID](./key-terms.ipynb/#dataset-ID) was sent to your email when you created your [corpus](./key-terms.ipynb#corpus). It should look like a long series of characters surrounded by dashes. \n",
    "\n",
    "We create a new variable **dset** and initialize its value using the **Dataset** function. A sample **dataset ID** featuring Shakespeare Quarterly (1950-2014) is provided here ('59c090b6-3851-3c65-e016-9181833b4a2c'). Pasting your unique **dataset ID** here will import your dataset from the JSTOR server.\n",
    "\n",
    "**Note**: If you are curious what is in your dataset, there is a download link in the email you received. The format and content of the files is described in the notebook [Building a Dataset](./1-building-a-dataset.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset('59c090b6-3851-3c65-e016-9181833b4a2c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find total number of documents in the dataset using the `len()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6687"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select a single volume from the dataset and examine its word frequencies. We create a new variable `my_doc` and initialize it to an item in our dataset. We have randomly chosen 2278 here, but any item from 0-6686 would be suitable for this analysis.\n",
    "Here, we also return `my_doc` to view a stable JSTOR link that describes the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.jstor.org/stable/2867774'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_doc = dset.items[2278]\n",
    "my_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we copy this URL into a search bar, we can see the article is \"Shakespeare and the Middling Sort\" by Theodore B. Leinwand. \n",
    "\n",
    "___\n",
    "Now, let's inspect the individual words from the article. First, we create a new variable `article_features` that will contain the extracted features from the dataset object. We can accomplish this using the `get_feature` method. This will copy the dictionary of terms and their frequencies for the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_features = dset.get_feature(my_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the `Counter` function from `collections`. This turns our dictionary into a Counter datatype that makes it easier to sum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(article_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we loop through all of the tokens in our new counter object `word_freq` to discover the most common words. The `most_common` method returns a list of the n most common elements and their counts. Here n=25 but you could set this to any value you like. If left blank, the `most_common` method will return all of the tokens from most to least common.\n",
    "\n",
    "We print the tokens out using the ljust() method that left justifies the output so we get a nice neat columns for the tokens and the counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the                            208\n",
      "of                             122\n",
      "to                             85\n",
      "in                             65\n",
      "and                            62\n",
      "is                             39\n",
      "that                           38\n",
      "The                            32\n",
      "be                             28\n",
      "for                            28\n",
      "a                              27\n",
      "as                             25\n",
      "by                             24\n",
      "this                           24\n",
      "his                            20\n",
      "with                           19\n",
      "he                             18\n",
      "Othello                        17\n",
      "reading                        17\n",
      "it                             16\n",
      "have                           15\n",
      "which                          15\n",
      "has                            13\n",
      "on                             13\n",
      "we                             13\n"
     ]
    }
   ],
   "source": [
    "for token, count in word_freq.most_common(25):\n",
    "    print(token.ljust(30), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple immediate problems with these preliminary results:\n",
    "1. There are many [function words](./key-terms.ipynb#function-words) (the, in, of) that we may not be interested in.\n",
    "2. The words represented here are actually case-sensitive [strings](./key-terms.ipynb#string)\n",
    "\n",
    "The most popular word is \"the\" which occurs 208 times. We can also see \"The\", with a capital \"T\", is listed as occurring 32 times. We need to find a way remove to remove common [function words](./key-terms.ipynb#function-words) and combine [strings](./key-terms.ipynb#string) that may have capital letters in them. \n",
    "\n",
    "We can solve these issues by:\n",
    "\n",
    "1. Using a [stopwords](./key-terms.ipynb#stop-words) list to remove common [function words](./key-terms.ipynb#function-words)\n",
    "2. Lowercasing all the characters in the text and combining the counts\n",
    "\n",
    "We will use NLTK's [stopwords](./key-terms.ipynb#stop-words) to get started. We create a new list variable `stop_words` and initialize it with the common English [stopwords](./key-terms.ipynb#stop-words) from the [Natural Language Toolkit](./key-terms.ipynb#nltk) library. We'll print a slice of the first ten words in `stop_words` to get a preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be that we want to add additional words to our stoplist. For example, we may want to remove character names. We can add items to the list by using the append method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'hamlet']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words.append(\"hamlet\")\n",
    "stop_words[-10:] #evaluate and show me a slice of the last 10 items in the `stop_words` list variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add multiple words to our stoplist by using the extend() method. Notice that this method requires using a set of brackets `[]` to clarify that we are adding \"gertrude\" and \"horatio\" as list items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'hamlet',\n",
       " 'gertrude',\n",
       " 'horatio']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words.extend([\"gertrude\", \"horatio\"])\n",
    "stop_words[-10:] #evaluate and show me a slice of the last 10 items in the `stop_words` list variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove words from our list with the remove() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.remove(\"hamlet\")\n",
    "stop_words.remove(\"gertrude\")\n",
    "stop_words.remove(\"horatio\")\n",
    "stop_words[-10:] #evaluate and show me a slice of the last 10 items in the `stop_words` list variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good stopwords list, let's use it to standardize and [clean](./key-terms.ipynb#clean-data) up the [tokens](./key-terms.ipynb#token) in our [dataset](./key-terms.ipynb#dataset). The function will:\n",
    "* discard [tokens](./key-terms.ipynb#token) less than 4 characters in length\n",
    "* discard [tokens](./key-terms.ipynb#token) with non-alphabetical characters\n",
    "* lowercase all characters in each [token](./key-terms.ipynb#token)\n",
    "* remove [stopwords](./key-terms.ipynb#stop-words) based on the list we created in `stop_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_word_freq = Counter() # define a new variable `clean_word_freq` that t\n",
    "for token, count in word_freq.items():\n",
    "    # require tokens to be 4+ characters\n",
    "    if len(token) < 4:\n",
    "        continue\n",
    "    # require tokens to be alphabetical\n",
    "    if not token.isalpha():\n",
    "        continue\n",
    "    # lower case\n",
    "    t = token.lower()\n",
    "    # don't include stopwords\n",
    "    if t in stop_words:\n",
    "        continue\n",
    "    clean_word_freq[t] += count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token(token): #define a function `process_token` that takes the argument `token`\n",
    "    token = token.lower() #set the string in token to a new string with all lowercase letters\n",
    "    corrected = htrc_corrections.get(token) #initialize a new variable `corrected` that runs toke through the `htrc_corrections.get()` function to fix common OCR errors\n",
    "    if corrected is not None: #if corrected has a value, set the `token` variable to the same value as `corrected`\n",
    "        token = corrected\n",
    "    if len(token) < 4: #if token is less than four characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    if not(token.isalpha()): #if token contains non-alphabetic characters, return nothing for process_function (no output here essentially erases this token)\n",
    "        return\n",
    "    return token #return the `token` variable which has been set equal to the `corrected` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "othello                        19\n",
      "reading                        17\n",
      "shakespeare                    12\n",
      "folio                          11\n",
      "first                          8\n",
      "iago                           8\n",
      "also                           8\n",
      "dramatic                       8\n",
      "quarto                         7\n",
      "text                           7\n",
      "sword                          6\n",
      "even                           5\n",
      "context                        5\n",
      "word                           5\n",
      "desdemona                      4\n",
      "john                           4\n",
      "readings                       4\n",
      "spirit                         4\n",
      "upon                           4\n",
      "walker                         4\n",
      "bloody                         4\n",
      "considered                     4\n",
      "editors                        4\n",
      "expression                     4\n",
      "hero                           4\n"
     ]
    }
   ],
   "source": [
    "for token, count in clean_word_freq.most_common(25):\n",
    "    print(token.ljust(30), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
